% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  a4paper,
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={MATH3714 Linear Regression and Robustness},
  pdfauthor={Jochen Voss},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{comment}
\specialcomment{myanswers}{\smallbreak\begingroup\leftskip=\leftmargini\noindent Solution: }%
  {\medbreak\endgroup}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}

\title{MATH3714 Linear Regression and Robustness}
\author{\href{mailto:J.Voss@leeds.ac.uk}{Jochen Voss}}
\date{University of Leeds, Semester 1, 2021--22}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{conjecture}{Conjecture}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[section]
\theoremstyle{definition}
\newtheorem{hypothesis}{Hypothesis}[section]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\newcommand{\argmin}{\mathop{\mathrm{arg\,min}}\limits}
\newcommand{\bias}{\mathop{\mathrm{bias}}}
\newcommand{\Cov}{\mathop{\mathrm{Cov}}}
\newcommand{\CN}{\mathcal{N}}
\newcommand{\ds}{\displaystyle}
\newcommand{\E}{\mathbb{E}}
\newcommand{\eps}{\varepsilon}
\newcommand{\R}{\mathbb{R}}
\newcommand{\rank}{\mathop{\mathrm{rank}}}
\newcommand{\se}{\mathop{\mathrm{se}}}
\newcommand{\stdev}{\mathop{\mathrm{stdev}}}
\newcommand{\Var}{\mathop{\mathrm{Var}}}
\newcommand{\xfrac}[2]{\frac{\,#1\,}{\,#2\,}}

\hypertarget{home}{%
\section*{Preface}\label{home}}
\addcontentsline{toc}{section}{Preface}

From previous modules we know how to fit a regression line through
points \((x_1, y_1), \ldots, (x_n, y_n) \in\mathbb{R}^2\). The underlying model
here is described by the equation
\begin{equation*}
  y_i
  = \alpha + \beta x_i + \varepsilon_i
\end{equation*}
for all \(i \in \{1, 2, \ldots, n\}\), and the aim is to find values for
the intercept \(\alpha\) and the slope \(\beta\) such that the residuals
\(\varepsilon_i\) are as small as possible. This procedure, called simple
linear regression, is illustrated in figure~\ref{fig:scatter-plot}.



\begin{figure}

{\centering \includegraphics{MATH3714_files/figure-latex/scatter-plot-1} 

}

\caption{An illustration of linear regression. Each of the black circles in the plot stands for one paired sample \((x_i, y_i)\). The regression line \(x \mapsto \alpha + \beta x\), with intercept~\(\alpha\) and slope~\(\beta\), aims to predict the value of~\(y\) using the observed value~\(x\). For the marked sample \((x_i, y_i)\), the predicted \(y\)-value is \(\hat y\).}\label{fig:scatter-plot}
\end{figure}

In this situation, the variable \(x\) is called a \textbf{input}, feature, or
sometimes the explanatory variable or the ``independent variable''.
The variable \(y\) is called \textbf{response} or output, or sometimes the
``dependent variable'', and \(\varepsilon\) is called the \textbf{residual} or error.

Extending the situation of simple linear regression, in this module
we will consider multiple linear regression, where the response~\(y\)
is allowed to depend on several input variables. The corresponding
model is now
\begin{equation*}
  y_i = \alpha + \beta_1 x_{i1} + \cdots + \beta_p x_{ip} + \varepsilon_i
\end{equation*}
for all \(i \in \{1, 2, \ldots, p\}\), where \(n\) is still the number
of observations, and \(p\) is now the number of inputs we observe for
each sample.

Note that for multiple linear regression, we still consider a
single response for each sample, only the number of inputs has been
increased. One way to deal with situations where there is more than
one output would be to fit separate models for each output.

We will discuss multiple linear regression in much detail; our discussion
will be guided by three different aims of linear regression:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Prediction: given a not previously observed value \(x\), try to predict
  the corresponding~\(y\).
\item
  In cases were the residuals \(\varepsilon_i\) correspond to unwanted noise,
  the fitted values \(\hat y_i = \alpha + \beta x_i\) can be considered
  to be de-noised versions of the observed values \(y_i\).
\item
  By studying a fitted regression model, sometimes better understanding
  of the data can be achieved. For example, one could ask whether
  all of the \(p\) input variables carry information about the response~\(y\).
\end{enumerate}

We will address these aims by considering different questions, like
how to estimate the coefficients \(\alpha, \beta_1, \ldots, \beta_p\),
how to assess model fit, or how to deal with outliers in the data.

\clearpage

\hypertarget{S00-about}{%
\section*{About MATH3714}\label{S00-about}}
\addcontentsline{toc}{section}{About MATH3714}

This module is \textbf{MATH3714 Linear Regression and Robustness}. The
module manager and lecturer is Dr Jochen Voss, and my email address is
\href{mailto:J.Voss@leeds.ac.uk}{\nolinkurl{J.Voss@leeds.ac.uk}}.

\hypertarget{notes}{%
\subsection*{Notes and videos}\label{notes}}
\addcontentsline{toc}{subsection}{Notes and videos}

The main way I expect you to learn the material for this course is by
reading these notes and by watching the accompanying videos. I will
release two sections of notes each week, for a total of 22 sections.

Reading mathematics is a slow process. Each section roughly
corresponds to one traditional lecture, which would have taken 50
minutes. If you find yourself regularly getting through sections in
much less than an hour, you're probably not reading carefully enough
through each sentence of explanation and each line of mathematics,
including understanding the motivation as well as checking the
accuracy.

It is possible (but not recommended) to learn the material by only
reading the notes and not watching the videos. It is not possible to
learn the material by only watching the videos and not reading the
notes.

Since we will all be relying heavily on these notes, I'm even more
keen than usual to hear about errors mathematical, typographical or
otherwise. Please, please \href{mailto:J.Voss@leeds.ac.uk}{email me} if
think you may have found any.

\hypertarget{lectures}{%
\subsection*{Lectures}\label{lectures}}
\addcontentsline{toc}{subsection}{Lectures}

There will be one online synchronous ``lecture'' session each week, on
Mondays at 2-3pm, with me, run through Microsoft Teams. These will
not be ``lectures'' in the traditional sense of the term, but will be an
opportunity to re-emphasise material you have already learned from
notes and videos, to give extra examples, and to answer common student
questions, with some degree of interactivity.

I will assume you have completed all the work for the previous week by
the time of the lecture, but I will not assume you've started the work
for that week itself.

I am very keen to hear about things you'd like to go through in the
lectures; please \href{mailto:J.Voss@leeds.ac.uk}{email me} with your
suggestions.

\hypertarget{workshops}{%
\subsection*{Workshops and Problem Sheets}\label{workshops}}
\addcontentsline{toc}{subsection}{Workshops and Problem Sheets}

There will be 5 problem sheets, corresponding to workshops in weeks 2,
4, 6, 8 and~10. The main goal of the workshops will be to go over
your answers to the problems sheets.

My recommended approach to problem sheets and workshops is the following:

\begin{itemize}
\tightlist
\item
  Work through the problem sheet before the workshop, spending plenty
  of time on it, and making multiple efforts at questions you get
  stuck on. I recommend spending \emph{at least three hours} on each
  problem sheet, in more than one block. Collaboration is encouraged
  when working through the problems, but I recommend writing up your
  work on your own.
\item
  Take advantage of the workshops to ask for help or clarification on
  questions you weren't able to complete.
\item
  After the workshop, attempt again the questions you were previously stuck on.
\item
  If you're still unable to complete a question after this second
  round of attempts, \emph{then} consult the solutions.
\end{itemize}

\hypertarget{teams}{%
\subsection*{Discussion Board}\label{teams}}
\addcontentsline{toc}{subsection}{Discussion Board}

I have set up \href{https://teams.microsoft.com/l/team/19\%3aopeKURHGjduA41CDtQUkmDjoPalh-zWDGuKhTk9zYq41\%40thread.tacv2/conversations?groupId=bde4675f-2c0d-4efd-9762-748cca83f4c4\&tenantId=bdeaeda8-c81d-45ce-863e-5232a535b7cb}{a Microsoft
Team}
for the course. I propose to use the ``Discussion'' channel there as a
discussion board. This is a good place to post questions about
material from the course, and --- even better! --- to help answer your
colleagues' questions. The idea is that you all as a group should help
each other out. I will visit a couple of times a week to clarify if
everybody is stumped by a question, or if there is disagreement.

\hypertarget{software}{%
\subsection*{Software}\label{software}}
\addcontentsline{toc}{subsection}{Software}

For the module we will use the statistical computing package R. This
program is free software, and you can find the program and
documentation at the \href{https://www.r-project.org/}{R project homepage}.
In particular, R will be used in the (assessed) practial.

My recommendation would be to install the \href{https://www.rstudio.com/}{RStudio
environment}, which includes R, on your own
computer and use this for your work. (Choose the open source
version, ``RStudio Desktop'', on the download page.) Alternatively you
can use RStudio or plain R on the university computers.

\hypertarget{assessments}{%
\subsection*{Assessments}\label{assessments}}
\addcontentsline{toc}{subsection}{Assessments}

Your final mark for the module will be based on a computer practical
(20\%) and a final exam (80\%). For the practical (I believe it will
take place in week 10) you will need to solve some problem using R and
the methods you learned in the course and to present your results in a
short report.

\clearpage

\hypertarget{S01-simple}{%
\section{Simple Linear Regression}\label{S01-simple}}

As a reminder, we consider simple linear regression in this section.
My hope is, that all of you have seen this material before at some
stage, \emph{e.g.}~in school or in some first or second year modules.

In preparation for notation introduced in the next section, we rename
the parameters from \(\alpha\) and \(\beta\) to the new names \(\beta_0\)
for the intercept and \(\beta_1\) for the slope.

\hypertarget{residual-sum-of-squares}{%
\subsection{Residual Sum of Squares}\label{residual-sum-of-squares}}

In simple linear regression, the aim is to find a regression line \(y = \beta_0 + \beta_1 x\), such that the line is ``close'' to given data points
\((x_1, y_1), \ldots, (x_n, y_n) \in\mathbb{R}^2\) for \(i \in \{1, 2, \ldots, n\}\). The ususal way to find \(alpha\) and \(\beta_1\), and thus the regression
line, is by minimising the \textbf{residual sum of squares}:
\begin{equation}
  r(\beta_0, \beta_1)
  = \sum_{i=1}^n \bigl( y_i - (\beta_0 + \beta_1 x_i) \bigr)^2.
  \label{eq:RSS}
\end{equation}
For given \(\beta_0\) and \(\beta_1\), the value \(r(\beta_0, \beta_1)\) measures
how close (in vertical direction) the given data points \((x_i, y_i)\)
are to the regression line \(\beta_0 + \beta_1 x\). By minimising
\(r(\beta_0, \beta_1)\) we find the regression line which is ``closest'' to
the data. The solution of this minimisation problem is usually
expressed in terms of the sample variance \(\mathrm{s}_x\) and the
sample covariance~\(\mathrm{s}_{xy}\).

\begin{definition}
\protect\hypertarget{def:sx}{}\label{def:sx}The \textbf{sample covariance} of \(x_1, \ldots, x_n \in \mathbb{R}\) and
\(y_1, \ldots, y_n\in\mathbb{R}\) is given by
\begin{equation*}
    \mathrm{s}_{xy}
    := \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar x) (y_i - \bar y),
  \end{equation*}
where \(\bar x\) and \(\bar y\) are the sample means.

The \textbf{sample variance} of \(x_1, \ldots, x_n \in \mathbb{R}\) is given by
\begin{equation*}
    \mathrm{s}_{x}^2
    := \mathrm{s}_{xx}
    = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar x)^2,
  \end{equation*}
where, again, \(\bar x\) is the sample mean of the \(x_i\).
\end{definition}

\begin{lemma}
\protect\hypertarget{lem:simple-LSQ}{}\label{lem:simple-LSQ}Assume that \(\mathrm{s}_x^2 > 0\). Then the function \(r(\beta_0, \beta_1)\)
from~\eqref{eq:RSS} takes its minimum at the point \((\beta_0, \beta_1)\)
given by
\begin{equation*}
    \hat\beta_1 = \frac{\mathrm{s}_{xy}}{\mathrm{s}_x^2},
    \qquad
    \hat\beta_0 = \bar y - \hat \beta_1 \bar x,
  \end{equation*}
where \(\bar x, \bar y\) are the sample means, \(\mathrm{s}_{xy}\) is the
sample covariance and \(\mathrm{s}_x^2\) is the sample variance.
\end{lemma}

\begin{proof}
We could find the minimum of \(r\) by differentiating and setting the
derivatives to zero. Here we follow a different approach which uses
a ``trick'' to simplify the algebra: Let \(\tilde x_i = x_i - \bar x\)
and \(\tilde y_i = y_i - \bar y\) for all \(i \in \{1, \ldots, n\}\).
Then we have
\begin{equation*}
    \sum_{i=1}^n \tilde x_i
    = \sum_{i=1}^n x_i - n \bar x
    = 0
  \end{equation*}
and, similarly, \(\sum_{i=1}^n \tilde y_i = 0\). Using the new
coordinates \(\tilde x_i\) and \(\tilde y_i\) we find
\begin{align*}
    r(\beta_0, \beta_1)
    &= \sum_{i=1}^n \bigl(y_i - \beta_0 - \beta_1 x_i \bigr)^2 \\
    &= \sum_{i=1}^n \bigl( \tilde y_i + \bar y - \beta_0 - \beta_1 \tilde x_i - \beta_1 \bar x \bigr)^2 \\
    &= \sum_{i=1}^n \Bigl( \bigl( \tilde y_i - \beta_1 \tilde x_i \bigr)
    + \bigl( \bar y - \beta_0 - \beta_1 \bar x \bigr) \Bigr)^2 \\
    &= \sum_{i=1}^n \bigl( \tilde y_i - \beta_1 \tilde x_i \bigr)^2
    + 2 \bigl( \bar y - \beta_0 - \beta_1 \bar x \bigr) \sum_{i=1}^n \bigl( \tilde y_i - \beta_1 \tilde x_i \bigr)
    + n \bigl( \bar y - \beta_0 - \beta_1 \bar x \bigr)^2
  \end{align*}
Since \(\sum_{i=1}^n \tilde x_i = \sum_{i=1}^n \tilde y_i = 0\), the
second term on the right-hand side vanishes and we get
\begin{equation}
    r(\beta_0, \beta_1)
    = \sum_{i=1}^n \bigl( \tilde y_i - \beta_1 \tilde x_i \bigr)^2
    + n \bigl( \bar y - \beta_0 - \beta_1 \bar x \bigr)^2.  \label{eq:RSS-rewrite}
  \end{equation}
Both of these terms are positive and we can minimise the second term
(without changing the first term) by setting
\(\beta_0 = \bar y - \beta_1 \bar x\).

To find the value of \(\beta_1\) which minimises the first term on the
right-hand side of~\eqref{eq:RSS-rewrite} we now set the
(one-dimensional) derivative w.r.t.~\(\beta_1\) equal to~\(0\). We get
the condition
\begin{align*}
    0
    &\overset{!}{=} \frac{d}{d\beta_1} \sum_{i=1}^n \bigl( \tilde y_i - \beta_1 \tilde x_i \bigr)^2 \\
    &= \sum_{i=1}^n 2 \bigl( \tilde y_i - \beta_1 \tilde x_i \bigr) \frac{d}{d\beta_1} \bigl( \tilde y_i - \beta_1 \tilde x_i \bigr) \\
    &= - 2 \sum_{i=1}^n \bigl( \tilde y_i - \beta_1 \tilde x_i \bigr) \tilde x_i \\
    &= -2  \sum_{i=1}^n \tilde x_i \tilde y_i + 2 \beta_1 \sum_{i=1}^n \tilde x_i^2.
  \end{align*}
The only solution to this equation is
\begin{align*}
    \beta_1
    &= \frac{\sum_{i=1}^n \tilde x_i \tilde y_i}{\sum_{i=1}^n \tilde x_i^2} \\
    &= \frac{\sum_{i=1}^n (x_i - \bar x) (y_i - \bar y)}%
    {\sum_{i=1}^n (x_i - \bar x)^2} \\
    &= \frac{\frac{1}{n-1}\sum_{i=1}^n (x_i - \bar x) (y_i - \bar y)}%
    {\frac{1}{n-1}\sum_{i=1}^n (x_i - \bar x)^2} \\
    &= \frac{\mathrm{s}_{xy}}{\mathrm{s}_x^2}.
  \end{align*}
Since the second derivative is \(2 \sum_{i=1}^n \tilde x_i^2 \geq 0\),
this is indeed a minimum and the proof is complete.
\end{proof}

\hypertarget{linear-regression-as-a-parameter-estimation-problem}{%
\subsection{Linear Regression as a Parameter Estimation Problem}\label{linear-regression-as-a-parameter-estimation-problem}}

In statistics, any analysis starts by making a statistical model of
the data. This is done by writing random variables which have the
same structure as the data, and which are chosen so that the data
``looks like'' a random sample from these random variables.

To construct a model for the data used in a simple linear regression
problem, we use random variables \(Y_1, \ldots, Y_n\) such that
\begin{equation}
  Y_i
  = \beta_0 + \beta_1 x_i + \varepsilon_i \label{eq:regression}
\end{equation}
for all \(i \in \{1, 2, \ldots, n\}\), where \(\varepsilon_1, \ldots, \varepsilon_n\)
are i.i.d.~random variables with \(\mathbb{E}(\varepsilon_i) = 0\) and
\(\mathop{\mathrm{Var}}(\varepsilon_i) = \sigma^2\).

\begin{itemize}
\tightlist
\item
  Here we assume that the \(x\)-values are fixed and known. The only
  random quantities in the model are \(\varepsilon_i\) and~\(Y_i\). (There are
  more complicated models which also allow for randomness of \(x\), but
  we won't consider such models here.)
\item
  The random variables \(\varepsilon_i\) are called \textbf{residuals} or
  \textbf{errors}. In a scatter plot, the residuals correspond to the
  vertical distance between the samples and the regression line.
  Often one assumes that \(\varepsilon_i \sim \mathcal{N}(0, \sigma^2)\) for all
  \(i \in \{1, 2, \ldots, n\}\).
\item
  The values \(\beta_0\), \(\beta_1\) and \(\sigma^2\) are parameters of
  the model. To fit the model to data, we need to estimate these
  parameters.
\end{itemize}

This model is more complex than the models considered in some
introductory courses to statistics:

\begin{itemize}
\tightlist
\item
  The data consists now of pairs of numbers, instead of just
  single numbers.
\item
  We have
  \begin{equation*}
    \mathbb{E}(Y_i)
    = \mathbb{E}\bigl( \beta_0 + \beta_1 x_i + \varepsilon_i \bigr)
    = \beta_0 + \beta_1 x_i + \mathbb{E}(\varepsilon_i)
    = \beta_0 + \beta_1 x_i.
  \end{equation*}
  Thus, the expectation of \(Y_i\) depends on \(x_i\) and, at least for
  \(\beta_1 \neq 0\), the random variables \(Y_i\) are not identically
  distributed.
\end{itemize}

In this setup, we can consider the estimates \(\hat\beta_0\) and \(\hat\beta_1\)
from the previous subsection as statistcal parameter estimates for the
model parameters \(\beta_0\) and~\(\beta_1\).

In order to fit a linear model we also need to estimate the residual
variance~\(\sigma^2\). This can be done using the estimator
\begin{equation}
  \hat\sigma^2
  = \frac{1}{n-2} \sum_{i=1}^n \hat\varepsilon_i^2
  = \frac{1}{n-2} \sum_{i=1}^n (y_i - \hat\beta_0 - \hat\beta_1 x_i)^2.
  \label{eq:reg-sigma-est}
\end{equation}
To understand the form of this estimator, we have to remember that
\(\sigma^2\) is the variance of the \(\varepsilon_i\). Thus, using the standard
estimator for the variance, we could estimate \(\sigma^2\) as
\begin{equation}
  \sigma^2
  \approx \frac{1}{n-1} \sum_{i=1}^n \bigl(\varepsilon_i - \bar\varepsilon\bigr)^2
  \approx \frac{1}{n-1} \sum_{i=1}^n \bigl(\hat\varepsilon_i - \overline{\hat\varepsilon}\bigr)^2,
  \label{eq:resid-var-est}
\end{equation}

where \(\bar\varepsilon\) and \(\overline{\hat\varepsilon}\) are the averages of the
\(\varepsilon_i\) and the \(\hat\varepsilon_i\), respectively. One can show that
\(\overline{\hat\varepsilon} = 0\). The estimates of \(\beta_0\) and \(\beta_1\) are
sensitive to fluctuations in the data, with the effect that the
estimated regression line is, on average, slightly closer to the data
points than the true regression line would be. This causes the sample
variance of the \(\hat\varepsilon_i\), on average, to be slightly smaller than
the true residual variance \(\sigma^2\) and the thus the
estimator~\eqref{eq:resid-var-est} is slightly biased. A more
detailed analysis reveals that an unbiased estimator can be obtained
if one replaces the pre-factor \(1/(n-1)\) in equation~\eqref{eq:resid-var-est}
with \(1/(n-2)\). This leads to the estimator~\eqref{eq:reg-sigma-est}.

The main advantage gained by considering a statistical model is, that
we now can consider how close the estimators \(\hat\beta_0\), \(\hat\beta_1\)
and \(\hat\sigma^2\) are to the true values. Results one can obtain
include the following:

\begin{itemize}
\item
  The estimators \(\hat\beta_0\), \(\hat\beta_1\) and \(\hat\sigma^2\)
  are unbiased: This means that when we plug in random data
  \((x_i, Y_i)\) from the model~\eqref{eq:regression}, on
  average we get the correc answer: \(\mathbb{E}(\hat\beta_0) = \beta_0\),
  \(\mathbb{E}(\hat\beta_1) = \beta_1\), \(\mathbb{E}(\hat\sigma^2) = \sigma^2\).
\item
  One can ask about the average distance between the estimated
  parameters \(\hat\beta_0\), \(\hat\beta_1\) and \(\hat\sigma^2\)
  and the (unknown) true values \(\beta_0\), \(\beta_1\) and \(\sigma^2\).
  One measure for these distances is the root mean squared error
  of the estimators.
\item
  One can consider confidence intervals for the parameters
  \(\beta_0\), \(\beta_1\) and \(\sigma^2\).
\item
  One can consider statistical hypothesis tests to answer
  yes/no questions about the parameters. For example, one might ask
  whether the data could have come from the model with \(\beta_0=0\).
\item
  One can consider whether the data is compatible with the model
  at all, irrespective of parameter values. If there is a non-linear
  relationship between \(x\) and \(y\), the model~\eqref{eq:regression}
  will no longer be appropriate.
\end{itemize}

We will consider most of these questions over the course of the
module.

\hypertarget{sec:simple-mat}{%
\subsection{Matrix Notation}\label{sec:simple-mat}}

To conclude this section, we will rewrite the results of this section
in a form which we will extensively use for multiple linear regression
in the rest of this module. The idea here is to arrange all quantities
in the problem as matrices and vectors in order to simplify notation.
We write
\begin{equation*}
  X = \begin{pmatrix}
    1 & x_1\\ 1 & x_2\\\vdots & \vdots\\1 & x_n
  \end{pmatrix}
  \in \mathbb{R}^{n\times 2},
  \qquad
  y = \begin{pmatrix}
    y_1 \\ y_2 \\ \vdots \\ y_n
  \end{pmatrix}
  \in \mathbb{R}^n,
  \qquad
  \varepsilon= \begin{pmatrix}
    \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_n
  \end{pmatrix}
  \in \mathbb{R}^n,
  \qquad
  \beta = \begin{pmatrix}
    \beta_0 \\
    \beta_1
  \end{pmatrix}
  \in\mathbb{R}^2
\end{equation*}
Using this notation, we can rewrite the \(n\) equations \(y_i = \beta_0 + x_i \beta_1 + \varepsilon_i\) for \(i \in \{1, \ldots, n\}\) as one
vector-valued equation in~\(\mathbb{R}^n\): we get
\begin{equation*}
  y = X\beta + \varepsilon,
\end{equation*}
and we want to ``solve'' this vector-valued equation for~\(\beta\).
The sum of squares can now be written as
\begin{equation*}
  r(\beta)
  = \sum_{i=1}^n \varepsilon_i^2
  = \varepsilon^\top \varepsilon
  = (y - X\beta)^\top (y - X\beta)
  = y^\top y - 2\beta^\top X^\top y + \beta^\top X^\top X \beta.
\end{equation*}
In the next section we will see that the minimum of \(r\) is attained
for
\begin{equation*}
  \hat\beta
  = (X X^\top)^{-1} X^\top y
\end{equation*}
and one can check that the components of this vector \(\hat\beta = (\hat\beta_0, \hat\beta_1)\) coincide with the estimates we obtained above.

\textbf{Summary}

\begin{itemize}
\tightlist
\item
  simple linear regression is the case where there is only one input
\item
  a regression line is fitted by minimising the residual sum of squares
\item
  linear regression is a statistical parameter estimation problem
\item
  the problem can be conveniently written in matrix/vector notation
\end{itemize}

\clearpage

\hypertarget{S02-multiple}{%
\section{Least Squares Estimates}\label{S02-multiple}}

\hypertarget{data-and-models}{%
\subsection{Data and Models}\label{data-and-models}}

For multiple linear regression we assume that there are \(p\) inputs and
one output. If we have a sample of \(n\) obervations, we have \(np\)
inputs and one output in total. Here we denote the \(i\)th observation
of the \(j\)th input by \(x_{ij}\) and the corresponding output by~\(y_j\).

As an example, we consider the \texttt{mtcars} dataset built into R. This is
a small dataset, which contains information about 32 automobiles
(1973--74 models). The table lists fuel consumption \texttt{mpg}, gross horsepower \texttt{hp},
and 9 other aspects of these cars. Here we consider \texttt{mpg} to be the output,
and the other listed aspects to be inputs. Type \texttt{help(mtcars)} in R to learn
more about this dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2
## Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1
## Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4
## Merc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2
## Merc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2
## Merc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4
## Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4
## Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3
## Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3
## Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3
## Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4
## Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4
## Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4
## Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1
## Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2
## Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1
## Toyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1
## Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2
## AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2
## Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4
## Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2
## Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1
## Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2
## Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2
## Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4
## Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6
## Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8
## Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2
\end{verbatim}

For this dataset we have \(n = 32\) (number of cars), and \(p = 10\) (number
of attributes, excluding \texttt{mpg}). The values \(y_1, \ldots, y_{32}\) are listed
in the first column of the table, the values \(x_{i,1}\) for \(i \in \{1, \ldots, 32\}\) are shown in the second column, and the values \(x_{i,10}\) are shown in
the last column.

In this data set it is easy to make scatter plots which show how a single
input affects the output. For example, we can show how the engine power
affects fuel consumption:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{hp, mtcars}\SpecialCharTok{$}\NormalTok{mpg,}
     \AttributeTok{xlab =} \StringTok{"power [hp]"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"fuel consumption [mpg]"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{MATH3714_files/figure-latex/mtcars-hp-mpg-plot-1.pdf}

We can see that cars with stronger engines tend to use more fuel
(\emph{i.e.}~a gallon of fuel lasts for fewer miles; the curve goes down),
but leaving out the other inputs omits a lot of information. It is
not easy to make a plot which takes all inputs into account. Is is
also not immediately obvious which of the variables are most
important.

In linear regression, we assume that the output depends on the
inputs in a linear (or more precisely, \emph{affine}) way. We write
this as
\begin{equation}
  y_i
  = \beta_0 + \beta_1 x_{i,1} + \cdots + \beta_p x_{i,p} + \varepsilon_i \label{eq:lmdata}
\end{equation}
where the residuals~\(\varepsilon_i\) are assumed to be ``small''.

The parameters \(\beta_j\) can be interpreted as the expected change in
the response \(y\) per unit change in \(x_j\) when all other regressor
variables are held fixed. For this reason the parameters \(\beta_j\)
(for \(j=1, \ldots, p\)) are sometimes called \emph{partial} regression
coefficients.

This model describes a hyperplane in the \((p+1)\)-dimensional space of
the inputs \(x_j\) and the output~\(y\). The hyperplane is easily
visualized when \(p=1\) (as a line in~\(\mathbb{R}^2\)), and visualisation can be
attempted for \(p=2\) (as a plane in \(\mathbb{R}^3\)) but is very hard for \(p>2\).

We defer making a proper statistical model for multiple linear regression
until the next section.

\hypertarget{the-normal-equations}{%
\subsection{The Normal Equations}\label{the-normal-equations}}

Similar to what we did in Section~\ref{sec:simple-mat}, we rewrite
the model using matrix notation. We define the vectors
\begin{equation*}
  y = \begin{pmatrix}
    y_1 \\ y_2 \\ \vdots \\ y_n
  \end{pmatrix}
  \in \mathbb{R}^n,
  \qquad
  \varepsilon= \begin{pmatrix}
    \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_n
  \end{pmatrix}
  \in \mathbb{R}^n,
  \qquad
  \beta = \begin{pmatrix}
    \beta_0 \\
    \beta_1 \\
    \vdots \\
    \beta_p
  \end{pmatrix}
  \in\mathbb{R}^{1+p}
\end{equation*}
as well as the matrix
\begin{equation*}
  X = \begin{pmatrix}
    1 & x_{1,1} & \cdots & x_{1,p} \\
    1 & x_{2,1} & \cdots & x_{2,p} \\
    \vdots & \vdots & & \vdots \\
    1 & x_{n,1} & \cdots & x_{n,p} \\
  \end{pmatrix}
  \in \mathbb{R}^{n\times (1+p)}.
\end{equation*}
The matrix \(X\) is often called the \textbf{design matrix}.

Using this notation, the model~\eqref{eq:lmdata} can be written
as
\begin{equation}
  y = X \beta + \varepsilon,  \label{eq:lmmodel}
\end{equation}
where again \(X\beta\) is a matrix-vector multiplication which ``hides''
the sums in equation~\eqref{eq:lmdata}, and \eqref{eq:lmmodel}
is an equation of vectors of size~\(n\), which combines the \(n\)
individual equations from \eqref{eq:lmdata} for the different
values of~\(i\).

To simplify notation, we index the columns of \(X\) by \(0, 1, \ldots, p\) (instead of the more conventional \(1, \ldots, p+1\)), so that we
can for example write
\begin{equation*}
  (X \beta)_i
  = \sum_{j=0}^p x_{i,j} \beta_j
  = \beta_0 + \sum_{j=1}^p x_{i,j} \beta_j.
\end{equation*}

As before, we find the regression coefficients by minimising
the residual sum of squares:
\begin{align*}
  r(\beta)
  &= \sum_{i=1}^n \varepsilon_i^2 \\
  &= \sum_{i=1}^n \bigl( y_i - (\beta_0 + \beta_1 x_{i,1} + \cdots + \beta_p x_{i,p}) \bigr)^2.
\end{align*}
In practice, this notation turns out to be cumbersome, and we will
use matrix notation in the following proof.

\begin{lemma}
\protect\hypertarget{lem:multiple-LSQ}{}\label{lem:multiple-LSQ}Assume that the matrix \(X^\top X \in \mathbb{R}^{(1+p) \times (1+p)}\) is
invertible. Then the function \(r(\beta)\) takes its minimum at the
vector \(\hat\beta\in\mathbb{R}^{p+1}\) given by
\begin{equation*}
  \hat\beta
  = (X^\top X)^{-1} X^\top y.
\end{equation*}
\end{lemma}

\begin{proof}
Using the vector equation \(\varepsilon= y - X \beta\), we can also write the
residual sum of squares as
\begin{align*}
    r(\beta)
    &= \sum_{i=1}^n \varepsilon_i^2 \\
    &= \varepsilon^\top \varepsilon\\
    &= (y - X \beta)^\top (y - X \beta) \\
    &= y^\top y - y^\top X\beta - (X\beta)^\top y + (X\beta)^\top (X\beta).
  \end{align*}
Using the linear algebra rules from Appendix~\ref{matrix-rules} we find
that \(y^\top X\beta = (X\beta)^\top y = \beta^\top X^\top y\)
and \((X\beta)^\top (X\beta) = \beta^\top X^\top X \beta\). Thus we get
\begin{equation*}
    r(\beta)
    = y^\top y - 2\beta^\top X^\top y + \beta^\top X^\top X \beta.
  \end{equation*}
Note that in this eqation \(X\) is a matrix, \(y\) and \(\beta\) are vectors,
and \(r(\beta)\) is a number.

To find the minimum of this function, we set all partial derivatives
\(\frac{\partial}{\partial \beta_i} r(\beta)\) equal to~\(0\). Going through the terms in
the formula for \(r(\beta)\) we find: (1) \(y^\top y\) does not depend on \(\beta\),
so we have \(\frac{\partial}{\partial \beta_i} y^\top y = 0\) for all \(i\), (2) we have
\begin{equation*}
    \frac{\partial}{\partial \beta_i} \beta^\top X^\top y
    = \frac{\partial}{\partial \beta_i} \sum_{j=1}^{p+1} \beta_j (X^\top y)_j
    = (X^\top y)_i
  \end{equation*}
and (3) finally
\begin{equation*}
    \frac{\partial}{\partial \beta_i} \beta^\top X^\top X \beta
    = \frac{\partial}{\partial \beta_i} \sum_{j,k=1}^{p+1} \beta_j (X^\top X)_{j,k} \beta_k
    = 2 \sum_{k=1}^{p+1} (X^\top X)_{i,k} \beta_k
    = 2 \bigl( (X^\top X) \beta \bigr)_i.
  \end{equation*}
(Some care is needed, when checking that the middle equality sign in
the previous equation is correct.)
Combining these derivatives, we find
\begin{equation}
    \frac{\partial}{\partial \beta_i} r(\beta)
    = 0 - 2 (X^\top y)_i + 2 \bigl( X^\top X \beta \bigr)_i
                           \label{eq:normal-first}
  \end{equation}
for all \(i \in \{0, 1, \ldots, p\}\). At a local minimum of \(r\),
all of these partial derivatives must be zero and using a vector
equation we find that a necessary condition for a minimum is
\begin{equation}
    X^\top X \beta = X^\top y.  \label{eq:normal-equations}
  \end{equation}
Since we assumed that \(X^\top X\) is invertible, there is exactly
one vector \(beta\) which solves \eqref{eq:normal-equations}. This
vector is given by
\begin{equation*}
    \hat\beta
    := (X^\top X)^{-1} X^\top y.
  \end{equation*}

As for one-dimensional minimisation, there is a condition on the
second derivatives which must be checked to see which local extrema
are local minima. Here we are only going to sketch this argument: A
sufficient condition for \(\hat\beta\) to be a minimum is for the
second derivative matrix (\href{https://en.wikipedia.org/wiki/Hessian_matrix}{the Hessian
matrix})
to be positive definite (see appendix~\ref{positive-definite}).
Using equation~\eqref{eq:normal-first} we find
\begin{equation*}
    \frac{\partial}{\partial\beta_i \partial\beta_j} r(\beta)
    = 2 (X^\top X)_{i,j}
  \end{equation*}
And thus the Hessian matrix is \(H = 2 X^\top X\). Using results from
linear algebra, one can show that this matrix is indeed positive
definite and thus \(\hat\beta\) is the unique minimum of~\(r\).
\end{proof}

Equation~\eqref{eq:normal-equations} gives a system of \(p+1\) linear
equations with \(p+1\) unknowns. This system of linear equations,
\(X^\top X \beta = X^\top y\) is called the \textbf{normal equations}.
If \(X^\top X\) is invertible, as assumed in the lemma, this
system of equations has \(\hat\beta\) as its unique solution.
Otherwise, there may be more than one \(\beta\) which leads to the
same value \(r(\beta)\) and the minimum will no longer be unique.
This happens for example, if two of the inputs are identical to
each other (or, more generally, one input is linearly dependent
on one or more other inputs).

The condition that \(X^\top X\) must be invertible in multiple linear
regression corresponds to the condition \(\mathrm{s}_x^2 > 0\) from
lemma~\ref{lem:simple-LSQ} for simple linear regression.

The value \(\hat\beta\) found in the lemma is called the \textbf{least squares
estimator} for \(\beta\), or sometimes the ordinary least squares (OLS)
estimator.

\hypertarget{fitted-values}{%
\subsection{Fitted Values}\label{fitted-values}}

Let us again consider our model
\begin{equation*}
  y = X \beta + \varepsilon,
\end{equation*}
using the matrix notation introduced above. Here we can think of
\(X\beta\) as the \textbf{true values}, while \(\varepsilon\) are the errors. The
design matrix \(X\) (containing the inputs) and the response \(y\) are
known to us, while the true coefficients \(\beta\) and the errors \(\varepsilon\)
are unknown. Solving for \(\varepsilon\) we find that the errors satisfy
\begin{equation*}
  \varepsilon= y - X\beta.
\end{equation*}

Using the least squares estimate \(\hat\beta\) we can estimate the true
values as
\begin{equation}
  \hat y = X \hat\beta.  \label{eq:fitted-values}
\end{equation}
These estimates are called the \textbf{fitted values}. Using the
definition of \(\hat\beta\) we get
\begin{equation*}
  \hat y
  = X (X^\top X)^{-1} X^\top y
  =: Hy.
\end{equation*}
The matrix \(H = X (X^\top X)^{-1} X^\top\) is commonly called the \textbf{hat
matrix} (because it ``puts the hat on \(y\)'').

Finally, we can estimate the errors using the residuals
\begin{equation}
  \hat\varepsilon
  = y - X \hat\beta
  = y - \hat y
  = y - H y
  = (I - H) y,  \label{eq:fitted-errors}
\end{equation}
where \(I\) is the \((p+1)\times (p+1)\) identity matrix.

\hypertarget{example}{%
\subsection{Example}\label{example}}

To conclude this section, we demonstrate how these methods can be used
in R. For this we consider the \texttt{mtcars} example from the beginning of
the section again. I will first show how to do the analysis ``by hand'',
and later show how the same result can be obtained using R's built-in functions.

We first split \texttt{mtcars} into the respons column \texttt{y} (the first column)
and the design matrix \texttt{X} (a column of ones, followed by columns 2 to 11
of \texttt{mtcars}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ mtcars[, }\DecValTok{1}\NormalTok{]}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FunctionTok{data.matrix}\NormalTok{(mtcars[, }\DecValTok{2}\SpecialCharTok{:}\DecValTok{11}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

Next we compute \(X^\top X\) and solve the normal equations. Often it is
faster, easier, and has lower numerical errors to solve the normal equations
rather than inverting the matrix \(X^\top X\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{XtX }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(X) }\SpecialCharTok{\%*\%}\NormalTok{ X}
\NormalTok{beta.hat }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(XtX, }\FunctionTok{t}\NormalTok{(X) }\SpecialCharTok{\%*\%}\NormalTok{ y)}
\NormalTok{beta.hat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             [,1]
##      12.30337416
## cyl  -0.11144048
## disp  0.01333524
## hp   -0.02148212
## drat  0.78711097
## wt   -3.71530393
## qsec  0.82104075
## vs    0.31776281
## am    2.52022689
## gear  0.65541302
## carb -0.19941925
\end{verbatim}

Without further checks it is hard to know whether the result is correct, or
whether we made a mistake somewhere along the lines. One good sign is that
we argued earlier that higher \texttt{hp} should lead to lower \texttt{mpg}, and indeed the
corresponding coefficient \texttt{-0.02148212} is negative.

Finally, compute the fitted values and generate a plot of fitted values
against responses. If everything worked, we would expect the points in this
plot to be close to the diagonal.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y.hat }\OtherTok{\textless{}{-}}\NormalTok{ X }\SpecialCharTok{\%*\%}\NormalTok{ beta.hat}
\FunctionTok{plot}\NormalTok{(y, y.hat, }\AttributeTok{xlab =} \StringTok{"responses"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"fitted values"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{a =} \DecValTok{0}\NormalTok{, }\AttributeTok{b =} \DecValTok{1}\NormalTok{) }\CommentTok{\# plot the diagonal}
\end{Highlighting}
\end{Shaded}

\includegraphics{MATH3714_files/figure-latex/y-y.hat-plot-1.pdf}

For comparison we now re-do the analysis using built-in R commands.
In the \texttt{lm()} command below, we use \texttt{data=mtcars} to tell R where the
data is stored, and the formula \texttt{mpg\ \textasciitilde{}\ .} states that we want to
model \texttt{mpg} as a function of all other variable (this is the meaning of \texttt{.}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ mtcars) }\CommentTok{\# fit a linear model}
\FunctionTok{coef}\NormalTok{(m) }\CommentTok{\# get the estimated coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)         cyl        disp          hp        drat          wt 
## 12.30337416 -0.11144048  0.01333524 -0.02148212  0.78711097 -3.71530393 
##        qsec          vs          am        gear        carb 
##  0.82104075  0.31776281  2.52022689  0.65541302 -0.19941925
\end{verbatim}

Comparing these coefficients to the vector \texttt{beta.hat} from above shows
that we got the same result using both methods. The fitted values
can be computed using \texttt{fitted.values(m)}. Here we just check that we get
the same result as above:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{max}\NormalTok{(}\FunctionTok{abs}\NormalTok{(}\FunctionTok{fitted.values}\NormalTok{(m) }\SpecialCharTok{{-}}\NormalTok{ y.hat))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.329071e-13
\end{verbatim}

This results \texttt{5.329071e-13} stands for the number \(5.329071 \cdot 10^{-13}\),
which is extremely small. The difference between our results and R's result
is caused by rounding errors.

\textbf{Summary}

\begin{itemize}
\tightlist
\item
  multiple linear regression allows for more than one input
  but still has only one output
\item
  the least squared estimate for the coefficients is found
  by minimising the residual sum of squares
\item
  the estimate can be computed as the solution to the normal equations
\item
  the hat matrix transforms responses into fitted values
\end{itemize}

\clearpage

\hypertarget{I01-lm}{%
\section*{Interlude: Linear Regression in R}\label{I01-lm}}
\addcontentsline{toc}{section}{Interlude: Linear Regression in R}

\hypertarget{lm-fitting}{%
\subsection*{Fitting a Model}\label{lm-fitting}}
\addcontentsline{toc}{subsection}{Fitting a Model}

The function \texttt{lm()} is used to fit a linear model in~R. There are different
ways to specify the form of the model and the data to be used for fitting the
model.

\begin{itemize}
\item
  The most basic way to call \texttt{lm()} is the case where the explanatory
  variables and the response variable are stored as separate vectors.
  Assuming, for example, that the explanatory variables are \texttt{x1}, \texttt{x2}, \texttt{x3}
  and that the response variable is~\texttt{y} in~R, we can tell R to fit the
  linear model \(y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \varepsilon\)
  by using the following command:

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+}\NormalTok{ x2 }\SpecialCharTok{+}\NormalTok{ x3)}
\end{Highlighting}
\end{Shaded}

  Note that R automatically added the intercept term \(\beta_0\) to this
  model. If we want to fit a model without an intercept,
  \emph{i.e.} the model
  \(y = \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \varepsilon\), we have to add
  \texttt{0\ +} in front of the explanatory variables:

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}} \DecValTok{0} \SpecialCharTok{+}\NormalTok{ x1 }\SpecialCharTok{+}\NormalTok{ x2 }\SpecialCharTok{+}\NormalTok{ x3)}
\end{Highlighting}
\end{Shaded}

  The general form of a model specification is the response variable,
  followed by \texttt{\textasciitilde{}}, followed by a plus-separated list of explanatory
  variables. For this form of calling \texttt{lm()}, the variables \texttt{y},
  \texttt{x1}, \texttt{x2}, and \texttt{x3} in the examples above must be already defined before
  \texttt{lm()} is called. It may be a good idea to double-check that the variables
  have the correct values before trying to call~\texttt{lm()}.
\item
  Both for the response and for explanatory variables we can
  specify arbitrary R expressions to compute the numeric values to be
  used. For example, to fit the model
  \(\log(y) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \varepsilon\) (assuming
  that all \(y_i\) are positive) we can use the following command:

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(y) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+}\NormalTok{ x2)}
\end{Highlighting}
\end{Shaded}

  Some care is needed, because \texttt{+}, \texttt{*} and \texttt{\^{}} have a
  special meaning inside the first argument of \texttt{lm()}; any time
  we want to compute a variable for \texttt{lm()} using these
  operations, we need to surround the corresponding expression with
  \texttt{I()}, to tell R that \texttt{+}, \texttt{*} or \texttt{\^{}} should
  have their usual, arithmetic meaning. For example, to fit a model
  of the form \(y \sim \beta_0 + \beta_1 x + \beta_2 x^2 + \varepsilon\), we
  can use the following R command:

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(x}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

  Here, the use of \texttt{I()} tells R that \texttt{x\^{}2} is to be
  interpreted as the vector \((x_1^2, \ldots, x_n^2)\). Similarly, we
  can fit a model of the form
  \(y = \beta_0 + \beta_1 (x_1 + x_2) + \varepsilon\):

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{I}\NormalTok{(x1}\SpecialCharTok{+}\NormalTok{x2))}
\end{Highlighting}
\end{Shaded}

  Here, the use of \texttt{I()} tells R that \texttt{x1+x2} indicates the
  vector \((x_{1,1}+x_{2,1}, \ldots, x_{1,n}+x_{2,n})\) instead of two
  separate explanatory variables.

  Details about how to specify models in calls to \texttt{lm()} can be found by
  using the \href{https://rdrr.io/r/stats/formula.html}{command \texttt{help(formula)}}
  in~R.
\item
  If the response and the explanatory variables are stored in the
  columns of a data frame, we can use the \texttt{data=...} argument to \texttt{lm()} to
  specify this data frame and then just use the column names to specify the
  regression model. For example, the
  \href{https://rdrr.io/r/datasets/stackloss.html}{\texttt{stackloss} data set}
  built into R consists of a data frame with columns \texttt{Air.Flow},
  \texttt{Water.Temp}, \texttt{Acid.Conc.}, \texttt{stack.loss}. To predict
  \texttt{stackloss\$stack.loss} from \texttt{stackloss\$Air.Flow} we can write

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{lm}\NormalTok{(stack.loss }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Air.Flow, }\AttributeTok{data=}\NormalTok{stackloss)}
\end{Highlighting}
\end{Shaded}

  As a special case, a single dot ``\texttt{.}'' can be used in place of
  the explanatory variables in the model to indicate that all columns
  except for the given response should be used. Thus, the following
  two commands are equivalent:

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{lm}\NormalTok{(stack.loss }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data=}\NormalTok{stackloss)}
  \FunctionTok{lm}\NormalTok{(stack.loss }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Air.Flow }\SpecialCharTok{+}\NormalTok{ Water.Temp }\SpecialCharTok{+}\NormalTok{ Acid.Conc., }\AttributeTok{data=}\NormalTok{stackloss)}
\end{Highlighting}
\end{Shaded}
\end{itemize}

\hypertarget{lm-model}{%
\subsection*{Understanding the Model}\label{lm-model}}
\addcontentsline{toc}{subsection}{Understanding the Model}

The output of the \texttt{lm()} function is an R object which can be used the extract
information about the fitted model. A good way to work with this object is to
store it in a variable and then use commands like the ones listed below to work
with this variable. For example, the following R command fits a model for the
\texttt{stackloss} data set and stores it in the variable~\texttt{m}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  m }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(stack.loss }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data=}\NormalTok{stackloss)}
\end{Highlighting}
\end{Shaded}

Many operations are available to use with this object~\texttt{m}:

\begin{itemize}
\item
  Printing \texttt{m} to the screen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  m}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = stack.loss ~ ., data = stackloss)
## 
## Coefficients:
## (Intercept)     Air.Flow   Water.Temp   Acid.Conc.  
##    -39.9197       0.7156       1.2953      -0.1521
\end{verbatim}

  This shows the estimated values for the regression coefficient.
\item
  The command \texttt{summary()} can be used to print additional
  information about the fitted model:

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{summary}\NormalTok{(m)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = stack.loss ~ ., data = stackloss)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.2377 -1.7117 -0.4551  2.3614  5.6978 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -39.9197    11.8960  -3.356  0.00375 ** 
## Air.Flow      0.7156     0.1349   5.307  5.8e-05 ***
## Water.Temp    1.2953     0.3680   3.520  0.00263 ** 
## Acid.Conc.   -0.1521     0.1563  -0.973  0.34405    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.243 on 17 degrees of freedom
## Multiple R-squared:  0.9136, Adjusted R-squared:  0.8983 
## F-statistic:  59.9 on 3 and 17 DF,  p-value: 3.016e-09
\end{verbatim}

  We will learn over the course of this module how to interpret
  most of this output.
\item
  The coefficient vector~\(\beta\) can be obtained using
  \texttt{coef(m)}:

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{coef}\NormalTok{(m)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)    Air.Flow  Water.Temp  Acid.Conc. 
## -39.9196744   0.7156402   1.2952861  -0.1521225
\end{verbatim}
\item
  The fitted values \(\hat y_i\) can be obtained using
  the command \texttt{fitted(m)}:

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{fitted}\NormalTok{(m)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         1         2         3         4         5         6         7         8 
## 38.765363 38.917485 32.444467 22.302226 19.711654 21.006940 21.389491 21.389491 
##         9        10        11        12        13        14        15        16 
## 18.144379 12.732806 11.363703 10.220540 12.428561 12.050499  5.638582  6.094949 
##        17        18        19        20        21 
##  9.519951  8.455093  9.598257 13.587853 22.237713
\end{verbatim}
\item
  The estimated residuals \(\hat\varepsilon_i = y_i - \hat y_i\) can
  be obtained using the command \texttt{resid(m)}:

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{resid}\NormalTok{(m)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           1           2           3           4           5           6 
##  3.23463723 -1.91748529  4.55553300  5.69777417 -1.71165358 -3.00693970 
##           7           8           9          10          11          12 
## -2.38949071 -1.38949071 -3.14437890  1.26719408  2.63629676  2.77946036 
##          13          14          15          16          17          18 
## -1.42856088 -0.05049929  2.36141836  0.90505080 -1.51995059 -0.45509295 
##          19          20          21 
## -0.59825656  1.41214728 -7.23771286
\end{verbatim}
\item
  The design matrix \(X\) can be found using \texttt{model.matrix(m)}:

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{model.matrix}\NormalTok{(m)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    (Intercept) Air.Flow Water.Temp Acid.Conc.
## 1            1       80         27         89
## 2            1       80         27         88
## 3            1       75         25         90
## 4            1       62         24         87
## 5            1       62         22         87
## 6            1       62         23         87
## 7            1       62         24         93
## 8            1       62         24         93
## 9            1       58         23         87
## 10           1       58         18         80
## 11           1       58         18         89
## 12           1       58         17         88
## 13           1       58         18         82
## 14           1       58         19         93
## 15           1       50         18         89
## 16           1       50         18         86
## 17           1       50         19         72
## 18           1       50         19         79
## 19           1       50         20         80
## 20           1       56         20         82
## 21           1       70         20         91
## attr(,"assign")
## [1] 0 1 2 3
\end{verbatim}
\end{itemize}

\hypertarget{lm-predict}{%
\subsection*{Making Predictions}\label{lm-predict}}
\addcontentsline{toc}{subsection}{Making Predictions}

One of the main aims of fitting a linear model is to use the model to make
predictions for new, not previously observed \(x\)-values, \emph{i.e.}~to compute
\(y_{\mathrm{new}} = X_{\mathrm{new}} \hat\beta\). The general form of the
command for prediction is \texttt{predict(m,\ newdata)}, where \texttt{m} is the model
previously fitted using \texttt{lm()}, and \texttt{newdata} specifies the new \(x\)-values to
predict responses for. The argument \texttt{new.data} should be a \texttt{data.frame} and
for each variable in the original model there should be a column in \texttt{newdata}
which has the name of the original variable and contains the new values. For
example, if the model was fitted using

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  m }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(x}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

and if the new samples are stored in \texttt{x.new}, then responses for
the \(x\)-values in \texttt{x.new} can be predicted using the following
command:

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{predict}\NormalTok{(m, }\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x=}\NormalTok{x.new))}
\end{Highlighting}
\end{Shaded}

As a second example, for the \texttt{stackloss} data set, the following
commands can be used to predict \texttt{stack.loss} for two new
\(x\)-values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  m }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(stack.loss }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data=}\NormalTok{stackloss)}
\NormalTok{  new.data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Air.Flow=}\FunctionTok{c}\NormalTok{(}\DecValTok{70}\NormalTok{, }\DecValTok{73}\NormalTok{), }\AttributeTok{Water.Temp=}\FunctionTok{c}\NormalTok{(}\DecValTok{25}\NormalTok{,}\DecValTok{24}\NormalTok{), }\AttributeTok{Acid.Conc.=}\FunctionTok{c}\NormalTok{(}\DecValTok{78}\NormalTok{,}\DecValTok{90}\NormalTok{))}
  \FunctionTok{predict}\NormalTok{(m, new.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        1        2 
## 30.69174 29.71790
\end{verbatim}

More information about \texttt{predict()} can be found by reading
the \href{https://rdrr.io/r/stats/predict.html}{output of \texttt{help(predict.lm)}}.

\clearpage

\hypertarget{P01}{%
\section*{Problem Sheet 1}\label{P01}}
\addcontentsline{toc}{section}{Problem Sheet 1}

\excludecomment{myanswers}

You should attempt all these questions and write up your solutions in advance
of your workshop in week 3 where the answers will be discussed.

\textbf{1} Consider the simple linear regression model
\(y_i = \beta_0 + x_{i} \beta_1 + \varepsilon_i\) for
\(i \in \{1, 2, \ldots, n\}\) and let \(X\) be the design matrix.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Show that \(\displaystyle X^\top X = \begin{pmatrix}  n & \sum_{i=1}^n x_i \\  \sum_{i=1}^n x_i & \sum_{i=1}^n x_i^2  \end{pmatrix} \in \mathbb{R}^{2\times 2}\).
\end{enumerate}

\begin{myanswers}
For simple linear regression, we have \(p=1\) and the design matrix is
\begin{equation*}
    X = \begin{pmatrix}
      1 & x_1 \\
      1 & x_2 \\
      \vdots & \vdots \\
      1 & x_n
    \end{pmatrix}.
  \end{equation*}
Thus we have
\begin{align*}
    X^\top X
    &= \begin{pmatrix}
        1 & 1 & \cdots & 1 \\
        x_1 & x_2 & \cdots & x_n
    \end{pmatrix} \begin{pmatrix}
      1 & x_1 \\
      1 & x_2 \\
      \vdots & \vdots \\
      1 & x_n
    \end{pmatrix} \\
    &= \begin{pmatrix}
      \sum_{i=1}^n 1 &  \sum_{i=1}^n 1 \cdot x_i \\
      \sum_{i=1}^n x_i \cdot 1 & \sum_{i=1}^n x_i^2
    \end{pmatrix} \\
    &= \begin{pmatrix}
      n & \sum_{i=1}^n x_i \\
      \sum_{i=1}^n x_i & \sum_{i=1}^n x_i^2
    \end{pmatrix}.
  \end{align*}

\end{myanswers}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Using the formula
  \begin{equation*}
   \begin{pmatrix} a & b \\ c & d \end{pmatrix}^{-1}
   = \frac{1}{ad-bc} \begin{pmatrix} \;d & -b \\ -c & \,a \end{pmatrix},
      \end{equation*}
  find \((X^\top X)^{-1}\).
\end{enumerate}

\begin{myanswers}
Using the formula for the inverse of a \(2\times 2\)-matrix, we find
\begin{align*}
    (X^\top X)^{-1}
    &= \begin{pmatrix}
      n & \sum_{i=1}^n x_i \\
      \sum_{i=1}^n x_i & \sum_{i=1}^n x_i^2
    \end{pmatrix}^{-1} \\
    &= \frac{1}{n \sum_{i=1}^n x_i^2 - \bigl(\sum_{i=1}^n x_i\bigr)^2}
      \begin{pmatrix}
      \sum_{i=1}^n x_i^2 & -\sum_{i=1}^n x_i \\
      -\sum_{i=1}^n x_i & n
    \end{pmatrix}.
  \end{align*}

\end{myanswers}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Find \(X^\top y\) and use this to derive an explicit formula for
  the least squares estimate \(\hat\beta = (X^\top X)^{-1} X^\top y\).
\end{enumerate}

\begin{myanswers}
Omitting the indices in the sums for brevity, we have
\begin{equation*}
    X^\top y
    = \begin{pmatrix}
        1 & 1 & \cdots & 1 \\
        x_1 & x_2 & \cdots & x_n
    \end{pmatrix} \begin{pmatrix}
      y_1 \\ y_2 \\ \vdots \\ y_n
    \end{pmatrix}
    = \begin{pmatrix}
      \sum y_i \\
      \sum x_i y_i
    \end{pmatrix}
  \end{equation*}
and thus
\begin{align*}
    \hat\beta
    &= (X^\top X)^{-1} X^\top y \\
    &= \frac{1}{n \sum x_i^2 - \bigl(\sum x_i\bigr)^2}
      \begin{pmatrix}
      \sum x_i^2 & -\sum x_i \\
      -\sum x_i & n
    \end{pmatrix}
    \begin{pmatrix}
      \sum y_i \\
      \sum x_i y_i
    \end{pmatrix} \\
    &= \frac{1}{n \sum x_i^2 - \bigl(\sum x_i\bigr)^2}
      \begin{pmatrix}
        \sum x_i^2 \sum y_i - \sum x_i \sum x_i y_i \\
        -\sum x_i \sum y_i + n \sum x_i y_i
      \end{pmatrix} \\
    &= \frac{1}{\frac1n \sum_{i=1}^n x_i^2 - \bigl(\frac1n \sum_{i=1}^n x_i\bigr)^2}
      \begin{pmatrix}
        \frac1n \sum_{i=1}^n x_i^2 \cdot \frac1n\sum_{i=1}^n y_i
            - \frac1n\sum_{i=1}^n x_i \cdot \frac1n\sum_{i=1}^n x_i y_i \\
        \frac1n\sum_{i=1}^n x_i y_i
            - \frac1n\sum_{i=1}^n x_i \cdot \frac1n\sum_{i=1}^n y_i
      \end{pmatrix}.
  \end{align*}
This completes the answer.

Inspection of the final result shows that we have recovered the traditional
formula for the coefficients in simple linear regression, only written in
slightly unusual form. For example, the term \(\frac1n \sum_{i=1}^n x_i^2 -  \bigl(\frac1n \sum_{i=1}^n x_i\bigr)^2\) equals the sample variance of the
\(x_i\) (up to a factor of \(n/(n-1)\)). The algebra in this answer could be
slightly simplified by changing to new coordinates \(\tilde x_i = x_i - \bar  x\) and \(\tilde y_i = y_i - \bar y\) before fitting the regression model.

\end{myanswers}

\textbf{2} Let \(H = X (X^\top X)^{-1} X^\top \in\mathbb{R}^{n\times n}\) be the hat matrix
and \(\mathbf{1} = (1, 1, \ldots, 1) \in\mathbb{R}^n\). Show that \(H \mathbf{1} = \mathbf{1}\).

\begin{myanswers}
We already know that \(H X = X (X^\top X)^{-1} X^\top X = X\). Since
the first column of \(X\) equals \(\mathbf{1}\), the first column of
the matrix equation \(HX = X\) is \(H\mathbf{1} = \mathbf{1}\). This
completes the proof.

\end{myanswers}

\textbf{3} For the \href{https://rdrr.io/r/datasets/stackloss.html}{\texttt{stackloss} data set}
built into R, predict a value for \texttt{stack.loss} when the inputs are
\texttt{Air.Flow\ =\ 60}, \texttt{Water.Temp\ =\ 21} and \texttt{Acid.Conc\ =\ 87}.

\begin{myanswers}
We can fit the model using \texttt{lm()} as usual:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(stack.loss }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ stackloss)}
\NormalTok{m}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = stack.loss ~ ., data = stackloss)
## 
## Coefficients:
## (Intercept)     Air.Flow   Water.Temp   Acid.Conc.  
##    -39.9197       0.7156       1.2953      -0.1521
\end{verbatim}

To predict a new value, we use the \texttt{predict()} command. Note that
\texttt{Acid.Conc.} is spelled with a trailing dot, which we need to include
in the name when we supply the new input values here.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{predict}\NormalTok{(m, }\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Air.Flow =} \DecValTok{60}\NormalTok{, }\AttributeTok{Water.Temp =} \DecValTok{21}\NormalTok{, }\AttributeTok{Acid.Conc. =} \DecValTok{87}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        1 
## 16.98509
\end{verbatim}

Thus, the predicted value for \texttt{stack.loss} is \(16.98509\).

\end{myanswers}

\textbf{4} Let \(\varepsilon_1, \ldots, \varepsilon_n \sim \mathcal{N}(\mu, \sigma^2)\) be independent.
Then \(\varepsilon= (\varepsilon_1, \ldots, \varepsilon_n)\) is a random vector. Determine
\(\mathbb{E}(\varepsilon)\), \(\mathop{\mathrm{Cov}}(\varepsilon)\) and \(\mathbb{E}\bigl( \|\varepsilon\|^2 \bigr)\).

\begin{myanswers}
We immediately find \(\mathbb{E}(\varepsilon) = \mu \mathbf{1}\) where
\(\mathbf{1} = (1, \ldots, 1) \in\mathbb{R}^n\). Since the \(\varepsilon_i\) are independent,
we have \(\mathop{\mathrm{Cov}}(\varepsilon) = \sigma^2 I\). Finally we have
\begin{align*}
  \mathbb{E}\bigl(\|\varepsilon\|^2\bigr)
  &= \mathbb{E}\bigl( \sum_{i=1}^n \varepsilon_i^2 \bigr) \\
  &= \sum_{i=1}^n E(\varepsilon_i^2) \\
  &= n \mathbb{E}(\varepsilon_1^2).
\end{align*}
Since \(\mathbb{E}(\varepsilon_1^2) = \mathop{\mathrm{Var}}(\varepsilon_1) + \mathbb{E}(\varepsilon_1)^2 = \sigma^2 + \mu^2\)
we get \(\mathbb{E}\bigl(\|\varepsilon\|^2\bigr) = n (\sigma^2 + \mu^2)\).

\end{myanswers}

\clearpage

\hypertarget{S03-cov}{%
\section{Random Vectors and Covariance}\label{S03-cov}}

Like in the one-dimensional case, we can build a \textbf{statistical model}
for the data where we assume that the errors are random. More
precisely we will assume
\begin{equation}
  Y_i
  = \beta_0 + \beta_1 x_{i,1} + \cdots + \beta_p x_{i,p} + \varepsilon_i
\end{equation}
for all \(i \in \{1, 2, \ldots, n\}\), where \(\varepsilon_1, \ldots, \varepsilon_n\)
are now independent and identically distributed (i.i.d.)
random variables with \(\mathbb{E}(\varepsilon_i) = 0\) and
\(\mathop{\mathrm{Var}}(\varepsilon_i) = \sigma^2\).
As in \eqref{eq:lmmodel}, the statistical model can be written in vector
form as
\begin{equation}
  Y = X \beta + \varepsilon.  \label{eq:lmstat0}
\end{equation}
This is a vector-valued equation which contains two ``random vectors'', \(Y\)
and~\(\varepsilon\).

A \textbf{random vector} is a vector \(Z = (Z_1, \ldots, Z_n)\)
where each component \(Z_i\) is a random variable.

\hypertarget{expectation}{%
\subsection{Expectation}\label{expectation}}

The expectation of a random vector is taken for each component separately.
This is formalised in the following definition.

\begin{definition}
Let \(Z = (Z_1, \ldots, Z_n) \in \mathbb{R}^n\) be a random vector.
Then the expectation of \(Z\) is the (non-random) vector
\begin{equation*}
    \mathbb{E}(X)
    = \begin{pmatrix}
      \mathbb{E}(Z_1) \\ \vdots \\ \mathbb{E}(Z_n)
    \end{pmatrix}
    \in \mathbb{R}^n.
  \end{equation*}
\end{definition}

The same convention is sometimes used for random matrices \(M\),
as \(\mathbb{E}(M)_{ij} = \mathbb{E}(M_{ij})\).

\begin{example}
The random vector~\(\varepsilon\) in \eqref{eq:lmstat0} has
\begin{equation*}
    \mathbb{E}(\varepsilon)_i = \mathbb{E}(\varepsilon_i) = 0
  \end{equation*}
for all \(i \in \{1, \ldots, n\}\) and thus \(\mathbb{E}(\varepsilon) = 0 \in \mathbb{R}^n\),
where \(0\) here denotes the zero-vector \((0, \ldots, 0) \in \mathbb{R}^n\).
\end{example}

Since the expectation of a random vector is defined in term of the
usual expectation, most rules we know for expectations still hold.
For example, if \(Y\) and \(Z\) are two random vectors, we have
\(\mathbb{E}(Y+Z) = \mathbb{E}(Y) + \mathbb{E}(Z)\).

\begin{example}
\protect\hypertarget{exm:E-of-Y}{}\label{exm:E-of-Y}The random vector~\(Y\) in \eqref{eq:lmstat0} has
\begin{equation*}
    \mathbb{E}(Y)_i
    = \mathbb{E}(Y_i)
    = \mathbb{E}\bigl( (X\beta)_i + \varepsilon_i \bigr)
    = (X\beta)_i + \mathbb{E}(\varepsilon_i)
    = (X\beta)_i
  \end{equation*}
for all \(i \in \{1, \ldots, n\}\) and thus \(\mathbb{E}(Y) = X\beta \in \mathbb{R}^n\).
We often will write the above derivation in vector form
as
\begin{equation*}
    \mathbb{E}(Y)
    = \mathbb{E}(X\beta + \varepsilon)
    = X\beta + \mathbb{E}(\varepsilon)
    = X\beta.
  \end{equation*}
\end{example}

\begin{example}
If \(A \in \mathbb{R}^{m\times n}\) is a matrix and \(Z \in \mathbb{R}^n\) is a random
vector, then we find the expectation of \(AZ\in\mathbb{R}^m\) as
\begin{equation*}
    \mathbb{E}(AZ)_i
    = \mathbb{E}(AZ_i)
    = \mathbb{E}\bigl( \sum_{j=1}^n a_{ij} Z_j \bigr)
    = \sum_{j=1}^n \mathbb{E}\bigl( a_{ij} Z_j \bigr)
    = \sum_{j=1}^n a_{ij} \mathbb{E}(Z_j)
    = \sum_{j=1}^n a_{ij} \mathbb{E}(Z)_j
  \end{equation*}
for all \(i \in \{1, \ldots, m\}\) and thus we have \(\mathbb{E}(AZ) = A \mathbb{E}(Z)\).
\end{example}

\hypertarget{sec:covariance}{%
\subsection{Covariance Matrix}\label{sec:covariance}}

The variance of random variables is replaced with the concept of
a ``covariance matrix'' for random vectors.

\begin{definition}
Let \(Z = (Z_1, \ldots, Z_n) \in \mathbb{R}^n\) be a random vector.
Then the covariance matrix of \(Z\) is the matrix \(\mathop{\mathrm{Cov}}(Z) \in \mathbb{R}^{n\times n}\)
given by
\begin{equation*}
    \mathop{\mathrm{Cov}}(Z)_{ij}
    = \mathop{\mathrm{Cov}}(Z_i, Z_j),
  \end{equation*}
for all \(i, j \in \{1, \ldots, n\}\),
where \(\mathop{\mathrm{Cov}}(Z_i, Z_j)\) denotes the usual covariance between random
variables.
\end{definition}

We collect some basic properties of covariance matrices here. Most of these
arguments use concepts and rules from linear algebra, as summarised in
section~\ref{Sx1-matrices} in the appendix.

\begin{itemize}
\item
  Since \(\mathop{\mathrm{Cov}}(Z_i, Z_j) = \mathop{\mathrm{Cov}}(Z_j, Z_i)\), covariance matrices are
  symmetric.
\item
  The diagonal elements of \(\mathop{\mathrm{Cov}}(Z)\) are
  \begin{equation}
    \mathop{\mathrm{Cov}}(Z)_{ii}
    = \mathop{\mathrm{Cov}}(Z_i, Z_i)
    = \mathop{\mathrm{Var}}(Z_i).  \label{eq:Cov-diag-elem}
  \end{equation}
\item
  If the elements \(Z_i\) of \(Z\) are (statistically) independent,
  we have \(\mathop{\mathrm{Cov}}(Z_i, Z_j) = 0\) and thus \(\mathop{\mathrm{Cov}}(Z)_{ij} = 0\)
  for \(i \neq j\). If \(Z\) is a vector of independent random variables,
  the covariance matrix of \(Z\) is diagonal.
\item
  Let \(\mu = \mathbb{E}(Z) \in \mathbb{R}^n\). If we interpret \(\mu\) as a column vector,
  then \(M = (Z - \mu) (Z - \mu)^\top\) is an \(n\times n\) matrix and we have
  \begin{equation*}
    M_{ij}
    = \bigl( (Z - \mu) (Z - \mu)^\top \bigr)_{ij}
    = (Z - \mu)_i (Z - \mu)_j.
  \end{equation*}
  Taking expectations gives \(\mathbb{E}(M_{ij}) = E\bigl( (Z - \mu)_i (Z - \mu)_j \bigr) = \mathop{\mathrm{Cov}}(Z_i, Z_j)\) and thus we can write
  \begin{equation}
    \mathop{\mathrm{Cov}}(Z)
    = \mathbb{E}\bigl( (Z - \mu) (Z - \mu)^\top \bigr).  \label{eq:cov-prod}
  \end{equation}
\item
  Covariance matrices are positive semi-definite. To see this, let
  \(C = \mathop{\mathrm{Cov}}(Z)\) and \(u \in\mathbb{R}^n\) be a vector. We have to show that
  \(u^\top C u \geq 0\). Writing \(\bar Z := Z - \mathbb{E}(Z)\) as an abbreviation,
  we get
  \begin{align*}
    u^\top C u
    &= u^\top \mathbb{E}\bigl( \bar Z \bar Z^\top \bigr) u \\
    &= \mathbb{E}\bigl( u^\top \bar Z \bar Z^\top u \bigr) \\
    &= \mathbb{E}\bigl( (\bar Z^\top u)^\top \bar Z^\top u \bigr) \\
    &= \mathbb{E}\bigl( \|\bar Z^\top u\|^2 \bigr),
  \end{align*}
  where \(\|\bar Z^\top u\|\) denotes the Euclidean length of the
  vector \(\bar Z^\top u\). Since \(\|\bar Z^\top u\|^2 \geq 0\)
  we find \(u^\top C u \geq 0\). This shows that the covariance matrix~\(C\)
  is positive semi-definite. (Note that, nevertheless,
  individual \emph{elements} of the matrix \(C\) can be negative numbers.)
\end{itemize}

\begin{example}
The random vector \(\varepsilon\) in equation~\eqref{eq:lmstat0} has \(\mathbb{E}(\varepsilon) = 0\).
We have
\(\mathop{\mathrm{Cov}}(\varepsilon)_{ii} = \mathop{\mathrm{Var}}(\varepsilon_i) = \sigma^2\) for all \(i\in\{1, \ldots, n\}\).
Since we assumed the \(\varepsilon_i\) to be independent, the covariance matrix is
diagonal and we find
\begin{equation*}
  \mathop{\mathrm{Cov}}(\varepsilon) = \sigma^2 I,
\end{equation*}
where \(I\) is the \(n\times n\) identity matrix.
\end{example}

An important results about covariance matrices is given in the following
lemma, which describes how the covariance matrix changes under affine
transformations.

\begin{lemma}
\protect\hypertarget{lem:Cov-is-quadratic}{}\label{lem:Cov-is-quadratic}Let \(Z\in\mathbb{R}^n\) be a random vector, \(A\in\mathbb{R}^{m\times n}\) a matrix
and \(b\in\mathbb{R}^m\) a vector. Then
\begin{equation*}
  \mathop{\mathrm{Cov}}(AZ+b)
  = A \mathop{\mathrm{Cov}}(Z) A^\top.
\end{equation*}
\end{lemma}

\begin{proof}
As in equation~\eqref{eq:cov-prod}, we can write \(\mathop{\mathrm{Cov}}(AZ+b)\)
as
\begin{equation*}
  \mathop{\mathrm{Cov}}(AZ+b)
  = \mathbb{E}\bigl( (AZ + b - \mu) (AZ + b - \mu)^\top \bigr),
\end{equation*}
where \(\mu = \mathbb{E}(AZ + b) = \mathbb{E}(AZ) + b\). Thus,
\(AZ + b - \mu = AZ - \mathbb{E}(AZ)\) and we find
\begin{align*}
  \mathop{\mathrm{Cov}}(AZ+b)
  &= \mathbb{E}\bigl( (AZ - \mathbb{E}(AZ)) (AZ - \mathbb{E}(AZ))^\top \bigr) \\
  &= \mathop{\mathrm{Cov}}(AZ).
\end{align*}
This shows that the covariance matrix ignores non-random shifts.

Furthermore, we have \(AZ - \mathbb{E}(AZ) = AZ - A\mathbb{E}(Z) = A\bigl(Z - \mathbb{E}(Z)\bigr)\).
Using equation~\eqref{eq:cov-prod} again, we find
\begin{align*}
  \mathop{\mathrm{Cov}}(AZ)
  &= \mathbb{E}\Bigl( \bigl(AZ - \mathbb{E}(AZ)\bigr) \bigl(AZ - \mathbb{E}(AZ)\bigr)^\top \Bigr) \\
  &= \mathbb{E}\Bigl( A \bigl(Z - \mathbb{E}(Z)\bigr) \bigl(Z - \mathbb{E}(Z)\bigr)^\top A^\top \Bigr) \\
  &= A \mathbb{E}\Bigl( \bigl(Z - \mathbb{E}(Z)\bigr) \bigl(Z - \mathbb{E}(Z)\bigr)^\top \Bigr) A^\top \\
  &= A \mathop{\mathrm{Cov}}(Z) A^\top.
\end{align*}
This completes the proof.
\end{proof}

\hypertarget{the-multivariate-normal-distribution}{%
\subsection{The Multivariate Normal Distribution}\label{the-multivariate-normal-distribution}}

Since we assume that the random errors \(\varepsilon_i\) are normally
distributed, we will need to understand how vectors of normal distributed
random variables behave.

\begin{definition}
A random vector \(Z\in\mathbb{R}^n\) follows a \textbf{multivariate normal distribution},
if \(u^\top Z\) is normally distributed or constant for every
vector~\(u\in\mathbb{R}^n\).
\end{definition}

This definition is takes its slightly surprising form to avoid
some boundary cases which I will discuss in an example, below.
To understand the
definition, a good start is to consider the cases where \(u\)
is one of the standard basis vectors, say \(u_i = 1\) and \(u_j = 0\)
for all \(j\neq i\). In this case we have
\begin{equation*}
  u^\top Z
  = \sum_{k=1}^n u_k Z_k
  = Z_i.
\end{equation*}
Thus, if \(Z\) follows a multivariate normal distribution, each of the
components \(Z_i\) is normally distributed. Example \ref{exm:not-normal},
below, shows that the converse is not true.

One can show that a multivariate normal distribution is completely determined
by the mean \(\mu = \mathbb{E}(Z)\) and the covariance \(\Sigma = \mathop{\mathrm{Cov}}(Z)\). The
distribution of such a \(Z\) is denoted by \(\mathcal{N}(\mu, \Sigma)\). Also, for every
\(\mu\in\mathbb{R}^n\) and every positive semi-definite matrix \(\Sigma\in\mathbb{R}^{n\times n}\)
there is a random vector \(Z\) which follows a multivariate normal distribution
with this mean and covariance.

\begin{example}
Consider the vector~\(\varepsilon\) from the model \eqref{eq:lmstat0}.
This vector has components \(\varepsilon_i \sim \mathcal{N}(0, \sigma^2)\) and by
assumption, the componens \(\varepsilon_i\) are independent.
For \(u\in\mathbb{R}^n\) we have
\begin{equation*}
  u^\top \varepsilon
  = \sum_{i=1}^n u_i \varepsilon_i.
\end{equation*}
Since this is a sum of independent, one-dimensional, normally distributed
random variables, \(u^\top \varepsilon\) is also normally distribution, for
every~\(u\). (The independence of the \(\varepsilon_i\) is important in this
argument.) Thus, \(\varepsilon\) is a normally distributed random vector.
We have already seen \(\mathbb{E}(\varepsilon) = 0\) and \(\mathop{\mathrm{Cov}}(\varepsilon) = \sigma^2 I\),
and thus \(\varepsilon\sim \mathcal{N}(0, \sigma^2 I)\).
\end{example}

Without proof we state here some properties of the multivariate normal
distribution:

\begin{itemize}
\item
  If \(Z \sim \mathcal{N}(\mu, \Sigma)\) and \(a \in \mathbb{R}^n\), then
  \(Z + a \sim \mathcal{N}(\mu + a, \Sigma)\).
\item
  If \(Z \sim \mathcal{N}(\mu, \Sigma)\) and \(A \in \mathbb{R}^{m\times n}\), then
  \(AZ \sim \mathcal{N}(A\mu, A\Sigma A^\top)\).
\item
  If \(Z_1 \sim \mathcal{N}(\mu_1, \Sigma_1)\) and \(Z_2 \sim \mathcal{N}(\mu_2, \Sigma_2)\)
  are independent, then
  \(Z_1 + Z_2 \sim \mathcal{N}(\mu_1 + \mu_2, \Sigma_1 + \Sigma_2)\).
\end{itemize}

\begin{example}
Let \(Z = (Z_1, Z_2)\) where \(Z_1\) and \(Z_2\) are independently standard
normal distributed. Let
\begin{equation*}
  A := \begin{pmatrix}
    2 & -1 \\
    2 & 1
  \end{pmatrix}
  \qquad \mbox{and} \qquad
  b := \begin{pmatrix}
    3 \\ 4
  \end{pmatrix}.
\end{equation*}
Then \(AZ + b \sim \mathcal{N}(b, \Sigma)\) where
\begin{equation*}
  \Sigma
  = A \mathop{\mathrm{Cov}}(Z) A^\top
  = \begin{pmatrix}
      2 & -1 \\
      2 & 1
    \end{pmatrix}
    \begin{pmatrix}
      1 & 0 \\
      0 & 1
    \end{pmatrix}
    \begin{pmatrix}
      2 & 2 \\
      -1 & 1
    \end{pmatrix}
  = \begin{pmatrix}
      5 & 3 \\
      3 & 5
    \end{pmatrix}
\end{equation*}

We can use R to plot a sample of this two-dimensional normal distribution.
(The grey cross indicates the mean.)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N }\OtherTok{\textless{}{-}} \DecValTok{500}
\NormalTok{Z }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(N), }\FunctionTok{rnorm}\NormalTok{(N))}
\NormalTok{A }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{b }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{V }\OtherTok{\textless{}{-}}\NormalTok{ A }\SpecialCharTok{\%*\%}\NormalTok{ Z }\SpecialCharTok{+}\NormalTok{ b}
\FunctionTok{plot}\NormalTok{(V[}\DecValTok{1}\NormalTok{,], V[}\DecValTok{2}\NormalTok{,], }\AttributeTok{asp =} \DecValTok{1}\NormalTok{, }\AttributeTok{cex =}\NormalTok{ .}\DecValTok{5}\NormalTok{,}
     \AttributeTok{xlab =} \FunctionTok{expression}\NormalTok{(V[}\DecValTok{1}\NormalTok{]),}
     \AttributeTok{ylab =} \FunctionTok{expression}\NormalTok{(V[}\DecValTok{2}\NormalTok{]))}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v =} \DecValTok{3}\NormalTok{, }\AttributeTok{col =} \StringTok{"grey"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{h =} \DecValTok{4}\NormalTok{, }\AttributeTok{col =} \StringTok{"grey"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{MATH3714_files/figure-latex/MVN-plot-1.pdf}
\end{example}

\begin{example}
The random vector \(Y\) in \eqref{eq:lmstat0} satisfies \(Y = X \beta + \varepsilon\)
and since \(\varepsilon\sim \mathcal{N}(0, \sigma^2 I)\) we have
\(Y \sim \mathcal{N}(X\beta, \sigma^2 I)\).
\end{example}

\begin{example}
\protect\hypertarget{exm:not-normal}{}\label{exm:not-normal}Let \(Y \sim \mathcal{N}(0, 1)\) be a random variable. Define a random vector
\(Z = (Z_1, Z_2)\) as \(Z_1 = Y\) and
\begin{equation*}
  Z_2 = \begin{cases}
    Y & \mbox{if $|Y|<1$, and}\\
    -Y & \mbox{otherwise.}
  \end{cases}
\end{equation*}
Clearly \(Z_1\) is standard normally distributed.
Since \(\mathcal{N}(0,1)\) is symmetric, both \(Y\) and \(-Y\) are standard normally
distributed and it follows that \(Z_2\) is also standard normally distributed.
Nevertheless, the random vector \(Z\) does not follow a multivariate
normal distribution. Instead of giving a proof of this fact,
we illustrate this here using an R experiment. We start by verifying
that \(Z_1\) and \(Z_2\) are normally distributed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{Y }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(N)}
\NormalTok{Z1 }\OtherTok{\textless{}{-}}\NormalTok{ Y}
\NormalTok{Z2 }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{abs}\NormalTok{(Y)}\SpecialCharTok{\textless{}}\DecValTok{1}\NormalTok{, Y, }\SpecialCharTok{{-}}\NormalTok{Y)}

\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{hist}\NormalTok{(Z1, }\AttributeTok{main=}\ConstantTok{NULL}\NormalTok{, }\AttributeTok{xlab=}\FunctionTok{expression}\NormalTok{(Z[}\DecValTok{1}\NormalTok{]))}
\FunctionTok{hist}\NormalTok{(Z2, }\AttributeTok{main=}\ConstantTok{NULL}\NormalTok{, }\AttributeTok{xlab=}\FunctionTok{expression}\NormalTok{(Z[}\DecValTok{2}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\includegraphics{MATH3714_files/figure-latex/not-a-normal-hist-1.pdf}

The histograms make it plausible that the components are indeed normally
distributed. Now we use a scatter plot to show the joint distribution
of \(Z_1\) and~\(Z_2\):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(Z1, Z2, }\AttributeTok{cex=}\FloatTok{0.5}\NormalTok{, }\AttributeTok{asp=}\DecValTok{1}\NormalTok{,}
     \AttributeTok{xlab=}\FunctionTok{expression}\NormalTok{(Z[}\DecValTok{1}\NormalTok{]),}
     \AttributeTok{ylab=}\FunctionTok{expression}\NormalTok{(Z[}\DecValTok{1}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\includegraphics{MATH3714_files/figure-latex/not-a-normal-scatter-1.pdf}

This plot looks peculiar! Most people would not call this a normal distribution
and the formal definition of a multivariate normal distribution is made to
exclude cases like this.
\end{example}

\textbf{Summary}

\begin{itemize}
\tightlist
\item
  We learned the rules for computing the expectation of a random vector.
\item
  The covariance matrix of random vectors plays the role of the
  variance for numeric random variables.
\item
  We learned about the definition of the multivariate normal distribution.
\end{itemize}

\clearpage

\hypertarget{S04-model}{%
\section{Properties of the Least Squares Estimate}\label{S04-model}}

Like in the one-dimensional case, we can build a \textbf{statistical model}
for the data. Here we assume that the residuals are random. More
precisely we have
\begin{equation}
  Y = X \beta + \varepsilon.  \label{eq:lmstats}
\end{equation}
for all \(i \in \{1, 2, \ldots, n\}\), where \(\varepsilon_1, \ldots, \varepsilon_n\)
are now assumed to be i.i.d.~random variables with \(\mathbb{E}(\varepsilon_i) = 0\) and
\(\mathop{\mathrm{Var}}(\varepsilon_i) = \sigma^2\).

\begin{itemize}
\item
  Again, we assume that the \(x\)-values are fixed and known. The only
  random quantities in the model are \(\varepsilon_i\) and \(Y_i\).
\item
  The parameters in this model are now \(\beta = (\beta_0, \beta_1, \cdots, \beta_p) \in \mathbb{R}^{p+1}\) and \(\sigma^2\). The parameters \(\beta_j\) are often
  called the regression coefficients.
\end{itemize}

The usual approach in statistics to quantify how well an estimator works
is to apply it to random samples from the statistical model, where we
can assume that we know the parameters, and then to study how well the
parameters are reconstructed by the estimators. Since this approach
uses random samples as input the the estimator, we obtain random estimates
and we need to use statistical methods to quantify how close the estimate
is to the truth.

\hypertarget{mean-and-covariance}{%
\subsection{Mean and Covariance}\label{mean-and-covariance}}

The bias of an estimator is the difference between the expected value
of the estimate and the truth. For the least squares estimator
we have
\begin{equation*}
  \mathop{\mathrm{bias}}(\hat\beta)
  = \mathbb{E}(\hat\beta) - \beta,
\end{equation*}
where
\begin{equation*}
  \hat\beta
  = (X^\top X)^{-1} X^\top Y
\end{equation*}
and \(Y\) is the random vector from~\eqref{eq:lmstats}.

\begin{lemma}
\protect\hypertarget{lem:hat-beta-dist}{}\label{lem:hat-beta-dist}

We have

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  \(\hat\beta = \beta + (X^\top X)^{-1} X^\top \varepsilon\) and
\item
  \(\hat\beta \sim \mathcal{N}\bigl( \beta, \sigma^2 (X^\top X)^{-1} \bigr)\).
\end{enumerate}

\end{lemma}

\begin{proof}
From lemma~\ref{lem:multiple-LSQ} we know
\begin{equation*}
  \hat\beta
  = (X^\top X)^{-1} X^\top Y,
\end{equation*}
Using the definition of \(Y\) we can write this as
\begin{align*}
  \hat\beta
  &= (X^\top X)^{-1} X^\top Y \\
  &= (X^\top X)^{-1} X^\top (X\beta + \varepsilon) \\
  &= (X^\top X)^{-1} X^\top X \beta + (X^\top X)^{-1} X^\top \varepsilon\\
  &= \beta + (X^\top X)^{-1} X^\top \varepsilon.
\end{align*}
This proves the first claim.

Since \(\varepsilon\) follows a multi-variate normal distribution,
\(\beta + (X^\top X)^{-1} X^\top \varepsilon\) is also normally distributed.
Taking expectations we get
\begin{align*}
  \mathbb{E}(\hat\beta)
  &= \mathbb{E}\bigl( \beta + (X^\top X)^{-1} X^\top \varepsilon\bigr) \\
  &= \beta + (X^\top X)^{-1} X^\top \mathbb{E}(\varepsilon) \\
  &= \beta,
\end{align*}
since \(\mathbb{E}(\varepsilon) = 0\).

For the covariance we find
\begin{align*}
  \mathop{\mathrm{Cov}}(\hat\beta)
  &= \mathop{\mathrm{Cov}}\bigl( \beta + (X^\top X)^{-1} X^\top \varepsilon\bigr) \\
  &= \mathop{\mathrm{Cov}}\bigl( (X^\top X)^{-1} X^\top \varepsilon\bigr) \\
  &= (X^\top X)^{-1} X^\top \mathop{\mathrm{Cov}}(\varepsilon) \bigl( (X^\top X)^{-1} X^\top \bigr)^\top \\
  &= (X^\top X)^{-1} X^\top \mathop{\mathrm{Cov}}(\varepsilon) X (X^\top X)^{-1}.
\end{align*}
Since \(\mathop{\mathrm{Cov}}(\varepsilon) = \sigma^2 I\), this simplifies to
\begin{align*}
  \mathop{\mathrm{Cov}}(\hat\beta)
  &= (X^\top X)^{-1} X^\top \sigma^2 I X (X^\top X)^{-1} \\
  &= \sigma^2 (X^\top X)^{-1} X^\top X (X^\top X)^{-1} \\
  &= \sigma^2 (X^\top X)^{-1}.
\end{align*}
This completes the proof.
\end{proof}

The lemma implies that \(\mathbb{E}(\hat\beta) = \beta\), \emph{i.e.}~the estimator
\(\hat\beta\) is unbiased. Note that for this statement we only used \(\mathbb{E}(\varepsilon) = 0\) to compute the expectation of~\(\hat\beta\). Thus, the estimator will still
be unbiases for correlated or for noise which is not normally distributed.

We have seen \protect\hyperlink{eq:Cov-diag-elem}{earlier} that the diagonal elements
of a covariance give the variances of the elements of the random vector.
Setting \(C := (X^\top X)^{-1}\) as a shorthand here, we find that the
individual estimated coefficients \(\hat\beta_i\) satisfy
\begin{equation}
  \hat\beta_i
  \sim \mathcal{N}\bigl( \beta, \sigma^2 C_{ii} \bigr)  \label{eq:beta-hat-i}
\end{equation}
for all \(i \in \{1, \ldots, n\}\).

These results about the (co-)variances of the estimator are not very
useful in practice, because the error variance~\(\sigma^2\) is unknown.
To derive more useful results, we will consider how to estimate this
variance.

\hypertarget{hat-matrix}{%
\subsection{Properties of the Hat Matrix}\label{hat-matrix}}

In this and the following sections we will use various
properties of the hat matrix \(H = X (X^\top X)^{-1} X^\top\).

\begin{lemma}

The hat matrix \(H\) has the following properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  \(H\) is symmetric, \emph{i.e.}~\(H^\top = H\).
\item
  \(H\) is idempotent, \emph{i.e.}~\(H^2 = H\).
\end{enumerate}

\end{lemma}

\begin{proof}
For the first statement we have
\begin{align*}
  H^\top
  &= \bigl( X (X^\top X)^{-1} X^\top \bigr)^\top \\
  &= (X^\top)^\top \bigl((X^\top X)^{-1}\bigr)^\top X^\top \\
  &= X (X^\top X)^{-1} X^\top \\
  &= H,
\end{align*}
where we used that the inverse of a symmetric matrix is symmetric.
The second statement follow from
\begin{align*}
  H^2
  &= \bigl( X (X^\top X)^{-1} X^\top \bigr) \bigl( X (X^\top X)^{-1} X^\top \bigr) \\
  &= X \bigl( (X^\top X)^{-1} X^\top X \bigr) (X^\top X)^{-1} X^\top \\
  &= X (X^\top X)^{-1} X^\top.
\end{align*}
This completes the proof.
\end{proof}

Both properties from the lemma carry over from \(H\) to \(I - H\): we
have \((I - H)^\top = I^\top - H^\top = I - H\) and
\begin{align*}
  (I - H)^2
  &= (I - H)(I - H) \\
  &= I^2 - HI - IH + H^2 \\
  &= I - H - H + H \\
  &= I - H.
\end{align*}

For future reference we also state two simpler results: we have
\begin{equation}
  H X = X (X^\top X)^{-1} X^\top X = X  \label{eq:XH}
\end{equation}
and
\begin{equation}
  (I - H) X = IX - HX = X - X = 0.  \label{eq:XIH}
\end{equation}

Finally, if we have a vector \(v \in \mathbb{R}^n\) we can write \(v\) as
\begin{align*}
  v
  &= (H + I - H)v \\
  &= Hv + (I-H)v.
\end{align*}
The inner product between these two components is
\begin{align*}
  (Hv)^\top (I-H)v
  &= v^\top H^\top (I-H) v \\
  &= v^\top H (I-H) v \\
  &= v^\top (H-H^2) v \\
  &= v^\top (H-H) v \\
  &= 0,
\end{align*}
so the two vectors are orthogonal.
As a result we get
\begin{align*}
  \|v\|^2
  &= v^\top v \\
  &= \bigl( Hv + (I-H)v \bigr)^\top \bigl( Hv + (I-H)v \bigr) \\
  &= (Hv)^\top Hv + 2 (Hv)^\top (I-H)v + \bigl((I-H)v\bigr)^\top (I-H)v \\
  &= \| Hv \|^2 + \|(I-H)v \|^2
\end{align*}
(This is \href{https://en.wikipedia.org/wiki/Pythagorean_theorem}{Pythagoras' theorem} in \(\mathbb{R}^n\).)
Since \(\hat y = Hy\) and \(\hat\varepsilon= (I - H)y\), we can apply this idea to the
vector \(y\) of observations to get \(\|y\|^2 = \|\hat y\|^2 + \|\hat\varepsilon\|^2\).

We note without proof that geometrically, \(H\) can be interpreted as the
orthogonal projection onto the subspace of \(\mathbb{R}^n\) which is spanned by
the columns of \(X\). This subspace contains the possible output vectors
of the linear system and the least squares procedure finds the point \(\hat y\)
in this subspace which is closest to the observed data \(y\in\mathbb{R}^n\).

Some authors define:

\begin{itemize}
\tightlist
\item
  \(\mathrm{SS}_\mathrm{T} = \sum_{i=1}^n y_i^2\) (where ``T'' stands for ``total'')
\item
  \(\mathrm{SS}_\mathrm{R} = \sum_{i=1}^n \hat y_i^2\) (where ``R'' stands for ``regression'')
\item
  \(\mathrm{SS}_\mathrm{E} = \sum_{i=1}^n (y_i-\hat y_i)^2\) (where ``E'' stands for ``error'')
\end{itemize}

Using this notation, our equation
\(\|y\|^2 = \|\hat y\|^2 + \|\hat\varepsilon\|^2\)
turns into
\begin{equation*}
  \mathrm{SS}_\mathrm{T}
  = \mathrm{SS}_\mathrm{R} + \mathrm{SS}_\mathrm{E}.
\end{equation*}

\hypertarget{Cochran}{%
\subsection{Cochran's theorem}\label{Cochran}}

Our main tool in this and the following section will be a simplified
version of \href{https://en.wikipedia.org/wiki/Cochran\%27s_theorem}{Cochran's theorem}.

\begin{theorem}

The following statements are true:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  \(\frac{1}{\sigma^2} \varepsilon^\top H \varepsilon\sim \chi^2(p+1)\)
\item
  \(\frac{1}{\sigma^2} \varepsilon^\top (I - H) \varepsilon\sim \chi^2(n - p - 1)\)
\item
  \(H \varepsilon\) and \((I-H)\varepsilon\) are independent.
\end{enumerate}

\end{theorem}

\begin{proof}
Since \(H\) is symmetric, we can diagonalise \(H\) (see \ref{thm:spectral}
in the appendix): there is an orthogonal matrix \(U\) such that
\(D := U H U^\top\) is diagonal, and the diagonal elements of \(D\) are
the eigenvalues of \(H\). Since \(H\) is idempotent, these diagonal elements
can only be \(0\) or~\(1\). Also, since \(U\) is orthogonal, we have \(U^\top U = I\)
and thus
\begin{equation*}
  U^\top D U
  = U^\top U H U^\top U
  = H.
\end{equation*}
The same matrix \(U\) also diagonalises \(I-H\), since
\(U (I -H) U^\top = U U^\top - U H U^\top = I - D\). Exactly one
of the diagonal elements \(D_{ii}\) and \((I - D)_{ii}\) is \(1\) and the other
one is \(0\) for every~\(i\).

Since \(\varepsilon\sim \mathcal{N}(0, \sigma^2 I)\) we find that \(\eta := U \varepsilon\) is normally
distributed with mean \(U 0 = 0\) and covariance matrix \(\sigma^2 U I U^\top = \sigma^2 U U^\top = \sigma^2 I\). Thus \(\eta\) has the same distribution as
\(\varepsilon\) does: \(\eta \sim \mathcal{N}(0, \sigma^2I)\) and the components \(\eta_i\)
are independent of each other. We have
\begin{equation*}
  H \varepsilon
  = U^\top D U \varepsilon
  = U^\top D \eta.
\end{equation*}
and
\begin{equation*}
  (I - H) \varepsilon
  = U^\top (I - D) U \varepsilon
  = U^\top (I - D) \eta.
\end{equation*}
Since \((D\eta)_i = 0\) if \(D_{ii}=0\) and \(\bigl((I - D) \eta)_i = 0\)
otherwise, each component of \(\eta\) contributes to exactly one of the
two vectors \(D\eta\) and \((I-D)\eta\). Thus, \(D\eta\) and \((I-D)\eta\)
are independent, and thus \(H\varepsilon\) and \((I - H)\varepsilon\) are also independent.
This proves the third statement of the theorem.

For the first statement, we note that
\begin{align*}
  \varepsilon^\top H \varepsilon
  &= \varepsilon^\top U^\top D U \varepsilon\\
  &= \eta^\top D \eta \\
  &= \sum_{i=1 \atop D_{ii}=1}^n \eta_i^2.
\end{align*}
Since \((X^\top X) \in\mathbb{R}^{(p+1)\times (p+1)}\) is invertible, one can show
that \(\mathop{\mathrm{rank}}(H) = p+1\) and thus that there are \(p+1\) terms contributing
to the sum (we skip the proof of this statement here). Thus,
\begin{equation*}
  \frac{1}{\sigma^2} \varepsilon^\top H \varepsilon
  = \sum_{i=1 \atop D_{ii}=1}^n \bigl(\eta_i/\sigma)^2
\end{equation*}
is the sum of the squares of \(p+1\) independent standard normals,
and thus is \(\chi^2(p+1)\) distributed. This completes the proof of the
first statement.

Finally, the second statement follows in much of the same way as the
first one, except that \(H\) is replaced with \(I-H\) and the sum is
over the \(n - p - 1\) indices \(i\) where \(D_{ii} = 0\).
This completes the proof.
\end{proof}

Expressions of the form \(x^\top A x\) for \(x\in\mathbb{R}^n\) and \(A\in\mathbb{R}^{n\times n}\)
are called \textbf{quadratic forms}.

While the theorem as written only states that \(H \varepsilon\) and \((I - H)\varepsilon\)
are independent of each other, we can replace one or both of these terms
the corresponding quadratic forms as still keep the independence. Since
\((H \varepsilon)^\top (H \varepsilon) = \varepsilon^\top H^\top H \varepsilon= \varepsilon^\top H \varepsilon\),
the quadratic form \(\varepsilon^\top H \varepsilon\) is a function of \(H \varepsilon\) and a similar
statement holds with \(H-I\) instead of \(H\).

\hypertarget{var-est-bias}{%
\subsection{Estimating the Error Variance}\label{var-est-bias}}

So far we have only considered how to estimate the parameter
vector~\(\beta\) and we have ignored the parameter~\(\sigma^2\).
We will see that an unbiased estimator for \(\sigma^2\) is
given by
\begin{equation}
  \hat\sigma^2
  = \frac{1}{n-p-1} \sum_{i=1}^n (y_i - \hat y_i)^2, \label{eq:hat-sigma-squared}
\end{equation}
where \(\hat y_i\) are the fitted values from equation~\eqref{eq:fitted-values}.
As for the one-dimensional case in~\eqref{eq:reg-sigma-est}, the estimate
does not have the prefactor \(1/n\), which one might naively expect,
but the denomintor is decreased by one for each component of the
vector~\(\beta\). Using Cochran's theorem, we can now show that the estimator
\(\hat\sigma^2\) is unbiased.

We first note that
\begin{align*}
  (n - p - 1) \hat\sigma^2
  &= (y - \hat y)^\top (y - \hat y) \\
  &= (y - H y)^\top (y - H y) \\
  &= y^\top (I - H)^\top (I - H) y \\
  &= y^\top (I - H) y
\end{align*}
To determine the bias, we need to use \(Y = X\beta + \varepsilon\) in place of the
data. This gives
\begin{align*}
  (n - p - 1) \hat\sigma^2
  &= Y^\top (I-H) Y \\
  &= (X\beta + \varepsilon)^\top (I-H) (X\beta + \varepsilon) \\
  &= \beta^\top X^\top (I-H) X \beta
      + 2 \varepsilon^\top (I-H) X \beta
      + \varepsilon^\top (I-H) \varepsilon\\
  &= \varepsilon^\top (I-H) \varepsilon,
\end{align*}
where we used equation~\eqref{eq:XIH} to see that the first two terms in
the sum equal zero.

Now we can apply Cochran's theorem. This shows that
\begin{equation}
  \frac{1}{\sigma^2} (n - p - 1) \hat\sigma^2
  = \frac{1}{\sigma^2} \varepsilon^\top (I-H) \varepsilon
  \sim \chi^2(n - p - 1).  \label{eq:sigma-hat-chi-squared}
\end{equation}
Since the expectation of a \(\chi^2(\nu)\) distribution equals \(\nu\)
(see appendix~\ref{chi-square}), we find
\begin{equation*}
  \frac{1}{\sigma^2} (n - p - 1) \mathbb{E}(\hat\sigma^2)
  = n - p - 1
\end{equation*}
and thus
\begin{equation*}
  \mathbb{E}(\hat\sigma^2)
  = \sigma^2.
\end{equation*}
This proves that \(\hat\sigma^2\) is an unbiased estimator for \(\sigma^2\).

\textbf{Summary}

\begin{itemize}
\tightlist
\item
  The least squares estimator for the regression coefficients is unbiased.
\item
  The hat matrix is idempotent and symmetric.
\item
  Cochran's theorem allows to understand the distribution of some
  quadratic forms involving the hat matrix.
\item
  \(\hat\sigma^2\) is an unbiased estimator for \(\sigma^2\).
\end{itemize}

\clearpage

\hypertarget{S05-single}{%
\section{Uncertainty for Individual Regression Coefficients}\label{S05-single}}

In this section we will consider different ways to study the uncertainty in the
estimates \(\hat\beta_i\) for the regression coefficient~\(\beta_i\) individually.
In the following sections we will then consider the problem of simultaneously
estimating several or all coefficients.

\hypertarget{measuring-the-estimation-error}{%
\subsection{Measuring the Estimation Error}\label{measuring-the-estimation-error}}

We have seen that \(\hat\beta \sim \mathcal{N}(0, \sigma^2 X)\), where
\((X^\top X)^{-1}\). Restricting this to a single coefficient, we find
\begin{equation*}
  \hat\beta_i
  \sim \mathcal{N}\bigl( \beta, \sigma^2 C_{ii} \bigr),
\end{equation*}
since the diagonal elements of the covariance matrix contains the
variances of the elements of a random vector. In practice we will not
know the value of \(\sigma^2\), so we have to estimate this from data,
using the estimator
\begin{equation*}
  \hat\sigma^2
  = \frac{1}{n-p-1} \sum_{i=1}^n (y_i - \hat y_i)^2,
\end{equation*}
from equation~\eqref{eq:hat-sigma-squared}. As a first application of
Cochran's theorem we showed in equation~\eqref{eq:sigma-hat-chi-squared}
that
\begin{equation*}
  \frac{1}{\sigma^2} (n - p - 1) \hat\sigma^2
  \sim \chi^2(n - p - 1).
\end{equation*}

Note that in the equations above, we index the rows and columns of \(C\)
using \(i,j\in \{0, 1, \ldots, p\}\), \emph{i.e.} the first row and column are
using the index 0 each. This is to match the convention for the
components of \(\beta = (\beta_0, \beta_1, \ldots, \beta_p)\).

\begin{lemma}
\protect\hypertarget{lem:hat-beta-sigma-indep}{}\label{lem:hat-beta-sigma-indep}The random vector \(\hat\beta\) and the random number
\(\hat\sigma^2\) are independent of each other.
\end{lemma}

\begin{proof}
We will show that \(\hat\beta\) can be written as a function of \(H\varepsilon\)
and that \(\hat\sigma^2\) can be written as a function of \((I-H)\varepsilon\).
The result then follows from Cochran's theorem.

From lemma~\ref{lem:hat-beta-dist}
we know that the least squares estimate \(\hat\beta\) can be written as
\begin{equation*}
  \hat\beta
  = \beta + (X^\top X)^{-1} X^\top \varepsilon
\end{equation*}
and that \(H = X (X^\top X)^{-1} X^\top\). Thus we can write \(\hat\beta\) as
\begin{align*}
  \hat\beta
  &= \beta + (X^\top X)^{-1} X^\top X \, (X^\top X)^{-1} X^\top \varepsilon\\
  &= \beta + (X^\top X)^{-1} X^\top H \varepsilon,
\end{align*}
which is a function of \(H\varepsilon\).

Similar to the argument at the end of the previous section, we can
write \(\hat\sigma^2\) as
\begin{align*}
  (n - p - 1) \hat\sigma^2
  &= (Y - \hat Y)^\top (Y - \hat Y) \\
  &= (Y - H Y)^\top (Y - H Y) \\
  &= Y^\top (I - H)^\top (I - H) Y \\
  &= \bigl\| (I - H) Y \|.
\end{align*}
Since \(Y = X\beta + \varepsilon\) and since we know \((I-H)X = 0\) from
equation~\eqref{eq:XIH}, we find
\begin{align*}
  \hat\sigma^2
  &= \frac{1}{n-p-1} \bigl\| (I - H) (X\beta + \varepsilon) \| \\
  &= \frac{1}{n-p-1} \bigl\| (I - H) \varepsilon\|,
\end{align*}
which is a function of \((I - H)\varepsilon\).

From Cochran's theorem we know that \(H \varepsilon\) and \((I-H)\varepsilon\) are independent
and thus we can conclude that \(\hat\beta\) and \(\hat\sigma^2\) are also
independent of each other. This completes the proof.
\end{proof}

We now construct a quantity \(T\) which measures the distance between
the estimated value \(\hat\beta_i\) and the unknown true value~\(\beta_i\):
\begin{equation}
  T
  := \frac{\hat\beta_i - \beta_i}{\sqrt{\hat\sigma^2 C_{ii}}}. \label{eq:single-T}
\end{equation}
While there are many ways to measure this distance, the \(T\) constructed
here has two main advantages:

\begin{itemize}
\item
  The value of \(T\) can be computed from the given data, without
  any reference to unknown quantities.
\item
  Below, we will be able to find the distribution of \(T\). This will
  allow us to use \(T\) to construct confidence intervals and statistical
  tests.
\end{itemize}

\begin{lemma}
\protect\hypertarget{lem:beta-i-follows-t}{}\label{lem:beta-i-follows-t}Assume that the data follows the model~\eqref{eq:lmstats}.
Then \(T \sim t_{n-p-1}\), \emph{i.e.} \(T\) follows a
\(t\)-distribution with \(n-p-1\) degrees of freedom (see appendix \ref{t}).
\end{lemma}

\begin{proof}
We have
\begin{align*}
  T
  &= \frac{(\hat\beta_i - \beta_i) / \sqrt{C_{ii}}}
      {\sqrt{\hat\sigma^2}} \\
  &= \frac{(\hat\beta_i - \beta_i) / \sqrt{\sigma^2 C_{ii}}}
      {\sqrt{\hat\sigma^2 / \sigma^2}} \\
  &= \frac{(\hat\beta_i - \beta_i) / \sqrt{\sigma^2 C_{ii}}}
      {\sqrt{(n - p - 1) \hat\sigma^2 / \sigma^2 / (n - p -1)}} \\
  &=: \frac{Z}{\sqrt{Y / (n - p - 1)}},
\end{align*}
where \(Z = (\hat\beta_i - \beta_i) / \sqrt{\sigma^2 C_{ii}} \sim \mathcal{N}(0,1)\)
and \(Y = (n - p - 1) \hat\sigma^2/\sigma^2 \sim \chi^2(n-p-1)\)
are independent, by lemma~\ref{lem:hat-beta-sigma-indep}.
Thus, \(T \sim t_{n-p-1}\) as required.
\end{proof}

The quantity \(\sqrt{\sigma^2 C_{ii}}\) is sometimes called the
\textbf{standard error} of the estimator \(\hat\beta_i\), denoted by
\(\mathop{\mathrm{se}}(\hat\beta_i)\).

\hypertarget{confidence-intervals}{%
\subsection{Confidence Intervals}\label{confidence-intervals}}

Using the scaled distance~\(T\), it is easy to construct a confidence
interval for \(\hat\beta_i\): For \(\alpha \in (0, 1)\), say \(\alpha = 5\%\),
lemma~\ref{lem:beta-i-follows-t} shows that
\begin{equation*}
  P\Bigl( T \in \bigl[-t_{n-p-1}(\alpha/2), +t_{n-p-1}(\alpha/2)\bigr] \Bigr)
  = 1 - \alpha,
\end{equation*}
where \(t_{n-p-1}(\alpha/2)\) is the \((1 - \alpha/2)\)-quantile of the
\(t(n-p-1)\)-distribution. Rewriting this expression as a condition on
\(\hat\beta_i\) instead of on \(T\) gives a confidence interval for \(\beta_i\).

\begin{lemma}
The interval
\begin{equation*}
  [U, V]
  := \Bigl[ \hat\beta_i - \sqrt{\hat\sigma^2 C_{ii}}t_{n-p-1}(\alpha/2), \hat\beta_i + \sqrt{\hat\sigma^2 C_{ii}}t_{n-p-1}(\alpha/2) \Bigr]
\end{equation*}
is a \((1-\alpha)\)-confidence interval for \(\beta_i\).
\end{lemma}

\begin{proof}
We have to show that \(P(\beta_i \in [U, V]) \geq 1-\alpha\). We have
\begin{align*}
  &\hskip-5mm \beta_i \in [U, V] \\
  &\Longleftrightarrow
    \bigl| \hat\beta_i - \beta_i \bigr|
        \leq \sqrt{\hat\sigma^2 C_{ii}}t_{n-p-1}(\alpha/2) \\
  &\Longleftrightarrow
    \Bigl| \frac{\hat\beta_i - \beta_i}{\sqrt{\hat\sigma^2 C_{ii}}} \Bigr|
        \leq t_{n-p-1}(\alpha/2) \\
  &\Longleftrightarrow
    T \in \bigl[-t_{n-p-1}(\alpha/2), +t_{n-p-1}(\alpha/2)\bigr]
\end{align*}
and thus \(P(\beta_i \in [U, V]) = 1 - \alpha\). This completes the proof.
\end{proof}

\hypertarget{hypthesis-tests}{%
\subsection{Hypthesis Tests}\label{hypthesis-tests}}

Very similar to the argument for confidence intervals, we can
derive a hypothesis test to test the hypothesis
\begin{equation*}
  H_0\colon \beta_i = b
\end{equation*}
against the alternative
\begin{equation*}
  H_0\colon \beta_i \neq b.
\end{equation*}

Here we redefine \(T\) as
\begin{equation*}
  T
  := \frac{\hat\beta_i - b}{\sqrt{\hat\sigma^2 C_{ii}}},
\end{equation*}
using \(b\) in place of the \(\beta_i\) above. Then the new defintion of \(T\)
is the same as~\eqref{eq:single-T} if \(H_0\) is true.

\begin{lemma}
The test which rejects \(H_0\) if and only if \(|T| > t_{n-p-1}(\alpha/2)\)
has confidence level~\(\alpha\).
\end{lemma}

\begin{proof}
We have to show that the probability of type~I errors (\emph{i.e.} of wrongly
rejecting \(H_0\) when it is true) is less than or equal to~\(\alpha\).
Assume that \(H_0\) is true. Then we have \(\beta_i = b\) and thus
the \(T\) defined in this section coincides with the expression from
equation~\eqref{eq:single-T}. From lemma~\ref{lem:beta-i-follows-t}
we know that \(T \sim t(n-p-1)\). Thus we have
\begin{align*}
  P( \mbox{type I error} )
  &= P\bigl( |T| > t_{n-p-1}(\alpha/2) \bigr) \\
  &= P\bigl(T < -t_{n-p-1}(\alpha/2) \bigr) + P\bigl(T > t_{n-p-1}(\alpha/2) \bigr) \\
  &= 2 P\bigl(T > t_{n-p-1}(\alpha/2) \bigr) \\
  &= 2 P\bigl(T > t_{n-p-1}(\alpha/2) \bigr) \\
  &= 2 \frac{\alpha}{2} \\
  &= \alpha.
\end{align*}
This completes the proof.
\end{proof}

If we use \(b = 0\) in the test, we can test whether \(\beta_i = 0\).
If \(\beta_i = 0\) is true, the corresponding input \(x_i\) has no influence
on the output.

As usual with statistical tests, one needs to be extremely careful when
performing several tests on the same data. In particular, it would be
unwise to test more than one component of \(\beta\) using this procedure
for the same data. Instead, in the next section we will consider how
to perform tests for several components of \(\beta\) simultaneously.
Before we do this, we will perform some experiments with R.

\hypertarget{r-experiments}{%
\subsection{R Experiments}\label{r-experiments}}

\hypertarget{fitting-the-model}{%
\subsubsection{Fitting the model}\label{fitting-the-model}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(stack.loss }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ stackloss)}
\FunctionTok{summary}\NormalTok{(m)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = stack.loss ~ ., data = stackloss)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.2377 -1.7117 -0.4551  2.3614  5.6978 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -39.9197    11.8960  -3.356  0.00375 ** 
## Air.Flow      0.7156     0.1349   5.307  5.8e-05 ***
## Water.Temp    1.2953     0.3680   3.520  0.00263 ** 
## Acid.Conc.   -0.1521     0.1563  -0.973  0.34405    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.243 on 17 degrees of freedom
## Multiple R-squared:  0.9136, Adjusted R-squared:  0.8983 
## F-statistic:  59.9 on 3 and 17 DF,  p-value: 3.016e-09
\end{verbatim}

\hypertarget{estimating-the-variance-of-the-error}{%
\subsubsection{Estimating the Variance of the Error}\label{estimating-the-variance-of-the-error}}

We can get the design matrix \(X\) and the covariance matrix \(C\) as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{model.matrix}\NormalTok{(m)}
\NormalTok{C }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(X) }\SpecialCharTok{\%*\%}\NormalTok{ X)}
\FunctionTok{round}\NormalTok{(C, }\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             (Intercept) Air.Flow Water.Temp Acid.Conc.
## (Intercept)     13.4527   0.0273    -0.0620    -0.1594
## Air.Flow         0.0273   0.0017    -0.0035    -0.0007
## Water.Temp      -0.0620  -0.0035     0.0129     0.0000
## Acid.Conc.      -0.1594  -0.0007     0.0000     0.0023
\end{verbatim}

Next we need to estimate the variance \(\sigma^2\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ stackloss}\SpecialCharTok{$}\NormalTok{stack.loss}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(stackloss)}
\NormalTok{p }\OtherTok{\textless{}{-}} \FunctionTok{ncol}\NormalTok{(stackloss) }\SpecialCharTok{{-}} \DecValTok{1}
\NormalTok{hat.sigma2 }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}} \FunctionTok{fitted}\NormalTok{(m))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ (n }\SpecialCharTok{{-}}\NormalTok{ p }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}
\NormalTok{hat.sigma2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10.51941
\end{verbatim}

The square root of this number, so the estimated standard deviation of the
\(\varepsilon_i\) is shown as \texttt{Residual\ standard\ error} in the summary output above.
We check that we get the same result:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sqrt}\NormalTok{(hat.sigma2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.243364
\end{verbatim}

This result is also listed as the \texttt{Residual\ standard\ error} near the
bottom of the \texttt{summary(m)} output, above.

\hypertarget{estimating-the-standard-errors}{%
\subsubsection{Estimating the Standard Errors}\label{estimating-the-standard-errors}}

We can the standard errors, \emph{i.e.}~the standard deviations
\(\mathop{\mathrm{stdev}}(\hat\beta)_i\) as \(\sqrt{\hat\sigma^2 C_ii}\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{se }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(hat.sigma2 }\SpecialCharTok{*} \FunctionTok{diag}\NormalTok{(C))}
\NormalTok{se}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)    Air.Flow  Water.Temp  Acid.Conc. 
##  11.8959969   0.1348582   0.3680243   0.1562940
\end{verbatim}

These values are also listed in the \texttt{Std.\ Error} column of the \texttt{summary(m)}
output.

\hypertarget{hypothesis-tests}{%
\subsubsection{Hypothesis tests}\label{hypothesis-tests}}

Let us now test the hypothesis \(H_0\colon \beta_i = 0\). The test statistic
for this case is the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{T }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(m) }\SpecialCharTok{/}\NormalTok{ se}
\NormalTok{T}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)    Air.Flow  Water.Temp  Acid.Conc. 
##  -3.3557234   5.3066130   3.5195672  -0.9733098
\end{verbatim}

These values are also listed in the \texttt{t\ value} column of the \texttt{summary(m)}
output.

Before we can perform the test, we need to choose \(\alpha\) and to
find the corresponding critical value:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alpha }\OtherTok{\textless{}{-}} \FloatTok{0.05}
\NormalTok{t }\OtherTok{\textless{}{-}} \FunctionTok{qt}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alpha}\SpecialCharTok{/}\DecValTok{2}\NormalTok{ , n }\SpecialCharTok{{-}}\NormalTok{ p }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}
\NormalTok{t}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.109816
\end{verbatim}

Using the critical value \(t\) we can decided whether \(H_0\) should be accepted or
rejected. For example, looking at the intercept \(\beta_0\), we find \(|T_0| = |-3.3557234| > 2.109816 = t_{n-p-1}(1-\alpha/2)\) and thus we can reject the
hypothesis \(H_0\colon \beta_0 = 0\). This means that the intercept is
significantly different from 0.

\hypertarget{confidence-intervals-1}{%
\subsubsection{Confidence Intervals}\label{confidence-intervals-1}}

Using the quantile \texttt{t} we can also get confidence intervals. Here we
only show the confidence interval for the intercept \(\beta_0\):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{c}\NormalTok{(}\FunctionTok{coef}\NormalTok{(m)[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ se[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ t, }\FunctionTok{coef}\NormalTok{(m)[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ se[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ t)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept) (Intercept) 
##   -65.01803   -14.82131
\end{verbatim}

Confidence intervals for the remaining coefficients can be obtained simimlarly.

\textbf{Summary}

\begin{itemize}
\tightlist
\item
  We know how to scale the distance between individual parameter estimates
  and the truth.
\item
  We have seen how to construct confidence intervals for \(\beta_i\).
\item
  We have seen how to construct statistical tests for \(\beta_i\).
\item
  We have understood some more of the summary output for the \texttt{lm()}
  function in~R.
\end{itemize}

\clearpage

\hypertarget{appendix-appendices}{%
\appendix}


\hypertarget{Sx1-matrices}{%
\section{Linear Algebra Reminders}\label{Sx1-matrices}}

\hypertarget{vectors}{%
\subsection{Vectors}\label{vectors}}

We write \(v \in \mathbb{R}^d\) if \(v = (v_1, \ldots, v_d)\) for numbers
\(v_1, \ldots, v_d \in\mathbb{R}\). We say that \(v\) is a \(d\)-dimensional vector,
and \(\mathbb{R}^d\) is the \(d\)-dimensional Euclidean space. Vectors are
often graphically represented as ``column vectors'':
\begin{equation*}
  v = \begin{pmatrix}
      v_1 \\ v_2 \\ \vdots \\ v_d
  \end{pmatrix}.
\end{equation*}

If \(u,v\in\mathbb{R}^d\) are two vectors, the \textbf{inner product} of \(u\) and~\(v\)
is given by
\begin{equation}
  u^\top v
  = \sum_{i=1}^d u_i v_i.  \label{eq:inner-product}
\end{equation}
Note that the two vectors must have the same length for the inner product
to exist. The vectors \(u\) and~\(v\) are said to be \textbf{orthogonal}, if
\(u^\top v = 0\).

Using this notation, the \textbf{Euclidean length} of a vector~\(v\)
can be written as
\begin{equation*}
  \|v\|
  = \sqrt{\sum_{i=1}^d v_i^2}
  = \sqrt{v^\top v}.
\end{equation*}

\hypertarget{matrix-rules}{%
\subsection{Matrices}\label{matrix-rules}}

We write \(A \in \mathbb{R}^{m\times n}\) if
\begin{equation*}
  A
  = \begin{pmatrix}
    a_{1,1} & \ldots  & a_{1,n}\\
    a_{2,1} & \ldots  & a_{2,n}\\
    \vdots & \ddots  & \vdots\\
    a_{m,1} & \ldots  & a_{m,n}
  \end{pmatrix},
\end{equation*}
where \(a_{i,j}\), sometimes also written as \(a_{ij}\) are numbers
for \(i \in \{1, \ldots, m\}\) and \(j \in \{1, \ldots, n\}\).

\hypertarget{transpose}{%
\subsubsection{Transpose}\label{transpose}}

If \(A \in \mathbb{R}^{m\times n}\), then the \textbf{transpose} of \(A\) is the matrix
\(A^\top \in \mathbb{R}^{n\times m}\), with \((A^\top)_{ij} = a_{ji}\) for all \(i \in \{1, \ldots, n\}\) and \(j \in \{1, \ldots, m\}\). Graphically,
this can be written as
\begin{equation*}
  A^\top
  = \begin{pmatrix}
    a_{1,1} & a_{2,1} \ldots  & a_{m,1}\\
    \vdots & \vdots & \ddots  & \vdots\\
    a_{1,n} & a_{2,n} \ldots  & a_{m,n}
  \end{pmatrix},
\end{equation*}

\begin{definition}
A matrix \(A\) is called \textbf{symmetric}, if \(A^\top = A\).
\end{definition}

\hypertarget{matrix-vector-product}{%
\subsubsection{Matrix-vector Product}\label{matrix-vector-product}}

If \(A \in \mathbb{R}^{m \times n}\) and \(v \in \mathbb{R}^n\), then
\(Av \in \mathbb{R}^m\) is the vector with
\begin{equation*}
  (Av)_i
  = \sum_{j=1}^n a_{ij} v_j
\end{equation*}
for all \(i \in \{1, \ldots, m\}\).

If we consider \(v\) to be a \((n\times 1)\)-matrix instead of a vector,
\(Av\) can also be interpreted as a matrix-matrix product between an \(m \times n\) and an \(n\times 1\) matrix. Using this convention, \(v^\top\)
is then interpreted as an \(1 \times n\) matrix and if \(u\in\mathbb{R}^m\) we
have \(u^\top A \in \mathbb{R}^{1 \times n} \cong \mathbb{R}^n\) with
\begin{equation*}
  (u^\top A)_j
  = \sum_{i=1}^m u_i a_{ij}
\end{equation*}
for all \(j \in \{1, \ldots, n\}\). Going one step further, this
notation also motivates the expression \(u^\top v\) in
equation~\eqref{eq:inner-product}.

\hypertarget{matrix-matrix-product}{%
\subsubsection{Matrix-matrix Product}\label{matrix-matrix-product}}

If \(A \in \mathbb{R}^{\ell \times m}\) and \(B \in \mathbb{R}^{m\times n}\), then
\(AB \in \mathbb{R}^{\ell \times n}\) is the matrix with
\begin{equation*}
  (AB)_{ik}
  = \sum_{j=1}^m a_{ij} b_{jk}
\end{equation*}
for all \(i \in \{1, \ldots, \ell\}\) and \(j \in \{1, \ldots, n\}\).
This is called the \textbf{matrix product} of \(A\) and \(B\). Note
that \(A\) and \(B\) must have compatible shapes for the product to exist.

Properties:

\begin{itemize}
\item
  The matrix product is associative: if \(A\), \(B\) and \(C\) are matrices
  with shapes such that \(AB\) and \(BC\) exist, then we have \(A(BC) = (AB)C\).
  It does not matter in which order we perform the matrix products here.
\item
  The matrix product is transitive: if \(A\), \(B\) and \(C\) have the
  correct shapes, we have \(A(B+C) = AB + AC\).
\item
  The matrix product is \emph{not} commutative: if \(AB\) exists, in general
  \(A\) and \(B\) don't have the correct shapes for \(BA\) to also exist,
  and even if \(BA\) exists, in general we have \(AB \neq BA\).
\item
  Taking the transpose swaps the order in a matrix product:
  we have
  \begin{equation}
    (AB)^\top = B^\top A^\top  \label{eq:AB-trans}
  \end{equation}
\end{itemize}

\hypertarget{matrix-inverse}{%
\subsubsection{Matrix Inverse}\label{matrix-inverse}}

If \(A\) is a square matrix and if there is a matrix \(B\) such that
\(AB = I\), then \(A\) is called \textbf{invertible} and the matrix \(B\) is
called the \textbf{inverse} of~\(A\), denoted by \(A^{-1} := B\).
Some important properties of the inverse:

\begin{itemize}
\item
  The inverse, if it exists, is unique.
\item
  Left-inverse and right-inverse for matrices are the same:
  \(A^{-1} A = I\) holds if and only if \(A A^{-1} = I\).
\item
  If \(A\) is symmetric and invertible, then \(A^{-1}\) is also
  symmetric. (Proof: \(A (A^{-1})^\top = (A^{-1} A)^\top = I^\top = I\) and thus \((A^{-1})^\top\) is an inverse of \(A\).
  Since the inverse is unique, \((A^{-1})^\top = A^{-1}\).)
\end{itemize}

\hypertarget{orthogonal-matrices}{%
\subsubsection{Orthogonal Matrices}\label{orthogonal-matrices}}

\begin{definition}
A matrix \(U\) is called \textbf{orthogonal}, if \(U^\top U = I = U U^\top\).
\end{definition}

If \(U\) is orthogonal, the inverse and the transpose are the same:
\(U^\top = U^{-1}\).

\hypertarget{positive-definite}{%
\subsubsection{Positive Definite Matrices}\label{positive-definite}}

\begin{definition}
A symmetric matrix \(A \in \mathbb{R}^{n\times n}\) is called \textbf{positive definite}, if
\begin{equation*}
  x^\top A x > 0
\end{equation*}
for all \(x \in \mathbb{R}^n\) with \(x\neq 0\). The matrix is called
\textbf{positive semi-definite}, if
\begin{equation*}
  x^\top A x \geq 0
\end{equation*}
for all \(x \in \mathbb{R}^n\).
\end{definition}

\hypertarget{idempotent}{%
\subsubsection{Idempotent Matrices}\label{idempotent}}

\begin{definition}
The matrix \(A\) is \textbf{idempotent}, if \(A^2 = A\).
\end{definition}

\hypertarget{eigenvalues}{%
\subsection{Eigenvalues}\label{eigenvalues}}

\begin{definition}
Let \(A \in\mathbb{R}^{n\times n}\) be a square matrix and \(\lambda\in R\).
The number \(\lambda\) is called an \textbf{eigenvalue} of \(A\), if there
exists a vector \(v \neq 0\) such that \(A x = \lambda x\). Any
such vector \(x\) is called an \textbf{eigenvector} of \(A\) with eigenvalue~\(\lambda\).
\end{definition}

While there are very many results about eigenvectors and eigenvalues
in Linear Algebra, here we will only use a small number of these results.
We summarise what we need for this module:

\begin{itemize}
\tightlist
\item
  If \(A\) is idempotent and \(x\) is an eigenvector with eigenvalue \(\lambda\),
  then we have \(\lambda x = A x = A^2 x = \lambda Ax = \lambda^2 x\). Thus we
  have \(\lambda^2 = \lambda\). This shows that the only eigenvalues possible
  for idempotent matrices are \(0\) and~\(1\).
\end{itemize}

\begin{theorem}
\protect\hypertarget{thm:spectral}{}\label{thm:spectral}Let \(A\in\mathbb{R}^{n\times n}\) be symmetric. Then there is an orthogonal
matrix \(U\) such that \(D := U A U^\top\) is diagonal. The diagonal
elements of \(D\) are the eigenvalues of \(A\) and the rows of \(U\)
are corresponding eigenvectors.
\end{theorem}

\clearpage

\hypertarget{Sx2-probability}{%
\section{Probability Reminders}\label{Sx2-probability}}

\hypertarget{independence}{%
\subsection{Independence}\label{independence}}

\begin{definition}
Two random variables \(X\) and \(Y\) are (statistically) \textbf{independent}, if
\(P(X\in A, Y\in B) = P(X\in A) P(Y\in B)\) for all sets \(A\) and~\(B\).
\end{definition}

We list some properties of independent random variables:

\begin{itemize}
\tightlist
\item
  If \(X\) and \(Y\) are independent, and if \(f\) and \(g\) are functions,
  then \(f(X)\) and \(g(Y)\) are also independent.
\end{itemize}

\hypertarget{chi-square}{%
\subsection{The Chi-Squared Distribution}\label{chi-square}}

\begin{definition}
\protect\hypertarget{def:chi-squared-dist}{}\label{def:chi-squared-dist}Let \(X_1, \ldots, X_\nu \sim \mathcal{N}(0, 1)\) be i.i.d. Then the
distribution of \(\sum_{i=1}^\nu X_i^2\) is called the
\(\chi^2\)-distribution with \(\nu\) degrees of freedom. The
distribution is denoted by~\(\chi^2(\nu)\).
\end{definition}

Some important results about the \(\chi^2\)-distribution are:

\begin{itemize}
\item
  \(\chi^2\)-distributed random variables are always positive.
\item
  If \(Y\sim \chi^2(\nu)\), then \(\mathbb{E}(Y) = \nu\) and \(\mathop{\mathrm{Var}}(Y) = 2\nu\).
\item
  The R command \texttt{pchisq(\textbar{}}\(x\)\texttt{,}\(\nu\)\texttt{)} gives the value
  \(\Phi_\nu(x)\) of the CDF of the \(\chi^2(\nu)\)-distribution.
\item
  The R command \texttt{qchisq(}\(\alpha\)\texttt{,}\(\nu\)\texttt{)} can
  be used to obtain the
  \(\alpha\)-quantile of the \(\chi^2(\nu)\)-distribution.
\item
  More properties can be found
  \href{https://en.wikipedia.org/wiki/Chi-squared_distribution}{on Wikipedia}.
\end{itemize}

\hypertarget{t}{%
\subsection{The t-distribution}\label{t}}

\begin{definition}
\protect\hypertarget{def:t-dist}{}\label{def:t-dist}Let \(Z \sim \mathcal{N}(0,1)\) and \(Y \sim \chi^2(\nu)\) be independent. Then
the distribution of
\begin{equation}
  T
  = \frac{\,Z\,}{\,\sqrt{Y / \nu}\,}  \label{eq:t-def}
\end{equation}
is called the \(t\)-distribution with \(\nu\) degrees of freedom.
This distribution is denoted by~\(t(\nu)\).
\end{definition}

Some important results about the \(t\)-distribution are:

\begin{itemize}
\item
  The \(t\)-distribution is symmetric: if \(T \sim t(\nu)\), then
  \(-T \sim t(\nu)\)
\item
  If \(T\sim t(\nu)\), then \(\mathbb{E}(T) = 0\).
\item
  The R command \texttt{pt(\textbar{}}\(x\)\texttt{,}\(\nu\)\texttt{)} gives the value
  \(\Phi_\nu(x)\) of the CDF of the \(t(\nu)\)-distribution.
\item
  The R command \texttt{qt(}\(\alpha\)\texttt{,}\(\nu\)\texttt{)} can
  be used to obtain the
  \(\alpha\)-quantile of the \(t(\nu)\)-distribution.
\item
  More properties can be found
  \href{https://en.wikipedia.org/wiki/Student\%27s_t-distribution}{on Wikipedia}.
\end{itemize}

\clearpage

\end{document}
