[["index.html", "MATH3714 Linear Regression and Robustness Preface", " MATH3714 Linear Regression and Robustness Jochen Voss University of Leeds, Semester 1, 2021–22 Preface From previous modules we know how to fit a regression line through points \\((x_1, y_1), \\ldots, (x_n, y_n) \\in\\mathbb{R}^2\\). The underlying model here is described by the equation \\[\\begin{equation*} y_i = \\alpha + \\beta x_i + \\varepsilon_i \\end{equation*}\\] for all \\(i \\in \\{1, 2, \\ldots, n\\}\\), and the aim is to find values for the intercept \\(\\alpha\\) and the slope \\(\\beta\\) such that the residuals \\(\\varepsilon_i\\) are as small as possible. This procedure, called simple linear regression, is illustrated in figure 0.1. Figure 0.1: An illustration of linear regression. Each of the black circles in the plot stands for one paired sample \\((x_i, y_i)\\). The regression line \\(x \\mapsto \\alpha + \\beta x\\), with intercept \\(\\alpha\\) and slope \\(\\beta\\), aims to predict the value of \\(y\\) using the observed value \\(x\\). For the marked sample \\((x_i, y_i)\\), the predicted \\(y\\)-value is \\(\\hat y\\). In this situation, the variable \\(x\\) is called a input, feature, or sometimes the explanatory variable or the “independent variable”. The variable \\(y\\) is called response or output, or sometimes the “dependent variable”, and \\(\\varepsilon\\) is called the residual or error. Extending the situation of simple linear regression, in this module we will consider multiple linear regression, where the response \\(y\\) is allowed to depend on several input variables. The corresponding model is now \\[\\begin{equation*} y_i = \\alpha + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip} + \\varepsilon_i \\end{equation*}\\] for all \\(i \\in \\{1, 2, \\ldots, p\\}\\), where \\(n\\) is still the number of observations, and \\(p\\) is now the number of inputs we observe for each sample. Note that for multiple linear regression, we still consider a single response for each sample, only the number of inputs has been increased. One way to deal with situations where there is more than one output would be to fit separate models for each output. We will discuss multiple linear regression in much detail; our discussion will be guided by three different aims of linear regression: Prediction: given a not previously observed value \\(x\\), try to predict the corresponding \\(y\\). In cases were the residuals \\(\\varepsilon_i\\) correspond to unwanted noise, the fitted values \\(\\hat y_i = \\alpha + \\beta x_i\\) can be considered to be de-noised versions of the observed values \\(y_i\\). By studying a fitted regression model, sometimes better understanding of the data can be achieved. For example, one could ask whether all of the \\(p\\) input variables carry information about the response \\(y\\). We will address these aims by considering different questions, like how to estimate the coefficients \\(\\alpha, \\beta_1, \\ldots, \\beta_p\\), how to assess model fit, or how to deal with outliers in the data. "],["S00-about.html", "About MATH3714 Notes and videos Lectures Workshops and Problem Sheets Discussion Board Software Assessments", " About MATH3714 This module is MATH3714 Linear Regression and Robustness. The module manager and lecturer is Dr Jochen Voss, and my email address is J.Voss@leeds.ac.uk. Notes and videos The main way I expect you to learn the material for this course is by reading these notes and by watching the accompanying videos. I will release two sections of notes each week, for a total of 22 sections. Reading mathematics is a slow process. Each section roughly corresponds to one traditional lecture, which would have taken 50 minutes. If you find yourself regularly getting through sections in much less than an hour, you’re probably not reading carefully enough through each sentence of explanation and each line of mathematics, including understanding the motivation as well as checking the accuracy. It is possible (but not recommended) to learn the material by only reading the notes and not watching the videos. It is not possible to learn the material by only watching the videos and not reading the notes. Since we will all be relying heavily on these notes, I’m even more keen than usual to hear about errors mathematical, typographical or otherwise. Please, please email me if think you may have found any. Lectures There will be one online synchronous “lecture” session each week, on Mondays at 2-3pm, with me, run through Microsoft Teams. These will not be “lectures” in the traditional sense of the term, but will be an opportunity to re-emphasise material you have already learned from notes and videos, to give extra examples, and to answer common student questions, with some degree of interactivity. I will assume you have completed all the work for the previous week by the time of the lecture, but I will not assume you’ve started the work for that week itself. I am very keen to hear about things you’d like to go through in the lectures; please email me with your suggestions. Workshops and Problem Sheets There will be 5 problem sheets, corresponding to workshops in weeks 2, 4, 6, 8 and 10. The main goal of the workshops will be to go over your answers to the problems sheets. My recommended approach to problem sheets and workshops is the following: Work through the problem sheet before the workshop, spending plenty of time on it, and making multiple efforts at questions you get stuck on. I recommend spending at least three hours on each problem sheet, in more than one block. Collaboration is encouraged when working through the problems, but I recommend writing up your work on your own. Take advantage of the workshops to ask for help or clarification on questions you weren’t able to complete. After the workshop, attempt again the questions you were previously stuck on. If you’re still unable to complete a question after this second round of attempts, then consult the solutions. Discussion Board I have set up a Microsoft Team for the course. I propose to use the “Discussion” channel there as a discussion board. This is a good place to post questions about material from the course, and — even better! — to help answer your colleagues’ questions. The idea is that you all as a group should help each other out. I will visit a couple of times a week to clarify if everybody is stumped by a question, or if there is disagreement. Software For the module we will use the statistical computing package R. This program is free software, and you can find the program and documentation at the R project homepage. In particular, R will be used in the (assessed) practial. My recommendation would be to install the RStudio environment, which includes R, on your own computer and use this for your work. (Choose the open source version, “RStudio Desktop”, on the download page.) Alternatively you can use RStudio or plain R on the university computers. Assessments Your final mark for the module will be based on a computer practical (20%) and a final exam (80%). For the practical (I believe it will take place in week 10) you will need to solve some problem using R and the methods you learned in the course and to present your results in a short report. "],["S01-simple.html", "Section 1 Simple Linear Regression 1.1 Residual Sum of Squares 1.2 Linear Regression as a Parameter Estimation Problem 1.3 Matrix Notation", " Section 1 Simple Linear Regression As a reminder, we consider simple linear regression in this section. My hope is, that all of you have seen this material before at some stage, e.g. in school or in some first or second year modules. In preparation for notation introduced in the next section, we rename the parameters from \\(\\alpha\\) and \\(\\beta\\) to the new names \\(\\beta_0\\) for the intercept and \\(\\beta_1\\) for the slope. 1.1 Residual Sum of Squares In simple linear regression, the aim is to find a regression line \\(y = \\beta_0 + \\beta_1 x\\), such that the line is “close” to given data points \\((x_1, y_1), \\ldots, (x_n, y_n) \\in\\mathbb{R}^2\\) for \\(i \\in \\{1, 2, \\ldots, n\\}\\). The ususal way to find \\(alpha\\) and \\(\\beta_1\\), and thus the regression line, is by minimising the residual sum of squares: \\[\\begin{equation} r(\\beta_0, \\beta_1) = \\sum_{i=1}^n \\bigl( y_i - (\\beta_0 + \\beta_1 x_i) \\bigr)^2. \\tag{1.1} \\end{equation}\\] For given \\(\\beta_0\\) and \\(\\beta_1\\), the value \\(r(\\beta_0, \\beta_1)\\) measures how close (in vertical direction) the given data points \\((x_i, y_i)\\) are to the regression line \\(\\beta_0 + \\beta_1 x\\). By minimising \\(r(\\beta_0, \\beta_1)\\) we find the regression line which is “closest” to the data. The solution of this minimisation problem is usually expressed in terms of the sample variance \\(\\mathrm{s}_x\\) and the sample covariance \\(\\mathrm{s}_{xy}\\). Definition 1.1 The sample covariance of \\(x_1, \\ldots, x_n \\in \\mathbb{R}\\) and \\(y_1, \\ldots, y_n\\in\\mathbb{R}\\) is given by \\[\\begin{equation*} \\mathrm{s}_{xy} := \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar x) (y_i - \\bar y), \\end{equation*}\\] where \\(\\bar x\\) and \\(\\bar y\\) are the sample means. The sample variance of \\(x_1, \\ldots, x_n \\in \\mathbb{R}\\) is given by \\[\\begin{equation*} \\mathrm{s}_{x}^2 := \\mathrm{s}_{xx} = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar x)^2, \\end{equation*}\\] where, again, \\(\\bar x\\) is the sample mean of the \\(x_i\\). Lemma 1.1 Assume that \\(\\mathrm{s}_x^2 &gt; 0\\). Then the function \\(r(\\beta_0, \\beta_1)\\) from (1.1) takes its minimum at the point \\((\\beta_0, \\beta_1)\\) given by \\[\\begin{equation*} \\hat\\beta_1 = \\frac{\\mathrm{s}_{xy}}{\\mathrm{s}_x^2}, \\qquad \\hat\\beta_0 = \\bar y - \\hat \\beta_1 \\bar x, \\end{equation*}\\] where \\(\\bar x, \\bar y\\) are the sample means, \\(\\mathrm{s}_{xy}\\) is the sample covariance and \\(\\mathrm{s}_x^2\\) is the sample variance. Proof. We could find the minimum of \\(r\\) by differentiating and setting the derivatives to zero. Here we follow a different approach which uses a “trick” to simplify the algebra: Let \\(\\tilde x_i = x_i - \\bar x\\) and \\(\\tilde y_i = y_i - \\bar y\\) for all \\(i \\in \\{1, \\ldots, n\\}\\). Then we have \\[\\begin{equation*} \\sum_{i=1}^n \\tilde x_i = \\sum_{i=1}^n x_i - n \\bar x = 0 \\end{equation*}\\] and, similarly, \\(\\sum_{i=1}^n \\tilde y_i = 0\\). Using the new coordinates \\(\\tilde x_i\\) and \\(\\tilde y_i\\) we find \\[\\begin{align*} r(\\beta_0, \\beta_1) &amp;= \\sum_{i=1}^n \\bigl(y_i - \\beta_0 - \\beta_1 x_i \\bigr)^2 \\\\ &amp;= \\sum_{i=1}^n \\bigl( \\tilde y_i + \\bar y - \\beta_0 - \\beta_1 \\tilde x_i - \\beta_1 \\bar x \\bigr)^2 \\\\ &amp;= \\sum_{i=1}^n \\Bigl( \\bigl( \\tilde y_i - \\beta_1 \\tilde x_i \\bigr) + \\bigl( \\bar y - \\beta_0 - \\beta_1 \\bar x \\bigr) \\Bigr)^2 \\\\ &amp;= \\sum_{i=1}^n \\bigl( \\tilde y_i - \\beta_1 \\tilde x_i \\bigr)^2 + 2 \\bigl( \\bar y - \\beta_0 - \\beta_1 \\bar x \\bigr) \\sum_{i=1}^n \\bigl( \\tilde y_i - \\beta_1 \\tilde x_i \\bigr) + n \\bigl( \\bar y - \\beta_0 - \\beta_1 \\bar x \\bigr)^2 \\end{align*}\\] Since \\(\\sum_{i=1}^n \\tilde x_i = \\sum_{i=1}^n \\tilde y_i = 0\\), the second term on the right-hand side vanishes and we get \\[\\begin{equation} r(\\beta_0, \\beta_1) = \\sum_{i=1}^n \\bigl( \\tilde y_i - \\beta_1 \\tilde x_i \\bigr)^2 + n \\bigl( \\bar y - \\beta_0 - \\beta_1 \\bar x \\bigr)^2. \\tag{1.2} \\end{equation}\\] Both of these terms are positive and we can minimise the second term (without changing the first term) by setting \\(\\beta_0 = \\bar y - \\beta_1 \\bar x\\). To find the value of \\(\\beta_1\\) which minimises the first term on the right-hand side of (1.2) we now set the (one-dimensional) derivative w.r.t. \\(\\beta_1\\) equal to \\(0\\). We get the condition \\[\\begin{align*} 0 &amp;\\overset{!}{=} \\frac{d}{d\\beta_1} \\sum_{i=1}^n \\bigl( \\tilde y_i - \\beta_1 \\tilde x_i \\bigr)^2 \\\\ &amp;= \\sum_{i=1}^n 2 \\bigl( \\tilde y_i - \\beta_1 \\tilde x_i \\bigr) \\frac{d}{d\\beta_1} \\bigl( \\tilde y_i - \\beta_1 \\tilde x_i \\bigr) \\\\ &amp;= - 2 \\sum_{i=1}^n \\bigl( \\tilde y_i - \\beta_1 \\tilde x_i \\bigr) \\tilde x_i \\\\ &amp;= -2 \\sum_{i=1}^n \\tilde x_i \\tilde y_i + 2 \\beta_1 \\sum_{i=1}^n \\tilde x_i^2. \\end{align*}\\] The only solution to this equation is \\[\\begin{align*} \\beta_1 &amp;= \\frac{\\sum_{i=1}^n \\tilde x_i \\tilde y_i}{\\sum_{i=1}^n \\tilde x_i^2} \\\\ &amp;= \\frac{\\sum_{i=1}^n (x_i - \\bar x) (y_i - \\bar y)}% {\\sum_{i=1}^n (x_i - \\bar x)^2} \\\\ &amp;= \\frac{\\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar x) (y_i - \\bar y)}% {\\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar x)^2} \\\\ &amp;= \\frac{\\mathrm{s}_{xy}}{\\mathrm{s}_x^2}. \\end{align*}\\] Since the second derivative is \\(2 \\sum_{i=1}^n \\tilde x_i^2 \\geq 0\\), this is indeed a minimum and the proof is complete. 1.2 Linear Regression as a Parameter Estimation Problem In statistics, any analysis starts by making a statistical model of the data. This is done by writing random variables which have the same structure as the data, and which are chosen so that the data “looks like” a random sample from these random variables. To construct a model for the data used in a simple linear regression problem, we use random variables \\(Y_1, \\ldots, Y_n\\) such that \\[\\begin{equation} Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\tag{1.3} \\end{equation}\\] for all \\(i \\in \\{1, 2, \\ldots, n\\}\\), where \\(\\varepsilon_1, \\ldots, \\varepsilon_n\\) are i.i.d. random variables with \\(\\mathbb{E}(\\varepsilon_i) = 0\\) and \\(\\mathop{\\mathrm{Var}}(\\varepsilon_i) = \\sigma^2\\). Here we assume that the \\(x\\)-values are fixed and known. The only random quantities in the model are \\(\\varepsilon_i\\) and \\(Y_i\\). (There are more complicated models which also allow for randomness of \\(x\\), but we won’t consider such models here.) The random variables \\(\\varepsilon_i\\) are called residuals or errors. In a scatter plot, the residuals correspond to the vertical distance between the samples and the regression line. Often one assumes that \\(\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\) for all \\(i \\in \\{1, 2, \\ldots, n\\}\\). The values \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\) are parameters of the model. To fit the model to data, we need to estimate these parameters. This model is more complex than the models considered in some introductory courses to statistics: The data consists now of pairs of numbers, instead of just single numbers. We have \\[\\begin{equation*} \\mathbb{E}(Y_i) = \\mathbb{E}\\bigl( \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\bigr) = \\beta_0 + \\beta_1 x_i + \\mathbb{E}(\\varepsilon_i) = \\beta_0 + \\beta_1 x_i. \\end{equation*}\\] Thus, the expectation of \\(Y_i\\) depends on \\(x_i\\) and, at least for \\(\\beta_1 \\neq 0\\), the random variables \\(Y_i\\) are not identically distributed. In this setup, we can consider the estimates \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) from the previous subsection as statistcal parameter estimates for the model parameters \\(\\beta_0\\) and \\(\\beta_1\\). In order to fit a linear model we also need to estimate the residual variance \\(\\sigma^2\\). This can be done using the estimator \\[\\begin{equation} \\hat\\sigma^2 = \\frac{1}{n-2} \\sum_{i=1}^n \\hat\\varepsilon_i^2 = \\frac{1}{n-2} \\sum_{i=1}^n (y_i - \\hat\\beta_0 - \\hat\\beta_1 x_i)^2. \\tag{1.4} \\end{equation}\\] To understand the form of this estimator, we have to remember that \\(\\sigma^2\\) is the variance of the \\(\\varepsilon_i\\). Thus, using the standard estimator for the variance, we could estimate \\(\\sigma^2\\) as \\[\\begin{equation} \\sigma^2 \\approx \\frac{1}{n-1} \\sum_{i=1}^n \\bigl(\\varepsilon_i - \\bar\\varepsilon\\bigr)^2 \\approx \\frac{1}{n-1} \\sum_{i=1}^n \\bigl(\\hat\\varepsilon_i - \\overline{\\hat\\varepsilon}\\bigr)^2, \\tag{1.5} \\end{equation}\\] where \\(\\bar\\varepsilon\\) and \\(\\overline{\\hat\\varepsilon}\\) are the averages of the \\(\\varepsilon_i\\) and the \\(\\hat\\varepsilon_i\\), respectively. One can show that \\(\\overline{\\hat\\varepsilon} = 0\\). The estimates of \\(\\beta_0\\) and \\(\\beta_1\\) are sensitive to fluctuations in the data, with the effect that the estimated regression line is, on average, slightly closer to the data points than the true regression line would be. This causes the sample variance of the \\(\\hat\\varepsilon_i\\), on average, to be slightly smaller than the true residual variance \\(\\sigma^2\\) and the thus the estimator (1.5) is slightly biased. A more detailed analysis reveals that an unbiased estimator can be obtained if one replaces the pre-factor \\(1/(n-1)\\) in equation (1.5) with \\(1/(n-2)\\). This leads to the estimator (1.4). The main advantage gained by considering a statistical model is, that we now can consider how close the estimators \\(\\hat\\beta_0\\), \\(\\hat\\beta_1\\) and \\(\\hat\\sigma^2\\) are to the true values. Results one can obtain include the following: The estimators \\(\\hat\\beta_0\\), \\(\\hat\\beta_1\\) and \\(\\hat\\sigma^2\\) are unbiased: This means that when we plug in random data \\((x_i, Y_i)\\) from the model (1.3), on average we get the correc answer: \\(\\mathbb{E}(\\hat\\beta_0) = \\beta_0\\), \\(\\mathbb{E}(\\hat\\beta_1) = \\beta_1\\), \\(\\mathbb{E}(\\hat\\sigma^2) = \\sigma^2\\). One can ask about the average distance between the estimated parameters \\(\\hat\\beta_0\\), \\(\\hat\\beta_1\\) and \\(\\hat\\sigma^2\\) and the (unknown) true values \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). One measure for these distances is the root mean squared error of the estimators. One can consider confidence intervals for the parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). One can consider statistical hypothesis tests to answer yes/no questions about the parameters. For example, one might ask whether the data could have come from the model with \\(\\beta_0=0\\). One can consider whether the data is compatible with the model at all, irrespective of parameter values. If there is a non-linear relationship between \\(x\\) and \\(y\\), the model (1.3) will no longer be appropriate. We will consider most of these questions over the course of the module. 1.3 Matrix Notation To conclude this section, we will rewrite the results of this section in a form which we will extensively use for multiple linear regression in the rest of this module. The idea here is to arrange all quantities in the problem as matrices and vectors in order to simplify notation. We write \\[\\begin{equation*} X = \\begin{pmatrix} 1 &amp; x_1\\\\ 1 &amp; x_2\\\\\\vdots &amp; \\vdots\\\\1 &amp; x_n \\end{pmatrix} \\in \\mathbb{R}^{n\\times 2}, \\qquad y = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix} \\in \\mathbb{R}^n, \\qquad \\varepsilon= \\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix} \\in \\mathbb{R}^n, \\qquad \\beta = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix} \\in\\mathbb{R}^2 \\end{equation*}\\] Using this notation, we can rewrite the \\(n\\) equations \\(y_i = \\beta_0 + x_i \\beta_1 + \\varepsilon_i\\) for \\(i \\in \\{1, \\ldots, n\\}\\) as one vector-valued equation in \\(\\mathbb{R}^n\\): we get \\[\\begin{equation*} y = X\\beta + \\varepsilon, \\end{equation*}\\] and we want to “solve” this vector-valued equation for \\(\\beta\\). The sum of squares can now be written as \\[\\begin{equation*} r(\\beta) = \\sum_{i=1}^n \\varepsilon_i^2 = \\varepsilon^\\top \\varepsilon = (y - X\\beta)^\\top (y - X\\beta) = y^\\top y - 2\\beta^\\top X^\\top y + \\beta^\\top X^\\top X \\beta. \\end{equation*}\\] In the next section we will see that the minimum of \\(r\\) is attained for \\[\\begin{equation*} \\hat\\beta = (X X^\\top)^{-1} X^\\top y \\end{equation*}\\] and one can check that the components of this vector \\(\\hat\\beta = (\\hat\\beta_0, \\hat\\beta_1)\\) coincide with the estimates we obtained above. Summary simple linear regression is the case where there is only one input a regression line is fitted by minimising the residual sum of squares linear regression is a statistical parameter estimation problem the problem can be conveniently written in matrix/vector notation "],["S02-multiple.html", "Section 2 Least Squares Estimates 2.1 Data and Models 2.2 The Normal Equations 2.3 Fitted Values 2.4 Example", " Section 2 Least Squares Estimates 2.1 Data and Models For multiple linear regression we assume that there are \\(p\\) inputs and one output. If we have a sample of \\(n\\) obervations, we have \\(np\\) inputs and one output in total. Here we denote the \\(i\\)th observation of the \\(j\\)th input by \\(x_{ij}\\) and the corresponding output by \\(y_j\\). As an example, we consider the mtcars dataset built into R. This is a small dataset, which contains information about 32 automobiles (1973–74 models). The table lists fuel consumption mpg, gross horsepower hp, and 9 other aspects of these cars. Here we consider mpg to be the output, and the other listed aspects to be inputs. Type help(mtcars) in R to learn more about this dataset: mtcars ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 For this dataset we have \\(n = 32\\) (number of cars), and \\(p = 10\\) (number of attributes, excluding mpg). The values \\(y_1, \\ldots, y_{32}\\) are listed in the first column of the table, the values \\(x_{i,1}\\) for \\(i \\in \\{1, \\ldots, 32\\}\\) are shown in the second column, and the values \\(x_{i,10}\\) are shown in the last column. In this data set it is easy to make scatter plots which show how a single input affects the output. For example, we can show how the engine power affects fuel consumption: plot(mtcars$hp, mtcars$mpg, xlab = &quot;power [hp]&quot;, ylab = &quot;fuel consumption [mpg]&quot;) We can see that cars with stronger engines tend to use more fuel (i.e. a gallon of fuel lasts for fewer miles; the curve goes down), but leaving out the other inputs omits a lot of information. It is not easy to make a plot which takes all inputs into account. Is is also not immediately obvious which of the variables are most important. In linear regression, we assume that the output depends on the inputs in a linear (or more precisely, affine) way. We write this as \\[\\begin{equation} y_i = \\beta_0 + \\beta_1 x_{i,1} + \\cdots + \\beta_p x_{i,p} + \\varepsilon_i \\tag{2.1} \\end{equation}\\] where the residuals \\(\\varepsilon_i\\) are assumed to be “small”. The parameters \\(\\beta_j\\) can be interpreted as the expected change in the response \\(y\\) per unit change in \\(x_j\\) when all other regressor variables are held fixed. For this reason the parameters \\(\\beta_j\\) (for \\(j=1, \\ldots, p\\)) are sometimes called partial regression coefficients. This model describes a hyperplane in the \\((p+1)\\)-dimensional space of the inputs \\(x_j\\) and the output \\(y\\). The hyperplane is easily visualized when \\(p=1\\) (as a line in \\(\\mathbb{R}^2\\)), and visualisation can be attempted for \\(p=2\\) (as a plane in \\(\\mathbb{R}^3\\)) but is very hard for \\(p&gt;2\\). We defer making a proper statistical model for multiple linear regression until the next section. 2.2 The Normal Equations Similar to what we did in Section 1.3, we rewrite the model using matrix notation. We define the vectors \\[\\begin{equation*} y = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix} \\in \\mathbb{R}^n, \\qquad \\varepsilon= \\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix} \\in \\mathbb{R}^n, \\qquad \\beta = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{pmatrix} \\in\\mathbb{R}^{1+p} \\end{equation*}\\] as well as the matrix \\[\\begin{equation*} X = \\begin{pmatrix} 1 &amp; x_{1,1} &amp; \\cdots &amp; x_{1,p} \\\\ 1 &amp; x_{2,1} &amp; \\cdots &amp; x_{2,p} \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 1 &amp; x_{n,1} &amp; \\cdots &amp; x_{n,p} \\\\ \\end{pmatrix} \\in \\mathbb{R}^{n\\times (1+p)}. \\end{equation*}\\] The matrix \\(X\\) is often called the design matrix. Using this notation, the model (2.1) can be written as \\[\\begin{equation} y = X \\beta + \\varepsilon, \\tag{2.2} \\end{equation}\\] where again \\(X\\beta\\) is a matrix-vector multiplication which “hides” the sums in equation (2.1), and (2.2) is an equation of vectors of size \\(n\\), which combines the \\(n\\) individual equations from (2.1) for the different values of \\(i\\). To simplify notation, we index the columns of \\(X\\) by \\(0, 1, \\ldots, p\\) (instead of the more conventional \\(1, \\ldots, p+1\\)), so that we can for example write \\[\\begin{equation*} (X \\beta)_i = \\sum_{j=0}^p x_{i,j} \\beta_j = \\beta_0 + \\sum_{j=1}^p x_{i,j} \\beta_j. \\end{equation*}\\] As before, we find the regression coefficients by minimising the residual sum of squares: \\[\\begin{align*} r(\\beta) &amp;= \\sum_{i=1}^n \\varepsilon_i^2 \\\\ &amp;= \\sum_{i=1}^n \\bigl( y_i - (\\beta_0 + \\beta_1 x_{i,1} + \\cdots + \\beta_p x_{i,p}) \\bigr)^2. \\end{align*}\\] In practice, this notation turns out to be cumbersome, and we will use matrix notation in the following proof. Lemma 2.1 Assume that the matrix \\(X^\\top X \\in \\mathbb{R}^{(1+p) \\times (1+p)}\\) is invertible. Then the function \\(r(\\beta)\\) takes its minimum at the vector \\(\\hat\\beta\\in\\mathbb{R}^{p+1}\\) given by \\[\\begin{equation*} \\hat\\beta = (X^\\top X)^{-1} X^\\top y. \\end{equation*}\\] Proof. Using the vector equation \\(\\varepsilon= y - X \\beta\\), we can also write the residual sum of squares as \\[\\begin{align*} r(\\beta) &amp;= \\sum_{i=1}^n \\varepsilon_i^2 \\\\ &amp;= \\varepsilon^\\top \\varepsilon\\\\ &amp;= (y - X \\beta)^\\top (y - X \\beta) \\\\ &amp;= y^\\top y - y^\\top X\\beta - (X\\beta)^\\top y + (X\\beta)^\\top (X\\beta). \\end{align*}\\] Using the linear algebra rules from Appendix A.2 we find that \\(y^\\top X\\beta = (X\\beta)^\\top y = \\beta^\\top X^\\top y\\) and \\((X\\beta)^\\top (X\\beta) = \\beta^\\top X^\\top X \\beta\\). Thus we get \\[\\begin{equation*} r(\\beta) = y^\\top y - 2\\beta^\\top X^\\top y + \\beta^\\top X^\\top X \\beta. \\end{equation*}\\] Note that in this eqation \\(X\\) is a matrix, \\(y\\) and \\(\\beta\\) are vectors, and \\(r(\\beta)\\) is a number. To find the minimum of this function, we set all partial derivatives \\(\\frac{\\partial}{\\partial \\beta_i} r(\\beta)\\) equal to \\(0\\). Going through the terms in the formula for \\(r(\\beta)\\) we find: (1) \\(y^\\top y\\) does not depend on \\(\\beta\\), so we have \\(\\frac{\\partial}{\\partial \\beta_i} y^\\top y = 0\\) for all \\(i\\), (2) we have \\[\\begin{equation*} \\frac{\\partial}{\\partial \\beta_i} \\beta^\\top X^\\top y = \\frac{\\partial}{\\partial \\beta_i} \\sum_{j=1}^{p+1} \\beta_j (X^\\top y)_j = (X^\\top y)_i \\end{equation*}\\] and (3) finally \\[\\begin{equation*} \\frac{\\partial}{\\partial \\beta_i} \\beta^\\top X^\\top X \\beta = \\frac{\\partial}{\\partial \\beta_i} \\sum_{j,k=1}^{p+1} \\beta_j (X^\\top X)_{j,k} \\beta_k = 2 \\sum_{k=1}^{p+1} (X^\\top X)_{i,k} \\beta_k = 2 \\bigl( (X^\\top X) \\beta \\bigr)_i. \\end{equation*}\\] (Some care is needed, when checking that the middle equality sign in the previous equation is correct.) Combining these derivatives, we find \\[\\begin{equation} \\frac{\\partial}{\\partial \\beta_i} r(\\beta) = 0 - 2 (X^\\top y)_i + 2 \\bigl( X^\\top X \\beta \\bigr)_i \\tag{2.3} \\end{equation}\\] for all \\(i \\in \\{0, 1, \\ldots, p\\}\\). At a local minimum of \\(r\\), all of these partial derivatives must be zero and using a vector equation we find that a necessary condition for a minimum is \\[\\begin{equation} X^\\top X \\beta = X^\\top y. \\tag{2.4} \\end{equation}\\] Since we assumed that \\(X^\\top X\\) is invertible, there is exactly one vector \\(beta\\) which solves (2.4). This vector is given by \\[\\begin{equation*} \\hat\\beta := (X^\\top X)^{-1} X^\\top y. \\end{equation*}\\] As for one-dimensional minimisation, there is a condition on the second derivatives which must be checked to see which local extrema are local minima. Here we are only going to sketch this argument: A sufficient condition for \\(\\hat\\beta\\) to be a minimum is for the second derivative matrix (the Hessian matrix) to be positive definite (see appendix A.2.6). Using equation (2.3) we find \\[\\begin{equation*} \\frac{\\partial}{\\partial\\beta_i \\partial\\beta_j} r(\\beta) = 2 (X^\\top X)_{i,j} \\end{equation*}\\] And thus the Hessian matrix is \\(H = 2 X^\\top X\\). Using results from linear algebra, one can show that this matrix is indeed positive definite and thus \\(\\hat\\beta\\) is the unique minimum of \\(r\\). Equation (2.4) gives a system of \\(p+1\\) linear equations with \\(p+1\\) unknowns. This system of linear equations, \\(X^\\top X \\beta = X^\\top y\\) is called the normal equations. If \\(X^\\top X\\) is invertible, as assumed in the lemma, this system of equations has \\(\\hat\\beta\\) as its unique solution. Otherwise, there may be more than one \\(\\beta\\) which leads to the same value \\(r(\\beta)\\) and the minimum will no longer be unique. This happens for example, if two of the inputs are identical to each other (or, more generally, one input is linearly dependent on one or more other inputs). The condition that \\(X^\\top X\\) must be invertible in multiple linear regression corresponds to the condition \\(\\mathrm{s}_x^2 &gt; 0\\) from lemma 1.1 for simple linear regression. The value \\(\\hat\\beta\\) found in the lemma is called the least squares estimator for \\(\\beta\\), or sometimes the ordinary least squares (OLS) estimator. 2.3 Fitted Values Let us again consider our model \\[\\begin{equation*} y = X \\beta + \\varepsilon, \\end{equation*}\\] using the matrix notation introduced above. Here we can think of \\(X\\beta\\) as the true values, while \\(\\varepsilon\\) are the errors. The design matrix \\(X\\) (containing the inputs) and the response \\(y\\) are known to us, while the true coefficients \\(\\beta\\) and the errors \\(\\varepsilon\\) are unknown. Solving for \\(\\varepsilon\\) we find that the errors satisfy \\[\\begin{equation*} \\varepsilon= y - X\\beta. \\end{equation*}\\] Using the least squares estimate \\(\\hat\\beta\\) we can estimate the true values as \\[\\begin{equation} \\hat y = X \\hat\\beta. \\tag{2.5} \\end{equation}\\] These estimates are called the fitted values. Using the definition of \\(\\hat\\beta\\) we get \\[\\begin{equation*} \\hat y = X (X^\\top X)^{-1} X^\\top y =: Hy. \\end{equation*}\\] The matrix \\(H = X (X^\\top X)^{-1} X^\\top\\) is commonly called the hat matrix (because it “puts the hat on \\(y\\)”). Finally, we can estimate the errors using the residuals \\[\\begin{equation} \\hat\\varepsilon = y - X \\hat\\beta = y - \\hat y = y - H y = (I - H) y, \\tag{2.6} \\end{equation}\\] where \\(I\\) is the \\((p+1)\\times (p+1)\\) identity matrix. 2.4 Example To conclude this section, we demonstrate how these methods can be used in R. For this we consider the mtcars example from the beginning of the section again. I will first show how to do the analysis “by hand”, and later show how the same result can be obtained using R’s built-in functions. We first split mtcars into the respons column y (the first column) and the design matrix X (a column of ones, followed by columns 2 to 11 of mtcars): y &lt;- mtcars[, 1] X &lt;- cbind(1, data.matrix(mtcars[, 2:11])) Next we compute \\(X^\\top X\\) and solve the normal equations. Often it is faster, easier, and has lower numerical errors to solve the normal equations rather than inverting the matrix \\(X^\\top X\\). XtX &lt;- t(X) %*% X beta.hat &lt;- solve(XtX, t(X) %*% y) beta.hat ## [,1] ## 12.30337416 ## cyl -0.11144048 ## disp 0.01333524 ## hp -0.02148212 ## drat 0.78711097 ## wt -3.71530393 ## qsec 0.82104075 ## vs 0.31776281 ## am 2.52022689 ## gear 0.65541302 ## carb -0.19941925 Without further checks it is hard to know whether the result is correct, or whether we made a mistake somewhere along the lines. One good sign is that we argued earlier that higher hp should lead to lower mpg, and indeed the corresponding coefficient -0.02148212 is negative. Finally, compute the fitted values and generate a plot of fitted values against responses. If everything worked, we would expect the points in this plot to be close to the diagonal. y.hat &lt;- X %*% beta.hat plot(y, y.hat, xlab = &quot;responses&quot;, ylab = &quot;fitted values&quot;) abline(a = 0, b = 1) # plot the diagonal For comparison we now re-do the analysis using built-in R commands. In the lm() command below, we use data=mtcars to tell R where the data is stored, and the formula mpg ~ . states that we want to model mpg as a function of all other variable (this is the meaning of .). m &lt;- lm(mpg ~ ., data = mtcars) # fit a linear model coef(m) # get the estimated coefficients ## (Intercept) cyl disp hp drat wt ## 12.30337416 -0.11144048 0.01333524 -0.02148212 0.78711097 -3.71530393 ## qsec vs am gear carb ## 0.82104075 0.31776281 2.52022689 0.65541302 -0.19941925 Comparing these coefficients to the vector beta.hat from above shows that we got the same result using both methods. The fitted values can be computed using fitted.values(m). Here we just check that we get the same result as above: max(abs(fitted.values(m) - y.hat)) ## [1] 5.329071e-13 This results 5.329071e-13 stands for the number \\(5.329071 \\cdot 10^{-13}\\), which is extremely small. The difference between our results and R’s result is caused by rounding errors. Summary multiple linear regression allows for more than one input but still has only one output the least squared estimate for the coefficients is found by minimising the residual sum of squares the estimate can be computed as the solution to the normal equations the hat matrix transforms responses into fitted values "],["I01-lm.html", "Interlude: Linear Regression in R Fitting a Model Understanding the Model Making Predictions", " Interlude: Linear Regression in R Fitting a Model The function lm() is used to fit a linear model in R. There are different ways to specify the form of the model and the data to be used for fitting the model. The most basic way to call lm() is the case where the explanatory variables and the response variable are stored as separate vectors. Assuming, for example, that the explanatory variables are x1, x2, x3 and that the response variable is y in R, we can tell R to fit the linear model \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\varepsilon\\) by using the following command: lm(y ~ x1 + x2 + x3) Note that R automatically added the intercept term \\(\\beta_0\\) to this model. If we want to fit a model without an intercept, i.e. the model \\(y = \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\varepsilon\\), we have to add 0 + in front of the explanatory variables: lm(y ~ 0 + x1 + x2 + x3) The general form of a model specification is the response variable, followed by ~, followed by a plus-separated list of explanatory variables. For this form of calling lm(), the variables y, x1, x2, and x3 in the examples above must be already defined before lm() is called. It may be a good idea to double-check that the variables have the correct values before trying to call lm(). Both for the response and for explanatory variables we can specify arbitrary R expressions to compute the numeric values to be used. For example, to fit the model \\(\\log(y) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon\\) (assuming that all \\(y_i\\) are positive) we can use the following command: lm(log(y) ~ x1 + x2) Some care is needed, because +, * and ^ have a special meaning inside the first argument of lm(); any time we want to compute a variable for lm() using these operations, we need to surround the corresponding expression with I(), to tell R that +, * or ^ should have their usual, arithmetic meaning. For example, to fit a model of the form \\(y \\sim \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\varepsilon\\), we can use the following R command: lm(y ~ x + I(x^2)) Here, the use of I() tells R that x^2 is to be interpreted as the vector \\((x_1^2, \\ldots, x_n^2)\\). Similarly, we can fit a model of the form \\(y = \\beta_0 + \\beta_1 (x_1 + x_2) + \\varepsilon\\): lm(y ~ I(x1+x2)) Here, the use of I() tells R that x1+x2 indicates the vector \\((x_{1,1}+x_{2,1}, \\ldots, x_{1,n}+x_{2,n})\\) instead of two separate explanatory variables. Details about how to specify models in calls to lm() can be found by using the command help(formula) in R. If the response and the explanatory variables are stored in the columns of a data frame, we can use the data=... argument to lm() to specify this data frame and then just use the column names to specify the regression model. For example, the stackloss data set built into R consists of a data frame with columns Air.Flow, Water.Temp, Acid.Conc., stack.loss. To predict stackloss$stack.loss from stackloss$Air.Flow we can write lm(stack.loss ~ Air.Flow, data=stackloss) As a special case, a single dot “.” can be used in place of the explanatory variables in the model to indicate that all columns except for the given response should be used. Thus, the following two commands are equivalent: lm(stack.loss ~ ., data=stackloss) lm(stack.loss ~ Air.Flow + Water.Temp + Acid.Conc., data=stackloss) Understanding the Model The output of the lm() function is an R object which can be used the extract information about the fitted model. A good way to work with this object is to store it in a variable and then use commands like the ones listed below to work with this variable. For example, the following R command fits a model for the stackloss data set and stores it in the variable m: m &lt;- lm(stack.loss ~ ., data=stackloss) Many operations are available to use with this object m: Printing m to the screen: m ## ## Call: ## lm(formula = stack.loss ~ ., data = stackloss) ## ## Coefficients: ## (Intercept) Air.Flow Water.Temp Acid.Conc. ## -39.9197 0.7156 1.2953 -0.1521 This shows the estimated values for the regression coefficient. The command summary() can be used to print additional information about the fitted model: summary(m) ## ## Call: ## lm(formula = stack.loss ~ ., data = stackloss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.2377 -1.7117 -0.4551 2.3614 5.6978 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -39.9197 11.8960 -3.356 0.00375 ** ## Air.Flow 0.7156 0.1349 5.307 5.8e-05 *** ## Water.Temp 1.2953 0.3680 3.520 0.00263 ** ## Acid.Conc. -0.1521 0.1563 -0.973 0.34405 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.243 on 17 degrees of freedom ## Multiple R-squared: 0.9136, Adjusted R-squared: 0.8983 ## F-statistic: 59.9 on 3 and 17 DF, p-value: 3.016e-09 We will learn over the course of this module how to interpret most of this output. The coefficient vector \\(\\beta\\) can be obtained using coef(m): coef(m) ## (Intercept) Air.Flow Water.Temp Acid.Conc. ## -39.9196744 0.7156402 1.2952861 -0.1521225 The fitted values \\(\\hat y_i\\) can be obtained using the command fitted(m): fitted(m) ## 1 2 3 4 5 6 7 8 ## 38.765363 38.917485 32.444467 22.302226 19.711654 21.006940 21.389491 21.389491 ## 9 10 11 12 13 14 15 16 ## 18.144379 12.732806 11.363703 10.220540 12.428561 12.050499 5.638582 6.094949 ## 17 18 19 20 21 ## 9.519951 8.455093 9.598257 13.587853 22.237713 The estimated residuals \\(\\hat\\varepsilon_i = y_i - \\hat y_i\\) can be obtained using the command resid(m): resid(m) ## 1 2 3 4 5 6 ## 3.23463723 -1.91748529 4.55553300 5.69777417 -1.71165358 -3.00693970 ## 7 8 9 10 11 12 ## -2.38949071 -1.38949071 -3.14437890 1.26719408 2.63629676 2.77946036 ## 13 14 15 16 17 18 ## -1.42856088 -0.05049929 2.36141836 0.90505080 -1.51995059 -0.45509295 ## 19 20 21 ## -0.59825656 1.41214728 -7.23771286 The design matrix \\(X\\) can be found using model.matrix(m): model.matrix(m) ## (Intercept) Air.Flow Water.Temp Acid.Conc. ## 1 1 80 27 89 ## 2 1 80 27 88 ## 3 1 75 25 90 ## 4 1 62 24 87 ## 5 1 62 22 87 ## 6 1 62 23 87 ## 7 1 62 24 93 ## 8 1 62 24 93 ## 9 1 58 23 87 ## 10 1 58 18 80 ## 11 1 58 18 89 ## 12 1 58 17 88 ## 13 1 58 18 82 ## 14 1 58 19 93 ## 15 1 50 18 89 ## 16 1 50 18 86 ## 17 1 50 19 72 ## 18 1 50 19 79 ## 19 1 50 20 80 ## 20 1 56 20 82 ## 21 1 70 20 91 ## attr(,&quot;assign&quot;) ## [1] 0 1 2 3 Making Predictions One of the main aims of fitting a linear model is to use the model to make predictions for new, not previously observed \\(x\\)-values, i.e. to compute \\(y_{\\mathrm{new}} = X_{\\mathrm{new}} \\hat\\beta\\). The general form of the command for prediction is predict(m, newdata), where m is the model previously fitted using lm(), and newdata specifies the new \\(x\\)-values to predict responses for. The argument new.data should be a data.frame and for each variable in the original model there should be a column in newdata which has the name of the original variable and contains the new values. For example, if the model was fitted using m &lt;- lm(y ~ x + I(x^2)) and if the new samples are stored in x.new, then responses for the \\(x\\)-values in x.new can be predicted using the following command: predict(m, data.frame(x=x.new)) As a second example, for the stackloss data set, the following commands can be used to predict stack.loss for two new \\(x\\)-values: m &lt;- lm(stack.loss ~ ., data=stackloss) new.data &lt;- data.frame(Air.Flow=c(70, 73), Water.Temp=c(25,24), Acid.Conc.=c(78,90)) predict(m, new.data) ## 1 2 ## 30.69174 29.71790 More information about predict() can be found by reading the output of help(predict.lm). "],["P01.html", "Problem Sheet 1", " Problem Sheet 1 .fold-btn { float: right; margin: -12px 0 0 0; } You should attempt all these questions and write up your solutions in advance of your workshop in week 3 where the answers will be discussed. 1 Consider the simple linear regression model \\(y_i = \\beta_0 + x_{i} \\beta_1 + \\varepsilon_i\\) for \\(i \\in \\{1, 2, \\ldots, n\\}\\) and let \\(X\\) be the design matrix. Show that \\(\\displaystyle X^\\top X = \\begin{pmatrix} n &amp; \\sum_{i=1}^n x_i \\\\ \\sum_{i=1}^n x_i &amp; \\sum_{i=1}^n x_i^2 \\end{pmatrix} \\in \\mathbb{R}^{2\\times 2}\\). For simple linear regression, we have \\(p=1\\) and the design matrix is \\[\\begin{equation*} X = \\begin{pmatrix} 1 &amp; x_1 \\\\ 1 &amp; x_2 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n \\end{pmatrix}. \\end{equation*}\\] Thus we have \\[\\begin{align*} X^\\top X &amp;= \\begin{pmatrix} 1 &amp; 1 &amp; \\cdots &amp; 1 \\\\ x_1 &amp; x_2 &amp; \\cdots &amp; x_n \\end{pmatrix} \\begin{pmatrix} 1 &amp; x_1 \\\\ 1 &amp; x_2 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n \\end{pmatrix} \\\\ &amp;= \\begin{pmatrix} \\sum_{i=1}^n 1 &amp; \\sum_{i=1}^n 1 \\cdot x_i \\\\ \\sum_{i=1}^n x_i \\cdot 1 &amp; \\sum_{i=1}^n x_i^2 \\end{pmatrix} \\\\ &amp;= \\begin{pmatrix} n &amp; \\sum_{i=1}^n x_i \\\\ \\sum_{i=1}^n x_i &amp; \\sum_{i=1}^n x_i^2 \\end{pmatrix}. \\end{align*}\\] Using the formula \\[\\begin{equation*} \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix}^{-1} = \\frac{1}{ad-bc} \\begin{pmatrix} \\;d &amp; -b \\\\ -c &amp; \\,a \\end{pmatrix}, \\end{equation*}\\] find \\((X^\\top X)^{-1}\\). Using the formula for the inverse of a \\(2\\times 2\\)-matrix, we find \\[\\begin{align*} (X^\\top X)^{-1} &amp;= \\begin{pmatrix} n &amp; \\sum_{i=1}^n x_i \\\\ \\sum_{i=1}^n x_i &amp; \\sum_{i=1}^n x_i^2 \\end{pmatrix}^{-1} \\\\ &amp;= \\frac{1}{n \\sum_{i=1}^n x_i^2 - \\bigl(\\sum_{i=1}^n x_i\\bigr)^2} \\begin{pmatrix} \\sum_{i=1}^n x_i^2 &amp; -\\sum_{i=1}^n x_i \\\\ -\\sum_{i=1}^n x_i &amp; n \\end{pmatrix}. \\end{align*}\\] Find \\(X^\\top y\\) and use this to derive an explicit formula for the least squares estimate \\(\\hat\\beta = (X^\\top X)^{-1} X^\\top y\\). Omitting the indices in the sums for brevity, we have \\[\\begin{equation*} X^\\top y = \\begin{pmatrix} 1 &amp; 1 &amp; \\cdots &amp; 1 \\\\ x_1 &amp; x_2 &amp; \\cdots &amp; x_n \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix} = \\begin{pmatrix} \\sum y_i \\\\ \\sum x_i y_i \\end{pmatrix} \\end{equation*}\\] and thus \\[\\begin{align*} \\hat\\beta &amp;= (X^\\top X)^{-1} X^\\top y \\\\ &amp;= \\frac{1}{n \\sum x_i^2 - \\bigl(\\sum x_i\\bigr)^2} \\begin{pmatrix} \\sum x_i^2 &amp; -\\sum x_i \\\\ -\\sum x_i &amp; n \\end{pmatrix} \\begin{pmatrix} \\sum y_i \\\\ \\sum x_i y_i \\end{pmatrix} \\\\ &amp;= \\frac{1}{n \\sum x_i^2 - \\bigl(\\sum x_i\\bigr)^2} \\begin{pmatrix} \\sum x_i^2 \\sum y_i - \\sum x_i \\sum x_i y_i \\\\ -\\sum x_i \\sum y_i + n \\sum x_i y_i \\end{pmatrix} \\\\ &amp;= \\frac{1}{\\frac1n \\sum_{i=1}^n x_i^2 - \\bigl(\\frac1n \\sum_{i=1}^n x_i\\bigr)^2} \\begin{pmatrix} \\frac1n \\sum_{i=1}^n x_i^2 \\cdot \\frac1n\\sum_{i=1}^n y_i - \\frac1n\\sum_{i=1}^n x_i \\cdot \\frac1n\\sum_{i=1}^n x_i y_i \\\\ \\frac1n\\sum_{i=1}^n x_i y_i - \\frac1n\\sum_{i=1}^n x_i \\cdot \\frac1n\\sum_{i=1}^n y_i \\end{pmatrix}. \\end{align*}\\] This completes the answer. Inspection of the final result shows that we have recovered the traditional formula for the coefficients in simple linear regression, only written in slightly unusual form. For example, the term \\(\\frac1n \\sum_{i=1}^n x_i^2 - \\bigl(\\frac1n \\sum_{i=1}^n x_i\\bigr)^2\\) equals the sample variance of the \\(x_i\\) (up to a factor of \\(n/(n-1)\\)). The algebra in this answer could be slightly simplified by changing to new coordinates \\(\\tilde x_i = x_i - \\bar x\\) and \\(\\tilde y_i = y_i - \\bar y\\) before fitting the regression model. 2 Let \\(H = X (X^\\top X)^{-1} X^\\top \\in\\mathbb{R}^{n\\times n}\\) be the hat matrix and \\(\\mathbf{1} = (1, 1, \\ldots, 1) \\in\\mathbb{R}^n\\). Show that \\(H \\mathbf{1} = \\mathbf{1}\\). We already know that \\(H X = X (X^\\top X)^{-1} X^\\top X = X\\). Since the first column of \\(X\\) equals \\(\\mathbf{1}\\), the first column of the matrix equation \\(HX = X\\) is \\(H\\mathbf{1} = \\mathbf{1}\\). This completes the proof. 3 For the stackloss data set built into R, predict a value for stack.loss when the inputs are Air.Flow = 60, Water.Temp = 21 and Acid.Conc = 87. We can fit the model using lm() as usual: m &lt;- lm(stack.loss ~ ., data = stackloss) m ## ## Call: ## lm(formula = stack.loss ~ ., data = stackloss) ## ## Coefficients: ## (Intercept) Air.Flow Water.Temp Acid.Conc. ## -39.9197 0.7156 1.2953 -0.1521 To predict a new value, we use the predict() command. Note that Acid.Conc. is spelled with a trailing dot, which we need to include in the name when we supply the new input values here. predict(m, data.frame(Air.Flow = 60, Water.Temp = 21, Acid.Conc. = 87)) ## 1 ## 16.98509 Thus, the predicted value for stack.loss is \\(16.98509\\). 4 Let \\(\\varepsilon_1, \\ldots, \\varepsilon_n \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) be independent. Then \\(\\varepsilon= (\\varepsilon_1, \\ldots, \\varepsilon_n)\\) is a random vector. Determine \\(\\mathbb{E}(\\varepsilon)\\), \\(\\mathop{\\mathrm{Cov}}(\\varepsilon)\\) and \\(\\mathbb{E}\\bigl( \\|\\varepsilon\\|^2 \\bigr)\\). We immediately find \\(\\mathbb{E}(\\varepsilon) = \\mu \\mathbf{1}\\) where \\(\\mathbf{1} = (1, \\ldots, 1) \\in\\mathbb{R}^n\\). Since the \\(\\varepsilon_i\\) are independent, we have \\(\\mathop{\\mathrm{Cov}}(\\varepsilon) = \\sigma^2 I\\). Finally we have \\[\\begin{align*} \\mathbb{E}\\bigl(\\|\\varepsilon\\|^2\\bigr) &amp;= \\mathbb{E}\\bigl( \\sum_{i=1}^n \\varepsilon_i^2 \\bigr) \\\\ &amp;= \\sum_{i=1}^n E(\\varepsilon_i^2) \\\\ &amp;= n \\mathbb{E}(\\varepsilon_1^2). \\end{align*}\\] Since \\(\\mathbb{E}(\\varepsilon_1^2) = \\mathop{\\mathrm{Var}}(\\varepsilon_1) + \\mathbb{E}(\\varepsilon_1)^2 = \\sigma^2 + \\mu^2\\) we get \\(\\mathbb{E}\\bigl(\\|\\varepsilon\\|^2\\bigr) = n (\\sigma^2 + \\mu^2)\\). "],["S03-cov.html", "Section 3 Random Vectors and Covariance 3.1 Expectation 3.2 Covariance Matrix 3.3 The Multivariate Normal Distribution", " Section 3 Random Vectors and Covariance Like in the one-dimensional case, we can build a statistical model for the data where we assume that the errors are random. More precisely we will assume \\[\\begin{equation} Y_i = \\beta_0 + \\beta_1 x_{i,1} + \\cdots + \\beta_p x_{i,p} + \\varepsilon_i \\end{equation}\\] for all \\(i \\in \\{1, 2, \\ldots, n\\}\\), where \\(\\varepsilon_1, \\ldots, \\varepsilon_n\\) are now independent and identically distributed (i.i.d.) random variables with \\(\\mathbb{E}(\\varepsilon_i) = 0\\) and \\(\\mathop{\\mathrm{Var}}(\\varepsilon_i) = \\sigma^2\\). As in (2.2), the statistical model can be written in vector form as \\[\\begin{equation} Y = X \\beta + \\varepsilon. \\tag{3.1} \\end{equation}\\] This is a vector-valued equation which contains two “random vectors”, \\(Y\\) and \\(\\varepsilon\\). A random vector is a vector \\(Z = (Z_1, \\ldots, Z_n)\\) where each component \\(Z_i\\) is a random variable. 3.1 Expectation The expectation of a random vector is taken for each component separately. This is formalised in the following definition. Definition 3.1 Let \\(Z = (Z_1, \\ldots, Z_n) \\in \\mathbb{R}^n\\) be a random vector. Then the expectation of \\(Z\\) is the (non-random) vector \\[\\begin{equation*} \\mathbb{E}(X) = \\begin{pmatrix} \\mathbb{E}(Z_1) \\\\ \\vdots \\\\ \\mathbb{E}(Z_n) \\end{pmatrix} \\in \\mathbb{R}^n. \\end{equation*}\\] The same convention is sometimes used for random matrices \\(M\\), as \\(\\mathbb{E}(M)_{ij} = \\mathbb{E}(M_{ij})\\). Example 3.1 The random vector \\(\\varepsilon\\) in (3.1) has \\[\\begin{equation*} \\mathbb{E}(\\varepsilon)_i = \\mathbb{E}(\\varepsilon_i) = 0 \\end{equation*}\\] for all \\(i \\in \\{1, \\ldots, n\\}\\) and thus \\(\\mathbb{E}(\\varepsilon) = 0 \\in \\mathbb{R}^n\\), where \\(0\\) here denotes the zero-vector \\((0, \\ldots, 0) \\in \\mathbb{R}^n\\). Since the expectation of a random vector is defined in term of the usual expectation, most rules we know for expectations still hold. For example, if \\(Y\\) and \\(Z\\) are two random vectors, we have \\(\\mathbb{E}(Y+Z) = \\mathbb{E}(Y) + \\mathbb{E}(Z)\\). Example 3.2 The random vector \\(Y\\) in (3.1) has \\[\\begin{equation*} \\mathbb{E}(Y)_i = \\mathbb{E}(Y_i) = \\mathbb{E}\\bigl( (X\\beta)_i + \\varepsilon_i \\bigr) = (X\\beta)_i + \\mathbb{E}(\\varepsilon_i) = (X\\beta)_i \\end{equation*}\\] for all \\(i \\in \\{1, \\ldots, n\\}\\) and thus \\(\\mathbb{E}(Y) = X\\beta \\in \\mathbb{R}^n\\). We often will write the above derivation in vector form as \\[\\begin{equation*} \\mathbb{E}(Y) = \\mathbb{E}(X\\beta + \\varepsilon) = X\\beta + \\mathbb{E}(\\varepsilon) = X\\beta. \\end{equation*}\\] Example 3.3 If \\(A \\in \\mathbb{R}^{m\\times n}\\) is a matrix and \\(Z \\in \\mathbb{R}^n\\) is a random vector, then we find the expectation of \\(AZ\\in\\mathbb{R}^m\\) as \\[\\begin{equation*} \\mathbb{E}(AZ)_i = \\mathbb{E}(AZ_i) = \\mathbb{E}\\bigl( \\sum_{j=1}^n a_{ij} Z_j \\bigr) = \\sum_{j=1}^n \\mathbb{E}\\bigl( a_{ij} Z_j \\bigr) = \\sum_{j=1}^n a_{ij} \\mathbb{E}(Z_j) = \\sum_{j=1}^n a_{ij} \\mathbb{E}(Z)_j \\end{equation*}\\] for all \\(i \\in \\{1, \\ldots, m\\}\\) and thus we have \\(\\mathbb{E}(AZ) = A \\mathbb{E}(Z)\\). 3.2 Covariance Matrix The variance of random variables is replaced with the concept of a “covariance matrix” for random vectors. Definition 3.2 Let \\(Z = (Z_1, \\ldots, Z_n) \\in \\mathbb{R}^n\\) be a random vector. Then the covariance matrix of \\(Z\\) is the matrix \\(\\mathop{\\mathrm{Cov}}(Z) \\in \\mathbb{R}^{n\\times n}\\) given by \\[\\begin{equation*} \\mathop{\\mathrm{Cov}}(Z)_{ij} = \\mathop{\\mathrm{Cov}}(Z_i, Z_j), \\end{equation*}\\] for all \\(i, j \\in \\{1, \\ldots, n\\}\\), where \\(\\mathop{\\mathrm{Cov}}(Z_i, Z_j)\\) denotes the usual covariance between random variables. We collect some basic properties of covariance matrices here. Most of these arguments use concepts and rules from linear algebra, as summarised in section A in the appendix. Since \\(\\mathop{\\mathrm{Cov}}(Z_i, Z_j) = \\mathop{\\mathrm{Cov}}(Z_j, Z_i)\\), covariance matrices are symmetric. The diagonal elements of \\(\\mathop{\\mathrm{Cov}}(Z)\\) are \\[\\begin{equation} \\mathop{\\mathrm{Cov}}(Z)_{ii} = \\mathop{\\mathrm{Cov}}(Z_i, Z_i) = \\mathop{\\mathrm{Var}}(Z_i). \\tag{3.2} \\end{equation}\\] If the elements \\(Z_i\\) of \\(Z\\) are (statistically) independent, we have \\(\\mathop{\\mathrm{Cov}}(Z_i, Z_j) = 0\\) and thus \\(\\mathop{\\mathrm{Cov}}(Z)_{ij} = 0\\) for \\(i \\neq j\\). If \\(Z\\) is a vector of independent random variables, the covariance matrix of \\(Z\\) is diagonal. Let \\(\\mu = \\mathbb{E}(Z) \\in \\mathbb{R}^n\\). If we interpret \\(\\mu\\) as a column vector, then \\(M = (Z - \\mu) (Z - \\mu)^\\top\\) is an \\(n\\times n\\) matrix and we have \\[\\begin{equation*} M_{ij} = \\bigl( (Z - \\mu) (Z - \\mu)^\\top \\bigr)_{ij} = (Z - \\mu)_i (Z - \\mu)_j. \\end{equation*}\\] Taking expectations gives \\(\\mathbb{E}(M_{ij}) = E\\bigl( (Z - \\mu)_i (Z - \\mu)_j \\bigr) = \\mathop{\\mathrm{Cov}}(Z_i, Z_j)\\) and thus we can write \\[\\begin{equation} \\mathop{\\mathrm{Cov}}(Z) = \\mathbb{E}\\bigl( (Z - \\mu) (Z - \\mu)^\\top \\bigr). \\tag{3.3} \\end{equation}\\] Covariance matrices are positive semi-definite. To see this, let \\(C = \\mathop{\\mathrm{Cov}}(Z)\\) and \\(u \\in\\mathbb{R}^n\\) be a vector. We have to show that \\(u^\\top C u \\geq 0\\). Writing \\(\\bar Z := Z - \\mathbb{E}(Z)\\) as an abbreviation, we get \\[\\begin{align*} u^\\top C u &amp;= u^\\top \\mathbb{E}\\bigl( \\bar Z \\bar Z^\\top \\bigr) u \\\\ &amp;= \\mathbb{E}\\bigl( u^\\top \\bar Z \\bar Z^\\top u \\bigr) \\\\ &amp;= \\mathbb{E}\\bigl( (\\bar Z^\\top u)^\\top \\bar Z^\\top u \\bigr) \\\\ &amp;= \\mathbb{E}\\bigl( \\|\\bar Z^\\top u\\|^2 \\bigr), \\end{align*}\\] where \\(\\|\\bar Z^\\top u\\|\\) denotes the Euclidean length of the vector \\(\\bar Z^\\top u\\). Since \\(\\|\\bar Z^\\top u\\|^2 \\geq 0\\) we find \\(u^\\top C u \\geq 0\\). This shows that the covariance matrix \\(C\\) is positive semi-definite. (Note that, nevertheless, individual elements of the matrix \\(C\\) can be negative numbers.) Example 3.4 The random vector \\(\\varepsilon\\) in equation (3.1) has \\(\\mathbb{E}(\\varepsilon) = 0\\). We have \\(\\mathop{\\mathrm{Cov}}(\\varepsilon)_{ii} = \\mathop{\\mathrm{Var}}(\\varepsilon_i) = \\sigma^2\\) for all \\(i\\in\\{1, \\ldots, n\\}\\). Since we assumed the \\(\\varepsilon_i\\) to be independent, the covariance matrix is diagonal and we find \\[\\begin{equation*} \\mathop{\\mathrm{Cov}}(\\varepsilon) = \\sigma^2 I, \\end{equation*}\\] where \\(I\\) is the \\(n\\times n\\) identity matrix. An important results about covariance matrices is given in the following lemma, which describes how the covariance matrix changes under affine transformations. Lemma 3.1 Let \\(Z\\in\\mathbb{R}^n\\) be a random vector, \\(A\\in\\mathbb{R}^{m\\times n}\\) a matrix and \\(b\\in\\mathbb{R}^m\\) a vector. Then \\[\\begin{equation*} \\mathop{\\mathrm{Cov}}(AZ+b) = A \\mathop{\\mathrm{Cov}}(Z) A^\\top. \\end{equation*}\\] Proof. As in equation (3.3), we can write \\(\\mathop{\\mathrm{Cov}}(AZ+b)\\) as \\[\\begin{equation*} \\mathop{\\mathrm{Cov}}(AZ+b) = \\mathbb{E}\\bigl( (AZ + b - \\mu) (AZ + b - \\mu)^\\top \\bigr), \\end{equation*}\\] where \\(\\mu = \\mathbb{E}(AZ + b) = \\mathbb{E}(AZ) + b\\). Thus, \\(AZ + b - \\mu = AZ - \\mathbb{E}(AZ)\\) and we find \\[\\begin{align*} \\mathop{\\mathrm{Cov}}(AZ+b) &amp;= \\mathbb{E}\\bigl( (AZ - \\mathbb{E}(AZ)) (AZ - \\mathbb{E}(AZ))^\\top \\bigr) \\\\ &amp;= \\mathop{\\mathrm{Cov}}(AZ). \\end{align*}\\] This shows that the covariance matrix ignores non-random shifts. Furthermore, we have \\(AZ - \\mathbb{E}(AZ) = AZ - A\\mathbb{E}(Z) = A\\bigl(Z - \\mathbb{E}(Z)\\bigr)\\). Using equation (3.3) again, we find \\[\\begin{align*} \\mathop{\\mathrm{Cov}}(AZ) &amp;= \\mathbb{E}\\Bigl( \\bigl(AZ - \\mathbb{E}(AZ)\\bigr) \\bigl(AZ - \\mathbb{E}(AZ)\\bigr)^\\top \\Bigr) \\\\ &amp;= \\mathbb{E}\\Bigl( A \\bigl(Z - \\mathbb{E}(Z)\\bigr) \\bigl(Z - \\mathbb{E}(Z)\\bigr)^\\top A^\\top \\Bigr) \\\\ &amp;= A \\mathbb{E}\\Bigl( \\bigl(Z - \\mathbb{E}(Z)\\bigr) \\bigl(Z - \\mathbb{E}(Z)\\bigr)^\\top \\Bigr) A^\\top \\\\ &amp;= A \\mathop{\\mathrm{Cov}}(Z) A^\\top. \\end{align*}\\] This completes the proof. 3.3 The Multivariate Normal Distribution Since we assume that the random errors \\(\\varepsilon_i\\) are normally distributed, we will need to understand how vectors of normal distributed random variables behave. Definition 3.3 A random vector \\(Z\\in\\mathbb{R}^n\\) follows a multivariate normal distribution, if \\(u^\\top Z\\) is normally distributed or constant for every vector \\(u\\in\\mathbb{R}^n\\). This definition is takes its slightly surprising form to avoid some boundary cases which I will discuss in an example, below. To understand the definition, a good start is to consider the cases where \\(u\\) is one of the standard basis vectors, say \\(u_i = 1\\) and \\(u_j = 0\\) for all \\(j\\neq i\\). In this case we have \\[\\begin{equation*} u^\\top Z = \\sum_{k=1}^n u_k Z_k = Z_i. \\end{equation*}\\] Thus, if \\(Z\\) follows a multivariate normal distribution, each of the components \\(Z_i\\) is normally distributed. Example 3.8, below, shows that the converse is not true. One can show that a multivariate normal distribution is completely determined by the mean \\(\\mu = \\mathbb{E}(Z)\\) and the covariance \\(\\Sigma = \\mathop{\\mathrm{Cov}}(Z)\\). The distribution of such a \\(Z\\) is denoted by \\(\\mathcal{N}(\\mu, \\Sigma)\\). Also, for every \\(\\mu\\in\\mathbb{R}^n\\) and every positive semi-definite matrix \\(\\Sigma\\in\\mathbb{R}^{n\\times n}\\) there is a random vector \\(Z\\) which follows a multivariate normal distribution with this mean and covariance. Example 3.5 Consider the vector \\(\\varepsilon\\) from the model (3.1). This vector has components \\(\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\) and by assumption, the componens \\(\\varepsilon_i\\) are independent. For \\(u\\in\\mathbb{R}^n\\) we have \\[\\begin{equation*} u^\\top \\varepsilon = \\sum_{i=1}^n u_i \\varepsilon_i. \\end{equation*}\\] Since this is a sum of independent, one-dimensional, normally distributed random variables, \\(u^\\top \\varepsilon\\) is also normally distribution, for every \\(u\\). (The independence of the \\(\\varepsilon_i\\) is important in this argument.) Thus, \\(\\varepsilon\\) is a normally distributed random vector. We have already seen \\(\\mathbb{E}(\\varepsilon) = 0\\) and \\(\\mathop{\\mathrm{Cov}}(\\varepsilon) = \\sigma^2 I\\), and thus \\(\\varepsilon\\sim \\mathcal{N}(0, \\sigma^2 I)\\). Without proof we state here some properties of the multivariate normal distribution: If \\(Z \\sim \\mathcal{N}(\\mu, \\Sigma)\\) and \\(a \\in \\mathbb{R}^n\\), then \\(Z + a \\sim \\mathcal{N}(\\mu + a, \\Sigma)\\). If \\(Z \\sim \\mathcal{N}(\\mu, \\Sigma)\\) and \\(A \\in \\mathbb{R}^{m\\times n}\\), then \\(AZ \\sim \\mathcal{N}(A\\mu, A\\Sigma A^\\top)\\). If \\(Z_1 \\sim \\mathcal{N}(\\mu_1, \\Sigma_1)\\) and \\(Z_2 \\sim \\mathcal{N}(\\mu_2, \\Sigma_2)\\) are independent, then \\(Z_1 + Z_2 \\sim \\mathcal{N}(\\mu_1 + \\mu_2, \\Sigma_1 + \\Sigma_2)\\). Example 3.6 Let \\(Z = (Z_1, Z_2)\\) where \\(Z_1\\) and \\(Z_2\\) are independently standard normal distributed. Let \\[\\begin{equation*} A := \\begin{pmatrix} 2 &amp; -1 \\\\ 2 &amp; 1 \\end{pmatrix} \\qquad \\mbox{and} \\qquad b := \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}. \\end{equation*}\\] Then \\(AZ + b \\sim \\mathcal{N}(b, \\Sigma)\\) where \\[\\begin{equation*} \\Sigma = A \\mathop{\\mathrm{Cov}}(Z) A^\\top = \\begin{pmatrix} 2 &amp; -1 \\\\ 2 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 2 &amp; 2 \\\\ -1 &amp; 1 \\end{pmatrix} = \\begin{pmatrix} 5 &amp; 3 \\\\ 3 &amp; 5 \\end{pmatrix} \\end{equation*}\\] We can use R to plot a sample of this two-dimensional normal distribution. (The grey cross indicates the mean.) N &lt;- 500 Z &lt;- rbind(rnorm(N), rnorm(N)) A &lt;- matrix(c(2, 2, -1, 1), 2, 2) b &lt;- c(3, 4) V &lt;- A %*% Z + b plot(V[1,], V[2,], asp = 1, cex = .5, xlab = expression(V[1]), ylab = expression(V[2])) abline(v = 3, col = &quot;grey&quot;) abline(h = 4, col = &quot;grey&quot;) Example 3.7 The random vector \\(Y\\) in (3.1) satisfies \\(Y = X \\beta + \\varepsilon\\) and since \\(\\varepsilon\\sim \\mathcal{N}(0, \\sigma^2 I)\\) we have \\(Y \\sim \\mathcal{N}(X\\beta, \\sigma^2 I)\\). Example 3.8 Let \\(Y \\sim \\mathcal{N}(0, 1)\\) be a random variable. Define a random vector \\(Z = (Z_1, Z_2)\\) as \\(Z_1 = Y\\) and \\[\\begin{equation*} Z_2 = \\begin{cases} Y &amp; \\mbox{if $|Y|&lt;1$, and}\\\\ -Y &amp; \\mbox{otherwise.} \\end{cases} \\end{equation*}\\] Clearly \\(Z_1\\) is standard normally distributed. Since \\(\\mathcal{N}(0,1)\\) is symmetric, both \\(Y\\) and \\(-Y\\) are standard normally distributed and it follows that \\(Z_2\\) is also standard normally distributed. Nevertheless, the random vector \\(Z\\) does not follow a multivariate normal distribution. Instead of giving a proof of this fact, we illustrate this here using an R experiment. We start by verifying that \\(Z_1\\) and \\(Z_2\\) are normally distributed. N &lt;- 1000 Y &lt;- rnorm(N) Z1 &lt;- Y Z2 &lt;- ifelse(abs(Y)&lt;1, Y, -Y) par(mfrow=c(1,2)) hist(Z1, main=NULL, xlab=expression(Z[1])) hist(Z2, main=NULL, xlab=expression(Z[2])) The histograms make it plausible that the components are indeed normally distributed. Now we use a scatter plot to show the joint distribution of \\(Z_1\\) and \\(Z_2\\): plot(Z1, Z2, cex=0.5, asp=1, xlab=expression(Z[1]), ylab=expression(Z[1])) This plot looks peculiar! Most people would not call this a normal distribution and the formal definition of a multivariate normal distribution is made to exclude cases like this. Summary We learned the rules for computing the expectation of a random vector. The covariance matrix of random vectors plays the role of the variance for numeric random variables. We learned about the definition of the multivariate normal distribution. "],["S04-model.html", "Section 4 Properties of the Least Squares Estimate 4.1 Mean and Covariance 4.2 Properties of the Hat Matrix 4.3 Cochran’s theorem 4.4 Estimating the Error Variance", " Section 4 Properties of the Least Squares Estimate Like in the one-dimensional case, we can build a statistical model for the data. Here we assume that the residuals are random. More precisely we have \\[\\begin{equation} Y = X \\beta + \\varepsilon. \\tag{4.1} \\end{equation}\\] for all \\(i \\in \\{1, 2, \\ldots, n\\}\\), where \\(\\varepsilon_1, \\ldots, \\varepsilon_n\\) are now assumed to be i.i.d. random variables with \\(\\mathbb{E}(\\varepsilon_i) = 0\\) and \\(\\mathop{\\mathrm{Var}}(\\varepsilon_i) = \\sigma^2\\). Again, we assume that the \\(x\\)-values are fixed and known. The only random quantities in the model are \\(\\varepsilon_i\\) and \\(Y_i\\). The parameters in this model are now \\(\\beta = (\\beta_0, \\beta_1, \\cdots, \\beta_p) \\in \\mathbb{R}^{p+1}\\) and \\(\\sigma^2\\). The parameters \\(\\beta_j\\) are often called the regression coefficients. The usual approach in statistics to quantify how well an estimator works is to apply it to random samples from the statistical model, where we can assume that we know the parameters, and then to study how well the parameters are reconstructed by the estimators. Since this approach uses random samples as input the the estimator, we obtain random estimates and we need to use statistical methods to quantify how close the estimate is to the truth. 4.1 Mean and Covariance The bias of an estimator is the difference between the expected value of the estimate and the truth. For the least squares estimator we have \\[\\begin{equation*} \\mathop{\\mathrm{bias}}(\\hat\\beta) = \\mathbb{E}(\\hat\\beta) - \\beta, \\end{equation*}\\] where \\[\\begin{equation*} \\hat\\beta = (X^\\top X)^{-1} X^\\top Y \\end{equation*}\\] and \\(Y\\) is the random vector from (4.1). Lemma 4.1 We have \\(\\hat\\beta = \\beta + (X^\\top X)^{-1} X^\\top \\varepsilon\\) and \\(\\hat\\beta \\sim \\mathcal{N}\\bigl( \\beta, \\sigma^2 (X^\\top X)^{-1} \\bigr)\\). Proof. From lemma 2.1 we know \\[\\begin{equation*} \\hat\\beta = (X^\\top X)^{-1} X^\\top Y, \\end{equation*}\\] Using the definition of \\(Y\\) we can write this as \\[\\begin{align*} \\hat\\beta &amp;= (X^\\top X)^{-1} X^\\top Y \\\\ &amp;= (X^\\top X)^{-1} X^\\top (X\\beta + \\varepsilon) \\\\ &amp;= (X^\\top X)^{-1} X^\\top X \\beta + (X^\\top X)^{-1} X^\\top \\varepsilon\\\\ &amp;= \\beta + (X^\\top X)^{-1} X^\\top \\varepsilon. \\end{align*}\\] This proves the first claim. Since \\(\\varepsilon\\) follows a multi-variate normal distribution, \\(\\beta + (X^\\top X)^{-1} X^\\top \\varepsilon\\) is also normally distributed. Taking expectations we get \\[\\begin{align*} \\mathbb{E}(\\hat\\beta) &amp;= \\mathbb{E}\\bigl( \\beta + (X^\\top X)^{-1} X^\\top \\varepsilon\\bigr) \\\\ &amp;= \\beta + (X^\\top X)^{-1} X^\\top \\mathbb{E}(\\varepsilon) \\\\ &amp;= \\beta, \\end{align*}\\] since \\(\\mathbb{E}(\\varepsilon) = 0\\). For the covariance we find \\[\\begin{align*} \\mathop{\\mathrm{Cov}}(\\hat\\beta) &amp;= \\mathop{\\mathrm{Cov}}\\bigl( \\beta + (X^\\top X)^{-1} X^\\top \\varepsilon\\bigr) \\\\ &amp;= \\mathop{\\mathrm{Cov}}\\bigl( (X^\\top X)^{-1} X^\\top \\varepsilon\\bigr) \\\\ &amp;= (X^\\top X)^{-1} X^\\top \\mathop{\\mathrm{Cov}}(\\varepsilon) \\bigl( (X^\\top X)^{-1} X^\\top \\bigr)^\\top \\\\ &amp;= (X^\\top X)^{-1} X^\\top \\mathop{\\mathrm{Cov}}(\\varepsilon) X (X^\\top X)^{-1}. \\end{align*}\\] Since \\(\\mathop{\\mathrm{Cov}}(\\varepsilon) = \\sigma^2 I\\), this simplifies to \\[\\begin{align*} \\mathop{\\mathrm{Cov}}(\\hat\\beta) &amp;= (X^\\top X)^{-1} X^\\top \\sigma^2 I X (X^\\top X)^{-1} \\\\ &amp;= \\sigma^2 (X^\\top X)^{-1} X^\\top X (X^\\top X)^{-1} \\\\ &amp;= \\sigma^2 (X^\\top X)^{-1}. \\end{align*}\\] This completes the proof. The lemma implies that \\(\\mathbb{E}(\\hat\\beta) = \\beta\\), i.e. the estimator \\(\\hat\\beta\\) is unbiased. Note that for this statement we only used \\(\\mathbb{E}(\\varepsilon) = 0\\) to compute the expectation of \\(\\hat\\beta\\). Thus, the estimator will still be unbiases for correlated or for noise which is not normally distributed. We have seen earlier that the diagonal elements of a covariance give the variances of the elements of the random vector. Setting \\(C := (X^\\top X)^{-1}\\) as a shorthand here, we find that the individual estimated coefficients \\(\\hat\\beta_i\\) satisfy \\[\\begin{equation} \\hat\\beta_i \\sim \\mathcal{N}\\bigl( \\beta, \\sigma^2 C_{ii} \\bigr) \\tag{4.2} \\end{equation}\\] for all \\(i \\in \\{1, \\ldots, n\\}\\). These results about the (co-)variances of the estimator are not very useful in practice, because the error variance \\(\\sigma^2\\) is unknown. To derive more useful results, we will consider how to estimate this variance. 4.2 Properties of the Hat Matrix In this and the following sections we will use various properties of the hat matrix \\(H = X (X^\\top X)^{-1} X^\\top\\). Lemma 4.2 The hat matrix \\(H\\) has the following properties: \\(H\\) is symmetric, i.e. \\(H^\\top = H\\). \\(H\\) is idempotent, i.e. \\(H^2 = H\\). Proof. For the first statement we have \\[\\begin{align*} H^\\top &amp;= \\bigl( X (X^\\top X)^{-1} X^\\top \\bigr)^\\top \\\\ &amp;= (X^\\top)^\\top \\bigl((X^\\top X)^{-1}\\bigr)^\\top X^\\top \\\\ &amp;= X (X^\\top X)^{-1} X^\\top \\\\ &amp;= H, \\end{align*}\\] where we used that the inverse of a symmetric matrix is symmetric. The second statement follow from \\[\\begin{align*} H^2 &amp;= \\bigl( X (X^\\top X)^{-1} X^\\top \\bigr) \\bigl( X (X^\\top X)^{-1} X^\\top \\bigr) \\\\ &amp;= X \\bigl( (X^\\top X)^{-1} X^\\top X \\bigr) (X^\\top X)^{-1} X^\\top \\\\ &amp;= X (X^\\top X)^{-1} X^\\top. \\end{align*}\\] This completes the proof. Both properties from the lemma carry over from \\(H\\) to \\(I - H\\): we have \\((I - H)^\\top = I^\\top - H^\\top = I - H\\) and \\[\\begin{align*} (I - H)^2 &amp;= (I - H)(I - H) \\\\ &amp;= I^2 - HI - IH + H^2 \\\\ &amp;= I - H - H + H \\\\ &amp;= I - H. \\end{align*}\\] For future reference we also state two simpler results: we have \\[\\begin{equation} H X = X (X^\\top X)^{-1} X^\\top X = X \\tag{4.3} \\end{equation}\\] and \\[\\begin{equation} (I - H) X = IX - HX = X - X = 0. \\tag{4.4} \\end{equation}\\] Finally, if we have a vector \\(v \\in \\mathbb{R}^n\\) we can write \\(v\\) as \\[\\begin{align*} v &amp;= (H + I - H)v \\\\ &amp;= Hv + (I-H)v. \\end{align*}\\] The inner product between these two components is \\[\\begin{align*} (Hv)^\\top (I-H)v &amp;= v^\\top H^\\top (I-H) v \\\\ &amp;= v^\\top H (I-H) v \\\\ &amp;= v^\\top (H-H^2) v \\\\ &amp;= v^\\top (H-H) v \\\\ &amp;= 0, \\end{align*}\\] so the two vectors are orthogonal. As a result we get \\[\\begin{align*} \\|v\\|^2 &amp;= v^\\top v \\\\ &amp;= \\bigl( Hv + (I-H)v \\bigr)^\\top \\bigl( Hv + (I-H)v \\bigr) \\\\ &amp;= (Hv)^\\top Hv + 2 (Hv)^\\top (I-H)v + \\bigl((I-H)v\\bigr)^\\top (I-H)v \\\\ &amp;= \\| Hv \\|^2 + \\|(I-H)v \\|^2 \\end{align*}\\] (This is Pythagoras’ theorem in \\(\\mathbb{R}^n\\).) Since \\(\\hat y = Hy\\) and \\(\\hat\\varepsilon= (I - H)y\\), we can apply this idea to the vector \\(y\\) of observations to get \\(\\|y\\|^2 = \\|\\hat y\\|^2 + \\|\\hat\\varepsilon\\|^2\\). We note without proof that geometrically, \\(H\\) can be interpreted as the orthogonal projection onto the subspace of \\(\\mathbb{R}^n\\) which is spanned by the columns of \\(X\\). This subspace contains the possible output vectors of the linear system and the least squares procedure finds the point \\(\\hat y\\) in this subspace which is closest to the observed data \\(y\\in\\mathbb{R}^n\\). Some authors define: \\(\\mathrm{SS}_\\mathrm{T} = \\sum_{i=1}^n y_i^2\\) (where “T” stands for “total”) \\(\\mathrm{SS}_\\mathrm{R} = \\sum_{i=1}^n \\hat y_i^2\\) (where “R” stands for “regression”) \\(\\mathrm{SS}_\\mathrm{E} = \\sum_{i=1}^n (y_i-\\hat y_i)^2\\) (where “E” stands for “error”) Using this notation, our equation \\(\\|y\\|^2 = \\|\\hat y\\|^2 + \\|\\hat\\varepsilon\\|^2\\) turns into \\[\\begin{equation*} \\mathrm{SS}_\\mathrm{T} = \\mathrm{SS}_\\mathrm{R} + \\mathrm{SS}_\\mathrm{E}. \\end{equation*}\\] 4.3 Cochran’s theorem Our main tool in this and the following section will be a simplified version of Cochran’s theorem. Theorem 4.1 The following statements are true: \\(\\frac{1}{\\sigma^2} \\varepsilon^\\top H \\varepsilon\\sim \\chi^2(p+1)\\) \\(\\frac{1}{\\sigma^2} \\varepsilon^\\top (I - H) \\varepsilon\\sim \\chi^2(n - p - 1)\\) \\(H \\varepsilon\\) and \\((I-H)\\varepsilon\\) are independent. Proof. Since \\(H\\) is symmetric, we can diagonalise \\(H\\) (see A.1 in the appendix): there is an orthogonal matrix \\(U\\) such that \\(D := U H U^\\top\\) is diagonal, and the diagonal elements of \\(D\\) are the eigenvalues of \\(H\\). Since \\(H\\) is idempotent, these diagonal elements can only be \\(0\\) or \\(1\\). Also, since \\(U\\) is orthogonal, we have \\(U^\\top U = I\\) and thus \\[\\begin{equation*} U^\\top D U = U^\\top U H U^\\top U = H. \\end{equation*}\\] The same matrix \\(U\\) also diagonalises \\(I-H\\), since \\(U (I -H) U^\\top = U U^\\top - U H U^\\top = I - D\\). Exactly one of the diagonal elements \\(D_{ii}\\) and \\((I - D)_{ii}\\) is \\(1\\) and the other one is \\(0\\) for every \\(i\\). Since \\(\\varepsilon\\sim \\mathcal{N}(0, \\sigma^2 I)\\) we find that \\(\\eta := U \\varepsilon\\) is normally distributed with mean \\(U 0 = 0\\) and covariance matrix \\(\\sigma^2 U I U^\\top = \\sigma^2 U U^\\top = \\sigma^2 I\\). Thus \\(\\eta\\) has the same distribution as \\(\\varepsilon\\) does: \\(\\eta \\sim \\mathcal{N}(0, \\sigma^2I)\\) and the components \\(\\eta_i\\) are independent of each other. We have \\[\\begin{equation*} H \\varepsilon = U^\\top D U \\varepsilon = U^\\top D \\eta. \\end{equation*}\\] and \\[\\begin{equation*} (I - H) \\varepsilon = U^\\top (I - D) U \\varepsilon = U^\\top (I - D) \\eta. \\end{equation*}\\] Since \\((D\\eta)_i = 0\\) if \\(D_{ii}=0\\) and \\(\\bigl((I - D) \\eta)_i = 0\\) otherwise, each component of \\(\\eta\\) contributes to exactly one of the two vectors \\(D\\eta\\) and \\((I-D)\\eta\\). Thus, \\(D\\eta\\) and \\((I-D)\\eta\\) are independent, and thus \\(H\\varepsilon\\) and \\((I - H)\\varepsilon\\) are also independent. This proves the third statement of the theorem. For the first statement, we note that \\[\\begin{align*} \\varepsilon^\\top H \\varepsilon &amp;= \\varepsilon^\\top U^\\top D U \\varepsilon\\\\ &amp;= \\eta^\\top D \\eta \\\\ &amp;= \\sum_{i=1 \\atop D_{ii}=1}^n \\eta_i^2. \\end{align*}\\] Since \\((X^\\top X) \\in\\mathbb{R}^{(p+1)\\times (p+1)}\\) is invertible, one can show that \\(\\mathop{\\mathrm{rank}}(H) = p+1\\) and thus that there are \\(p+1\\) terms contributing to the sum (we skip the proof of this statement here). Thus, \\[\\begin{equation*} \\frac{1}{\\sigma^2} \\varepsilon^\\top H \\varepsilon = \\sum_{i=1 \\atop D_{ii}=1}^n \\bigl(\\eta_i/\\sigma)^2 \\end{equation*}\\] is the sum of the squares of \\(p+1\\) independent standard normals, and thus is \\(\\chi^2(p+1)\\) distributed. This completes the proof of the first statement. Finally, the second statement follows in much of the same way as the first one, except that \\(H\\) is replaced with \\(I-H\\) and the sum is over the \\(n - p - 1\\) indices \\(i\\) where \\(D_{ii} = 0\\). This completes the proof. Expressions of the form \\(x^\\top A x\\) for \\(x\\in\\mathbb{R}^n\\) and \\(A\\in\\mathbb{R}^{n\\times n}\\) are called quadratic forms. While the theorem as written only states that \\(H \\varepsilon\\) and \\((I - H)\\varepsilon\\) are independent of each other, we can replace one or both of these terms the corresponding quadratic forms as still keep the independence. Since \\((H \\varepsilon)^\\top (H \\varepsilon) = \\varepsilon^\\top H^\\top H \\varepsilon= \\varepsilon^\\top H \\varepsilon\\), the quadratic form \\(\\varepsilon^\\top H \\varepsilon\\) is a function of \\(H \\varepsilon\\) and a similar statement holds with \\(H-I\\) instead of \\(H\\). 4.4 Estimating the Error Variance So far we have only considered how to estimate the parameter vector \\(\\beta\\) and we have ignored the parameter \\(\\sigma^2\\). We will see that an unbiased estimator for \\(\\sigma^2\\) is given by \\[\\begin{equation} \\hat\\sigma^2 = \\frac{1}{n-p-1} \\sum_{i=1}^n (y_i - \\hat y_i)^2, \\tag{4.5} \\end{equation}\\] where \\(\\hat y_i\\) are the fitted values from equation (2.5). As for the one-dimensional case in (1.4), the estimate does not have the prefactor \\(1/n\\), which one might naively expect, but the denomintor is decreased by one for each component of the vector \\(\\beta\\). Using Cochran’s theorem, we can now show that the estimator \\(\\hat\\sigma^2\\) is unbiased. We first note that \\[\\begin{align*} (n - p - 1) \\hat\\sigma^2 &amp;= (y - \\hat y)^\\top (y - \\hat y) \\\\ &amp;= (y - H y)^\\top (y - H y) \\\\ &amp;= y^\\top (I - H)^\\top (I - H) y \\\\ &amp;= y^\\top (I - H) y \\end{align*}\\] To determine the bias, we need to use \\(Y = X\\beta + \\varepsilon\\) in place of the data. This gives \\[\\begin{align*} (n - p - 1) \\hat\\sigma^2 &amp;= Y^\\top (I-H) Y \\\\ &amp;= (X\\beta + \\varepsilon)^\\top (I-H) (X\\beta + \\varepsilon) \\\\ &amp;= \\beta^\\top X^\\top (I-H) X \\beta + 2 \\varepsilon^\\top (I-H) X \\beta + \\varepsilon^\\top (I-H) \\varepsilon\\\\ &amp;= \\varepsilon^\\top (I-H) \\varepsilon, \\end{align*}\\] where we used equation (4.4) to see that the first two terms in the sum equal zero. Now we can apply Cochran’s theorem. This shows that \\[\\begin{equation} \\frac{1}{\\sigma^2} (n - p - 1) \\hat\\sigma^2 = \\frac{1}{\\sigma^2} \\varepsilon^\\top (I-H) \\varepsilon \\sim \\chi^2(n - p - 1). \\tag{4.6} \\end{equation}\\] Since the expectation of a \\(\\chi^2(\\nu)\\) distribution equals \\(\\nu\\) (see appendix B.2), we find \\[\\begin{equation*} \\frac{1}{\\sigma^2} (n - p - 1) \\mathbb{E}(\\hat\\sigma^2) = n - p - 1 \\end{equation*}\\] and thus \\[\\begin{equation*} \\mathbb{E}(\\hat\\sigma^2) = \\sigma^2. \\end{equation*}\\] This proves that \\(\\hat\\sigma^2\\) is an unbiased estimator for \\(\\sigma^2\\). Summary The least squares estimator for the regression coefficients is unbiased. The hat matrix is idempotent and symmetric. Cochran’s theorem allows to understand the distribution of some quadratic forms involving the hat matrix. \\(\\hat\\sigma^2\\) is an unbiased estimator for \\(\\sigma^2\\). "],["S05-single.html", "Section 5 Uncertainty for Individual Regression Coefficients 5.1 Measuring the Estimation Error 5.2 Confidence Intervals 5.3 Hypthesis Tests 5.4 R Experiments", " Section 5 Uncertainty for Individual Regression Coefficients In this section we will consider different ways to study the uncertainty in the estimates \\(\\hat\\beta_i\\) for the regression coefficient \\(\\beta_i\\), individually. In the following sections we will then consider the problem of simultaneously estimating several or all coefficients. 5.1 Measuring the Estimation Error We have seen that \\(\\hat\\beta \\sim \\mathcal{N}(\\beta, \\sigma^2 C)\\), where \\(C := (X^\\top X)^{-1}\\). Restricting this to a single coefficient, we find \\[\\begin{equation*} \\hat\\beta_i \\sim \\mathcal{N}\\bigl( \\beta_i, \\sigma^2 C_{ii} \\bigr), \\end{equation*}\\] since the diagonal elements of the covariance matrix contains the variances of the elements of a random vector. In practice we will not know the value of \\(\\sigma^2\\), so we have to estimate this from data, using the estimator \\[\\begin{equation*} \\hat\\sigma^2 = \\frac{1}{n-p-1} \\sum_{i=1}^n (y_i - \\hat y_i)^2, \\end{equation*}\\] from equation (4.5). As a first application of Cochran’s theorem we showed in equation (4.6) that \\[\\begin{equation*} \\frac{1}{\\sigma^2} (n - p - 1) \\hat\\sigma^2 \\sim \\chi^2(n - p - 1). \\end{equation*}\\] Note that in the equations above, we index the rows and columns of \\(C\\) using \\(i,j\\in \\{0, 1, \\ldots, p\\}\\), i.e. the first row and column are using the index 0 each. This is to match the convention for the components of \\(\\beta = (\\beta_0, \\beta_1, \\ldots, \\beta_p)\\). Lemma 5.1 The random vector \\(\\hat\\beta\\) and the random number \\(\\hat\\sigma^2\\) are independent of each other. Proof. We will show that \\(\\hat\\beta\\) can be written as a function of \\(H\\varepsilon\\) and that \\(\\hat\\sigma^2\\) can be written as a function of \\((I-H)\\varepsilon\\). The result then follows from Cochran’s theorem. From lemma 4.1 we know that the least squares estimate \\(\\hat\\beta\\) can be written as \\[\\begin{equation*} \\hat\\beta = \\beta + (X^\\top X)^{-1} X^\\top \\varepsilon \\end{equation*}\\] and that \\(H = X (X^\\top X)^{-1} X^\\top\\). Thus we can write \\(\\hat\\beta\\) as \\[\\begin{align*} \\hat\\beta &amp;= \\beta + (X^\\top X)^{-1} X^\\top X \\, (X^\\top X)^{-1} X^\\top \\varepsilon\\\\ &amp;= \\beta + (X^\\top X)^{-1} X^\\top H \\varepsilon, \\end{align*}\\] which is a function of \\(H\\varepsilon\\). Similar to the argument at the end of the previous section, we can write \\(\\hat\\sigma^2\\) as \\[\\begin{align*} (n - p - 1) \\hat\\sigma^2 &amp;= (Y - \\hat Y)^\\top (Y - \\hat Y) \\\\ &amp;= (Y - H Y)^\\top (Y - H Y) \\\\ &amp;= Y^\\top (I - H)^\\top (I - H) Y \\\\ &amp;= \\bigl\\| (I - H) Y \\|. \\end{align*}\\] Since \\(Y = X\\beta + \\varepsilon\\) and since we know \\((I-H)X = 0\\) from equation (4.4), we find \\[\\begin{align*} \\hat\\sigma^2 &amp;= \\frac{1}{n-p-1} \\bigl\\| (I - H) (X\\beta + \\varepsilon) \\| \\\\ &amp;= \\frac{1}{n-p-1} \\bigl\\| (I - H) \\varepsilon\\|, \\end{align*}\\] which is a function of \\((I - H)\\varepsilon\\). From Cochran’s theorem we know that \\(H \\varepsilon\\) and \\((I-H)\\varepsilon\\) are independent and thus we can conclude that \\(\\hat\\beta\\) and \\(\\hat\\sigma^2\\) are also independent of each other. This completes the proof. We now construct a quantity \\(T\\) which measures the distance between the estimated value \\(\\hat\\beta_i\\) and the unknown true value \\(\\beta_i\\): \\[\\begin{equation} T := \\frac{\\hat\\beta_i - \\beta_i}{\\sqrt{\\hat\\sigma^2 C_{ii}}}. \\tag{5.1} \\end{equation}\\] While there are many ways to measure this distance, the \\(T\\) constructed here has two main advantages: The value of \\(T\\) can be computed from the given data, without any reference to unknown quantities. Below, we will be able to find the distribution of \\(T\\). This will allow us to use \\(T\\) to construct confidence intervals and statistical tests. Lemma 5.2 Assume that the data follows the model (4.1). Then \\(T \\sim t_{n-p-1}\\), i.e. \\(T\\) follows a \\(t\\)-distribution with \\(n-p-1\\) degrees of freedom (see appendix B.3). Proof. We have \\[\\begin{align*} T &amp;= \\frac{(\\hat\\beta_i - \\beta_i) / \\sqrt{C_{ii}}} {\\sqrt{\\hat\\sigma^2}} \\\\ &amp;= \\frac{(\\hat\\beta_i - \\beta_i) / \\sqrt{\\sigma^2 C_{ii}}} {\\sqrt{\\hat\\sigma^2 / \\sigma^2}} \\\\ &amp;= \\frac{(\\hat\\beta_i - \\beta_i) / \\sqrt{\\sigma^2 C_{ii}}} {\\sqrt{((n - p - 1) \\hat\\sigma^2 / \\sigma^2) / (n - p -1)}} \\\\ &amp;=: \\frac{Z}{\\sqrt{Y / (n - p - 1)}}, \\end{align*}\\] where \\(Z = (\\hat\\beta_i - \\beta_i) / \\sqrt{\\sigma^2 C_{ii}} \\sim \\mathcal{N}(0,1)\\) and \\(Y = (n - p - 1) \\hat\\sigma^2/\\sigma^2 \\sim \\chi^2(n-p-1)\\) are independent, by lemma 5.1. Thus, \\(T \\sim t_{n-p-1}\\) as required. The quantity \\(\\sqrt{\\sigma^2 C_{ii}}\\) is sometimes called the standard error of the estimator \\(\\hat\\beta_i\\), denoted by \\(\\mathop{\\mathrm{se}}(\\hat\\beta_i)\\). 5.2 Confidence Intervals Using the scaled distance \\(T\\), it is easy to construct a confidence interval for \\(\\hat\\beta_i\\): For \\(\\alpha \\in (0, 1)\\), say \\(\\alpha = 5\\%\\), lemma 5.2 shows that \\[\\begin{equation*} P\\Bigl( T \\in \\bigl[-t_{n-p-1}(\\alpha/2), +t_{n-p-1}(\\alpha/2)\\bigr] \\Bigr) = 1 - \\alpha, \\end{equation*}\\] where \\(t_{n-p-1}(\\alpha/2)\\) is the \\((1 - \\alpha/2)\\)-quantile of the \\(t(n-p-1)\\)-distribution. Rewriting this expression as a condition on \\(\\hat\\beta_i\\) instead of on \\(T\\) gives a confidence interval for \\(\\beta_i\\). Lemma 5.3 The interval \\[\\begin{equation*} [U, V] := \\Bigl[ \\hat\\beta_i - \\sqrt{\\hat\\sigma^2 C_{ii}}t_{n-p-1}(\\alpha/2), \\hat\\beta_i + \\sqrt{\\hat\\sigma^2 C_{ii}}t_{n-p-1}(\\alpha/2) \\Bigr] \\end{equation*}\\] is a \\((1-\\alpha)\\)-confidence interval for \\(\\beta_i\\). Proof. We have to show that \\(P(\\beta_i \\in [U, V]) \\geq 1-\\alpha\\). We have \\[\\begin{align*} &amp;\\hskip-5mm \\beta_i \\in [U, V] \\\\ &amp;\\Longleftrightarrow \\bigl| \\hat\\beta_i - \\beta_i \\bigr| \\leq \\sqrt{\\hat\\sigma^2 C_{ii}}t_{n-p-1}(\\alpha/2) \\\\ &amp;\\Longleftrightarrow \\Bigl| \\frac{\\hat\\beta_i - \\beta_i}{\\sqrt{\\hat\\sigma^2 C_{ii}}} \\Bigr| \\leq t_{n-p-1}(\\alpha/2) \\\\ &amp;\\Longleftrightarrow T \\in \\bigl[-t_{n-p-1}(\\alpha/2), +t_{n-p-1}(\\alpha/2)\\bigr] \\end{align*}\\] and thus \\(P(\\beta_i \\in [U, V]) = 1 - \\alpha\\). This completes the proof. 5.3 Hypthesis Tests Very similar to the argument for confidence intervals, we can derive a hypothesis test to test the hypothesis \\[\\begin{equation*} H_0\\colon \\beta_i = b \\end{equation*}\\] against the alternative \\[\\begin{equation*} H_1\\colon \\beta_i \\neq b. \\end{equation*}\\] Here we redefine \\(T\\) as \\[\\begin{equation} T := \\frac{\\hat\\beta_i - b}{\\sqrt{\\hat\\sigma^2 C_{ii}}}, \\tag{5.2} \\end{equation}\\] using \\(b\\) in place of the \\(\\beta_i\\) above. When \\(H_0\\) is true, this new defintion of \\(T\\) is the same as (5.1). Lemma 5.4 The test which rejects \\(H_0\\) if and only if \\(|T| &gt; t_{n-p-1}(\\alpha/2)\\) has significance level \\(\\alpha\\). Proof. We have to show that the probability of type I errors (i.e. of wrongly rejecting \\(H_0\\) when it is true) is less than or equal to \\(\\alpha\\). Assume that \\(H_0\\) is true. Then we have \\(\\beta_i = b\\) and thus the \\(T\\) defined in this section coincides with the expression from equation (5.1). From lemma 5.2 we know that \\(T \\sim t(n-p-1)\\). Thus we have \\[\\begin{align*} P( \\mbox{type I error} ) &amp;= P\\bigl( |T| &gt; t_{n-p-1}(\\alpha/2) \\bigr) \\\\ &amp;= P\\bigl(T &lt; -t_{n-p-1}(\\alpha/2) \\bigr) + P\\bigl(T &gt; t_{n-p-1}(\\alpha/2) \\bigr) \\\\ &amp;= 2 P\\bigl(T &gt; t_{n-p-1}(\\alpha/2) \\bigr) \\\\ &amp;= 2 P\\bigl(T &gt; t_{n-p-1}(\\alpha/2) \\bigr) \\\\ &amp;= 2 \\frac{\\alpha}{2} \\\\ &amp;= \\alpha. \\end{align*}\\] This completes the proof. If we use \\(b = 0\\) in the test, we can test whether \\(\\beta_i = 0\\). If \\(\\beta_i = 0\\) is true, the corresponding input \\(x_i\\) has no influence on the output. As usual with statistical tests, one needs to be extremely careful when performing several tests on the same data. In particular, it would be unwise to test more than one component of \\(\\beta\\) using this procedure for the same data. Instead, in the next section we will consider how to perform tests for several components of \\(\\beta\\) simultaneously. Before we do this, we will perform some experiments with R. 5.4 R Experiments 5.4.1 Fitting the model m &lt;- lm(stack.loss ~ ., data = stackloss) summary(m) ## ## Call: ## lm(formula = stack.loss ~ ., data = stackloss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.2377 -1.7117 -0.4551 2.3614 5.6978 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -39.9197 11.8960 -3.356 0.00375 ** ## Air.Flow 0.7156 0.1349 5.307 5.8e-05 *** ## Water.Temp 1.2953 0.3680 3.520 0.00263 ** ## Acid.Conc. -0.1521 0.1563 -0.973 0.34405 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.243 on 17 degrees of freedom ## Multiple R-squared: 0.9136, Adjusted R-squared: 0.8983 ## F-statistic: 59.9 on 3 and 17 DF, p-value: 3.016e-09 5.4.2 Estimating the Variance of the Error We can get the design matrix \\(X\\) and the covariance matrix \\(C\\) as follows: X &lt;- model.matrix(m) C &lt;- solve(t(X) %*% X) round(C, 4) ## (Intercept) Air.Flow Water.Temp Acid.Conc. ## (Intercept) 13.4527 0.0273 -0.0620 -0.1594 ## Air.Flow 0.0273 0.0017 -0.0035 -0.0007 ## Water.Temp -0.0620 -0.0035 0.0129 0.0000 ## Acid.Conc. -0.1594 -0.0007 0.0000 0.0023 Next we need to estimate the variance \\(\\sigma^2\\): y &lt;- stackloss$stack.loss n &lt;- nrow(stackloss) p &lt;- ncol(stackloss) - 1 hat.sigma2 &lt;- sum((y - fitted(m))^2) / (n - p - 1) hat.sigma2 ## [1] 10.51941 The square root of this number, so the estimated standard deviation of the \\(\\varepsilon_i\\) is shown as Residual standard error in the summary output above. We check that we get the same result: sqrt(hat.sigma2) ## [1] 3.243364 This result is also listed as the Residual standard error near the bottom of the summary(m) output, above. 5.4.3 Estimating the Standard Errors We can the standard errors, i.e. the standard deviations \\(\\mathop{\\mathrm{stdev}}(\\hat\\beta)_i\\) as \\(\\sqrt{\\hat\\sigma^2 C_{ii}}\\): se &lt;- sqrt(hat.sigma2 * diag(C)) se ## (Intercept) Air.Flow Water.Temp Acid.Conc. ## 11.8959969 0.1348582 0.3680243 0.1562940 These values are also listed in the Std. Error column of the summary(m) output. 5.4.4 Hypothesis tests Let us now test the hypothesis \\(H_0\\colon \\beta_i = 0\\). The test statistic for this case is the following: T &lt;- coef(m) / se T ## (Intercept) Air.Flow Water.Temp Acid.Conc. ## -3.3557234 5.3066130 3.5195672 -0.9733098 These values are also listed in the t value column of the summary(m) output. Before we can perform the test, we need to choose \\(\\alpha\\) and to find the corresponding critical value: alpha &lt;- 0.05 t &lt;- qt(1 - alpha/2 , n - p - 1) t ## [1] 2.109816 Using the critical value \\(t\\) we can decided whether \\(H_0\\) should be accepted or rejected. For example, looking at the intercept \\(\\beta_0\\), we find \\(|T_0| = |-3.3557234| &gt; 2.109816 = t_{n-p-1}(1-\\alpha/2)\\) and thus we can reject the hypothesis \\(H_0\\colon \\beta_0 = 0\\). This means that the intercept is significantly different from 0. 5.4.5 Confidence Intervals Using the quantile t we can also get confidence intervals. Here we only show the confidence interval for the intercept \\(\\beta_0\\): c(coef(m)[1] - se[1] * t, coef(m)[1] + se[1] * t) ## (Intercept) (Intercept) ## -65.01803 -14.82131 Confidence intervals for the remaining coefficients can be obtained simimlarly. Summary We know how to scale the distance between individual parameter estimates and the truth. We have seen how to construct confidence intervals for \\(\\beta_i\\). We have seen how to construct statistical tests for \\(\\beta_i\\). We have understood some more of the summary output for the lm() function in R. "],["S06-simultaneous.html", "Section 6 Estimating Coefficients Simultaneously 6.1 Linear Combinations of Coefficients 6.2 Confidence Regions 6.3 Hypothesis Tests", " Section 6 Estimating Coefficients Simultaneously In this section we will study how to assess the uncertainty in two or more regression coefficients simultaneously. This is needed since the estimates for different coefficients will usually not be independent. 6.1 Linear Combinations of Coefficients As a general setup which allows to describe which coefficients we are interested in, we consider the image \\(K\\beta\\), where \\(K \\in \\mathbb{R}^{k \\times (p+1)}\\) with \\(k \\leq p+1\\). Example 6.1 If \\(p = 3\\) and \\[\\begin{equation*} K = \\begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\end{pmatrix}, \\end{equation*}\\] then \\[\\begin{equation*} K\\beta = \\begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{pmatrix} = \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\end{pmatrix}. \\end{equation*}\\] Thus, this choice of \\(K\\) would be appropriate if we are interested in \\(\\beta_1\\) and \\(\\beta_2\\) only. The setup allows for more general questions than just selecting components of \\(\\beta\\). We can also derive statements about linear combination of the \\(\\beta_i\\), e.g. we will be able to derive confidence intervals for quantities like \\(\\beta_1 - \\beta_2\\). Since \\(\\hat\\beta\\) is an estimator for \\(\\beta\\), we can use \\(K\\hat\\beta\\) as an estimator for \\(K\\beta\\). From lemma 4.1 we know that \\(\\hat\\beta \\sim \\mathcal{N}\\bigl( \\beta, \\sigma^2 (X^\\top X)^{-1} \\bigr)\\) and thus we get \\[\\begin{equation*} K\\hat\\beta \\sim \\mathcal{N}\\bigl( K \\beta, \\sigma^2 K (X^\\top X)^{-1} K^\\top \\bigr). \\end{equation*}\\] This shows that the estimator \\(K\\hat\\beta\\) is unbiased. Given an invertible matrix \\(Q\\), we define the shorthand notation \\[\\begin{equation*} \\| v \\|_Q^2 := v^\\top Q^{-1} v. \\end{equation*}\\] Using this notation for \\(Q = K(X^\\top X)^{-1} K^\\top\\), we define \\[\\begin{equation} F := \\frac{\\bigl\\| K \\hat\\beta - K \\beta \\bigr\\|_{K(X^\\top X)^{-1} K^\\top}^2} {k \\hat\\sigma^2} \\tag{6.1} \\end{equation}\\] as a measure for the distance between \\(K\\hat\\beta\\) and \\(K\\beta\\). This quantity plays the role of \\(T\\) (or more precisely, \\(T^2\\)) from the previous section. We also need to introduce the \\(F\\)-distribution, which will take the place of the \\(t\\)-distribution from the previous section. Definition 6.1 The \\(F\\)-distribution with \\(\\nu_1\\) and \\(\\nu_2\\) degrees of freedom is the distribution of \\[\\begin{equation*} X =\\frac{S_1/\\nu_1}{S_2/\\nu_2}, \\end{equation*}\\] where \\(S_1\\) and \\(S_2\\) are independent random variables with chi-square distributions with \\(\\nu_1\\) and \\(\\nu_2\\) degrees of freedom, respectively. With these preparations in place, we can state the main result. Lemma 6.1 Assume that the data follows the model (4.1) and that \\(Q := K(X^\\top X)^{-1} K^\\top\\) is invertible. Then \\(F \\sim F_{k,n-p-1}\\), i.e. \\(F\\) follows a \\(F\\)-distribution with \\(k\\) and \\(n-p-1\\) degrees of freedom. Proof. We have \\[\\begin{align*} \\hskip-5mm &amp; \\bigl\\| K \\hat\\beta - K \\beta \\bigr\\|_Q^2 \\\\ &amp;= \\bigl\\| K (\\hat\\beta - \\beta) \\bigr\\|_Q^2 \\\\ &amp;= \\bigl\\| K (X^\\top X)^{-1} X^\\top \\varepsilon\\bigr\\|_Q^2 \\\\ &amp;= \\varepsilon^\\top X (X^\\top X)^{-1} K^\\top Q^{-1} K (X^\\top X)^{-1} X^\\top \\varepsilon\\\\ &amp;=: \\varepsilon^\\top R \\varepsilon. \\end{align*}\\] It is tedious but easy to check that \\(R\\) is idempotent: \\[\\begin{align*} R^2 &amp;= \\Bigl(X (X^\\top X)^{-1} K^\\top Q^{-1} K (X^\\top X)^{-1} X^\\top\\Bigr) \\Bigl(X (X^\\top X)^{-1} K^\\top Q^{-1} K (X^\\top X)^{-1} X^\\top\\Bigr) \\\\ &amp;= X (X^\\top X)^{-1} K^\\top Q^{-1} K (X^\\top X)^{-1} \\Bigl(X^\\top X (X^\\top X)^{-1} \\Bigr) K^\\top Q^{-1} K (X^\\top X)^{-1} X^\\top \\\\ &amp;= X (X^\\top X)^{-1} K^\\top \\Bigl( Q^{-1} K (X^\\top X)^{-1} K^\\top \\Bigr) Q^{-1} K (X^\\top X)^{-1} X^\\top \\\\ &amp;= X (X^\\top X)^{-1} K^\\top Q^{-1} K (X^\\top X)^{-1} X^\\top \\\\ &amp;= R. \\end{align*}\\] When we checked in the proof of Cochran’s theorem that \\(\\varepsilon^\\top H \\varepsilon\\) was chi-squared distributed, so only property of \\(H\\) we used was that \\(H\\) is idempotent. (If you don’t remember the details, it would be a good idea to re-read the proof before continuing.) Thus, the same argument gives that \\(\\varepsilon^\\top R \\varepsilon/ \\sigma^2\\) is chi-squared distributed, and as before the number of degrees of freedom of this chi-squared distribution equals the rank of \\(R\\). Using the assumption that \\(Q\\) is invertible, one can show (we skip this part of the proof again) that the rank of \\(Q\\) equals \\(\\min(k, p+1) = k\\) and thus we find that \\[\\begin{equation*} S_1 := \\frac{1}{\\sigma^2} \\bigl\\| K \\hat\\beta - K \\beta \\bigr\\|_Q^2 \\sim \\chi^2(k). \\end{equation*}\\] Similarly, from the direct application of Cochran’s theorem in equation (4.6), we know \\[\\begin{equation*} S_2 := \\frac{1}{\\sigma^2} (n - p - 1) \\hat\\sigma^2 \\sim \\chi^2(n - p - 1). \\end{equation*}\\] Since \\(S_1\\) is a function of \\(\\hat\\beta\\) and \\(S_2\\) is a function of \\(\\hat\\sigma^2\\), we can use lemma @ref{lem:hat-beta-sigma-indep} to conclude that \\(S_1\\) and \\(S_2\\) are independent. Combining these results we find \\[\\begin{align*} F &amp;= \\frac{\\bigl\\| K \\hat\\beta - K \\beta \\bigr\\|_{K(X^\\top X)^{-1} K^\\top}^2} {k \\hat\\sigma^2} \\\\ &amp;= \\frac{\\frac{1}{\\sigma^2} \\bigl\\| K \\hat\\beta - K \\beta \\bigr\\|_{K(X^\\top X)^{-1} K^\\top}^2 / k} {\\frac{1}{\\sigma^2} (n - p - 1) \\hat\\sigma^2 / (n - p - 1)} \\\\ &amp;= \\frac{S_1 / k}{S_2 / (n - p - 1)} \\\\ &amp;\\sim F_{k, n - p - 1}. \\end{align*}\\] This completes the proof. 6.2 Confidence Regions Using \\(F\\) as a distance between the unknown true \\(K\\beta\\) and the estimator \\(\\hat\\beta\\), it is easy to find a region of \\(\\mathbb{R}^k\\) which covers \\(K\\beta\\) with high probability. Since we now have an \\(k\\)-dimensional parameter vector, this region will no longer be an interval. Instead, we will get an \\(k\\)-dimensional ellipsoid. 6.2.1 Result Define \\[\\begin{equation*} E := \\Bigl\\{ z \\in \\mathbb{R}^k \\Bigm| \\bigl\\| z - K \\hat\\beta \\bigr\\|_{K(X^\\top X)^{-1} K^\\top} \\leq \\sqrt{k \\hat\\sigma^2 f_{k,n-p-1}(\\alpha)} \\Bigr\\}, \\end{equation*}\\] where \\(f_{k,n-p-1}(\\alpha)\\) is the \\((1-\\alpha)\\)-quantile of the \\(F_{k,n-p-1}\\)-distribution. This is a “ball” around \\(K\\hat\\beta\\) in \\(\\mathbb{R}^k\\), where distance is measured using the norm \\(\\| \\;\\cdot\\; \\|_{K(X^\\top X)^{-1} K^\\top}\\) introduced above. The following lemma shows that \\(\\sqrt{k \\hat\\sigma^2 f_{k,n-p-1}(\\alpha)}\\) is the correct choice of “radius” to make the ball cover the true value \\(K\\beta\\) with probability \\(1-\\alpha\\). Lemma 6.2 We have \\[\\begin{equation*} P\\bigl( K\\beta \\in E \\bigr) = 1 - \\alpha, \\end{equation*}\\] i.e. the set \\(E\\) is a \\((1-\\alpha)\\)-confidence region for \\(K\\beta\\). Proof. We have \\[\\begin{align*} &amp;\\hskip-5mm K\\beta \\in E \\\\ &amp;\\Longleftrightarrow \\bigl\\| K\\beta - K \\hat\\beta \\bigr\\|_{K(X^\\top X)^{-1} K^\\top} \\leq \\sqrt{k \\hat\\sigma^2 f_{k,n-p-1}(\\alpha)} \\\\ &amp;\\Longleftrightarrow \\bigl\\| K\\hat\\beta - K\\beta \\bigr\\|_{K(X^\\top X)^{-1} K^\\top}^2 \\leq k \\hat\\sigma^2 f_{k,n-p-1}(\\alpha) \\\\ &amp;\\Longleftrightarrow \\frac{\\bigl\\| K\\hat\\beta - K\\beta \\bigr\\|_{K(X^\\top X)^{-1} K^\\top}^2} {k \\hat\\sigma^2} \\leq f_{k,n-p-1}(\\alpha) \\\\ &amp;\\Longleftrightarrow F \\leq f_{k,n-p-1}(\\alpha) \\end{align*}\\] Now the claim follows, since \\(f_{k,n-p-1}(\\alpha)\\) is the \\((1-\\alpha)\\)-quantile of \\(F\\). 6.2.2 Numerical Experiments We start by fitting a linear model to the stackloss data set as before: m &lt;- lm(stack.loss ~ ., data = stackloss) summary(m) ## ## Call: ## lm(formula = stack.loss ~ ., data = stackloss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.2377 -1.7117 -0.4551 2.3614 5.6978 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -39.9197 11.8960 -3.356 0.00375 ** ## Air.Flow 0.7156 0.1349 5.307 5.8e-05 *** ## Water.Temp 1.2953 0.3680 3.520 0.00263 ** ## Acid.Conc. -0.1521 0.1563 -0.973 0.34405 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.243 on 17 degrees of freedom ## Multiple R-squared: 0.9136, Adjusted R-squared: 0.8983 ## F-statistic: 59.9 on 3 and 17 DF, p-value: 3.016e-09 Here I want to consider the two regressions coefficients \\(\\beta_1\\) (Air.Flow) and \\(\\beta_2\\) (Water.Temp). For this we need to construct a matrix \\(K\\) with two rows, where each row selects one of the two coefficients: X &lt;- model.matrix(m) n &lt;- nrow(X) p &lt;- ncol(X) - 1 K &lt;- matrix(c(0, 1, 0, 0, # indices in R start at 1, so beta_1 is col. 2 0, 0, 1, 0), # ... and beta_2 is column 3. byrow = TRUE, nrow = 2, ncol = 4) k &lt;- nrow(K) K.beta.hat &lt;- as.vector(K %*% coef(m)) As we have seen in the previous section, for this \\(K\\) the covariance matrix for the ellipse is \\(Q = K (X^\\top X)^{-1} K^\\top\\): Q &lt;- K %*% solve(t(X) %*% X) %*% t(K) 6.2.2.1 A Single Point To try out the method, we first consider the test point \\(m = K\\beta = (1, 1)\\) and we compute the \\(F\\) value for this point to see whether the point is inside or outside the ellipse: a &lt;- c(1, 1) sigma.hat &lt;- summary(m)$sigma d &lt;- a - K.beta.hat F &lt;- t(d) %*% solve(Q, d) / (k * sigma.hat^2) F ## [,1] ## [1,] 2.834083 The \\(F\\) value, measuring the distance from the centre of the ellipse is \\(2.834\\) for this test point. This has to be compared to the critical value: alpha &lt;- 0.05 f.crit &lt;- qf(1 - alpha, k, n - p - 1) f.crit ## [1] 3.591531 Since the \\(F\\) value is less than the critical value, the point \\((1, 1)\\) is inside the ellipse. As the next step, we will plot a picture of the full ellipse. 6.2.2.2 Points on a Grid An easy way to show the ellipse is to repeat the above procedure for all points on a rectangular grid, and then colour the points depending on whether they are inside or outside the ellipse. We start by making a list of grid points. x.min &lt;- -1 x.max &lt;- 3 y.min &lt;- -0.5 y.max &lt;- 3 L &lt;- 200 xx &lt;- seq(x.min, x.max, length.out = L) yy &lt;- seq(y.min, y.max, length.out = L) Z &lt;- as.matrix(expand.grid(x = xx - K.beta.hat[1], y = yy - K.beta.hat[2], KEEP.OUT.ATTRS = FALSE)) dim(Z) ## [1] 40000 2 The matrix \\(Z\\) now has two columns, containing the \\(x\\) and \\(y\\) coordinates respectively of the \\(200\\times 200\\) points in our grid. Now we need to compute the \\(F\\) value for every grid point: F &lt;- rowSums(Z * t(solve(Q, t(Z)))) / (k * sigma.hat^2) F &lt;- matrix(F, byrow = TRUE, L, L) dim(F) ## [1] 200 200 The resulting matrix contains the \\(F\\) value for every grid point. Finally, we can mark all the points where \\(F\\) exceeds the critical value in a plot: image(x = xx, y = yy, t(F &gt; f.crit), asp = 1, col = c(&quot;green&quot;, &quot;white&quot;), xlab = expression(beta[1]), ylab = expression(beta[2])) points(K.beta.hat[1], K.beta.hat[2], pch = &quot;+&quot;) The green ellipse in this plot is the 95% confidence ellipse for the vector \\((\\beta_1, \\beta_2)\\). 6.2.2.3 Coordinates of the Outline This sub-section is non-examinable (but hopefully interesting). Using some more linear algebra, we can find a formula for the coordinates of the points on the ellipse which forms the boundary of the confidence region. For this, we use the Singular Value Decomposition of the matrix \\(Q\\). This allows to write \\(Q\\) as \\[\\begin{equation*} Q = U D V^\\top \\end{equation*}\\] where \\(U\\) and \\(V\\) are orthogonal matrices and \\(D\\) is a diagonal matrix: svd &lt;- La.svd(Q) svd ## $d ## [1] 0.0138678012 0.0007364967 ## ## $u ## [,1] [,2] ## [1,] -0.2749061 0.9614711 ## [2,] 0.9614711 0.2749061 ## ## $vt ## [,1] [,2] ## [1,] -0.2749061 0.9614711 ## [2,] 0.9614711 0.2749061 The R output shows the diagonal elements \\(d_{ii}\\) of \\(D\\) and the matrices \\(U\\) and \\(V^\\top\\). Since \\(Q\\) is symmetric, we have \\(U = V\\) and thus \\(Q = U D U^\\top\\) in this case, \\(i.e.\\) we have found a diagonalisation of \\(Q\\). Using the SVD we can simplify the norm used in the definition of the ellipse. We write \\(D^{-1/2}\\) for the matrix which has \\(1/\\sqrt{d_{ii}}\\) on the diagonal. Then we have \\[\\begin{align*} Q \\bigl(D^{-1/2} U^\\top \\bigr)^\\top \\bigl(D^{-1/2} U^\\top \\bigr) &amp;= Q U D^{-1/2} D^{-1/2} U^\\top \\\\ &amp;= U D U^\\top U D^{-1} U^\\top \\\\ &amp;= U D D^{-1} U^\\top \\\\ &amp;= U U^\\top \\\\ &amp;= I. \\end{align*}\\] Thus, \\(\\bigl(D^{-1/2} U^\\top\\bigr)^\\top \\bigl(D^{-1/2} U^\\top\\bigr)\\) is the inverse of \\(Q\\) and we get \\[\\begin{align*} \\bigl\\| z - K \\hat\\beta \\bigr\\|_Q^2 &amp;= (z - K \\hat\\beta)^\\top Q^{-1} (z - K \\hat\\beta) \\\\ &amp;= (z - K \\hat\\beta)^\\top \\bigl(D^{-1/2} U^\\top\\bigr)^\\top \\bigl(D^{-1/2} U^\\top\\bigr) (z - K \\hat\\beta) \\\\ &amp;= \\Bigl\\| D^{-1/2} U^\\top (z - K \\hat\\beta) \\Bigr\\|^2, \\end{align*}\\] where the norm on the right-hand side is the usual Euclidean norm. Thus, the boundary of the ellipse consists of all points of the form \\(K \\hat\\beta + d\\), where \\(e := D^{-1} U^\\top d\\) is a vector of length \\[ \\| e \\| = \\sqrt{k \\hat\\sigma^2 f_{k,n-p-1}(\\alpha)}. \\] Finally, using polar coordinates, we find the points on the boundary to be \\[\\begin{equation*} d = U D e = U D \\sqrt{k \\hat\\sigma^2 f_{k,n-p-1}(\\alpha)} \\begin{pmatrix} \\cos(\\varphi) \\\\ \\sin(\\varphi) \\end{pmatrix} \\end{equation*}\\] with \\(\\varphi\\in [0, 2pi]\\). This allows to plot the boundary line in R. phi &lt;- seq(0, 2*pi, length.out = 201) circ &lt;- rbind(cos(phi), sin(phi)) * sqrt(f.crit * k * sigma.hat^2) ellipse &lt;- svd$u %*% (circ * sqrt(svd$d)) + K.beta.hat image(x = xx, y = yy, t(F &gt; f.crit), asp = 1, col = c(&quot;green&quot;, &quot;white&quot;), xlab = expression(beta[1]), ylab = expression(beta[2])) lines(ellipse[1,], ellipse[2,]) To verify that everything is consistent, we have plotted the line on top of the image from the previous subsection. Since the black line perfectly surrounds the green area, both approaches are consistent. 6.3 Hypothesis Tests We can easily derive a hypothesis test to test the hypothesis \\[\\begin{equation*} H_0\\colon K\\beta = m \\end{equation*}\\] against the alternative \\[\\begin{equation*} H_1\\colon K\\beta \\neq m, \\end{equation*}\\] where \\(m \\in \\mathbb{R}^k\\). We redefine \\(F\\) as \\[\\begin{equation*} F := \\frac{\\bigl\\| K \\hat\\beta - m \\bigr\\|_{K(X^\\top X)^{-1} K^\\top}^2} {k \\hat\\sigma^2} \\end{equation*}\\] using \\(m\\) in place of the \\(K\\beta\\) above. Then the new defintion of \\(F\\) is the same as (6.1) if \\(H_0\\) is true. Lemma 6.3 The test which rejects \\(H_0\\) if and only if \\(|F| &gt; f_{k,n-p-1}(\\alpha)\\) has significance level \\(\\alpha\\). Proof. Assume that \\(H_0\\) is true. Then we have \\(K\\beta = m\\) and thus the \\(F\\) defined in this section coincides with the expression from equation (6.1). From lemma 6.1 we know that \\(F \\sim F_{k, n-p-1}\\). Thus we have \\[\\begin{align*} P( \\mbox{type I error} ) &amp;= P\\bigl( F &gt; f_{k,n-p-1}(\\alpha) \\bigr) \\\\ &amp;= 1 - P\\bigl( F \\leq f_{k,n-p-1}(\\alpha) \\bigr) \\\\ &amp;= 1 - (1 - \\alpha) \\\\ &amp;= \\alpha. \\end{align*}\\] This completes the proof. "],["I02-read.html", "Interlude: Loading Data into R Importing CSV Files Importing Microsoft Excel Files Checking the Imported Data Common Problems", " Interlude: Loading Data into R Before we can analyse data using R, we have to “import” the data into R. How exactly this is done depends on how the data is stored, and more specifically on which file format is used. Here we consider two commonly used formats: comma-separated values (.csv) and Microsoft Excel files (.xls or .xlsx). Importing CSV Files The read.csv() command can be used to import .csv files into R: if we use the command x &lt;- read.csv(&quot;file.csv&quot;) then the contents of the file file.csv will be stored in the variable x. Optional arguments to read.csv() can be used to specify whether or not the file includes column names, and allow to deal with variations of how the data may be stored in the file. Some of these are explained below. The most important things to know about the function read.csv() are: The filename can either denote a file on the local computer, or a file available for download from the internet. If you want R to read the file directly from the internet, replace the file name with the web address (starting with http:// or https://). If the input file is on the local computer, you may need to change R’s current directory to the directory where the file is stored before calling read.csv(). In RStudio you can use the menu “Session &gt; Set Working Directory &gt; Choose Directory …” to do this. If you are not sure what the filename for a given input file is, you can use the command file.choose() in place of the file name, to choose a file interactively. By default, R uses the first row of the .csv file to set the column names of the data frame and assumes that the actual data starts in row 2 of the .csv file. If the file does not contain column names, you can use the header=FALSE option with read.csv() to tell R that the column names are not included in the data: x &lt;- read.csv(&quot;file.csv&quot;, header=FALSE) You can see whether this option is needed by opening the file in Excel and looking whether the first row contains headers or not. Alternatively you can inspect the column names and the contents of the first data row in R to see whether everything looks right after importing the data. R uses a special data type called Factor to represent categorical data. If the .csv file contains such data, the option stringsAsFactors=TRUE can be used to automatically convert text to factors. (This option used to be the default for R versions before 4.0.0.) Sometimes, the columns in a .csv file are separated not by a comma, but using a semicolon instead. In this case you need to use the option sep=\";\" when you import the data: x &lt;- read.csv(&quot;file.csv&quot;, sep=&quot;;&quot;) Missing values should be represented by empty cells in the .csv file and are represented as special NA values in the data frame. If the .csv file uses a different encoding for missing values, the na.strings option can be used to tell read.csv() which cell values should be interpreted as missing values. For example, read.csv(\"file.csv\", na.strings=c(\"\", \"-\")) can be used for a file where missing values are indicated by either empty cells or cells containing a hyphen. Further details about the function read.csv() can be found using the command help(read.csv) in R. Example 6.2 In the 2016 version of the MATH1712 module, I performed a small questionnaire in the first lecture. The following R command can be used to load the data from the questionnaire into R x &lt;- read.csv(&quot;http://www1.maths.leeds.ac.uk/~voss/rintro/Q2016.csv&quot;) The variable x now contains the questionnaire results. Instead of directly downloading the data from the internet into R, you can alternatively first download the data using a web browser, and then import the data directly from your computer: x &lt;- read.csv(&quot;Q2016.csv&quot;) Both approaches give the same result. Importing Microsoft Excel Files The easiest way to import Excel files into R is to first convert these files to .csv format. To do this: Open the file with the data in Excel. Open a new, empty file (choosing “Blank workbook” in Excel). Copy and paste the relevant cells into the empty file. It is important to just copy the required data and to leave out any explanatory text and and empty rows/columns. The data must form a tidy rectangle, with one sample per row and one variate per column. Optionally, you can put column headers into the first row. Save the new file in .csv format in a folder where you will find it again. Read the resulting .csv into R as explained above. Alternatively, you can try the read_excel() function from the readxl package. (You may need to install this R package first.) You can learn how to use this function at https://readxl.tidyverse.org/ or using the command help(read_excel). Note that these functions return a “tibble” instead of a data frame, so some additional knowledge is required to use the result. Checking the Imported Data The following commands can be used to get a quick overview over the data: dim(x) gives the number of rows and columns of the data frame. Similarly, nrow(x) shows the number of rows and ncol(x) shows the number of columns in the data frame. str(x) shows the structure of any R object. This command is often an excellent way to understand the nature of any problems one may encounter while importing data into R. summary(x) prints, for each variate, the values of various summary statistics. For variates with numeric values, these are the minimum, first quartile, median, mean, third quartile, maximum, and the number of missing values. For attribute data this gives the counts for each observed value. head(x) shows the first few rows of the data. Every time you import a data set into R, you should use some of these commands to check that everything went well. In case you discover problems, you should either fix the data file (e.g. using Microsoft Excel) or by using the correct options for the read.csv() command. Example 6.3 Continuing with the data set from example above, we can try the following commands: We first check that the data has plausible dimensions: dim(x) ## [1] 220 5 This tells us that the data has \\(n=220\\) rows and \\(p=5\\) columns, as expected. Next, we get some details about the five columns of the data frame: str(x) ## &#39;data.frame&#39;: 220 obs. of 5 variables: ## $ gender : chr &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; ... ## $ height : num 180 183 183 182 164 ... ## $ handedness : chr &quot;R&quot; &quot;R&quot; &quot;R&quot; &quot;R&quot; ... ## $ travel.time: int 20 25 20 12 5 20 15 10 11 7 ... ## $ R.skills : chr &quot;basic&quot; &quot;basic&quot; &quot;basic&quot; &quot;basic&quot; ... This shows, for each column, the name, the “data type” and the first few values. The “data type” is a good indicator to detect problems; it should read int for integer valued numeric data, num for all other numeric data, and Factor for attribute data. For attribute data, the range of observed values is also shown. For our data set, we see that the fields $gender, $handedness and $R.skills are represented as text data (type chr) instead of categorical data. To fix this, we re-import the data using the stringsAsFactors=TRUE option: x &lt;- read.csv(&quot;data/Q2016.csv&quot;, stringsAsFactors=TRUE) str(x) ## &#39;data.frame&#39;: 220 obs. of 5 variables: ## $ gender : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 2 2 2 2 1 1 1 1 1 2 ... ## $ height : num 180 183 183 182 164 ... ## $ handedness : Factor w/ 3 levels &quot;L&quot;,&quot;R&quot;,&quot;both&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ travel.time: int 20 25 20 12 5 20 15 10 11 7 ... ## $ R.skills : Factor w/ 4 levels &quot;basic&quot;,&quot;good&quot;,..: 1 1 1 1 1 1 1 3 1 1 ... Now the output is as expected, and the correct “levels” are shown for the categorical variables. Finally, we can print summary statistics for all columns: summary(x) ## gender height handedness travel.time R.skills ## F:102 Min. :147.0 L : 18 Min. : 0.00 basic :157 ## M:118 1st Qu.:167.0 R :201 1st Qu.: 5.00 good : 4 ## Median :175.0 both: 1 Median : 15.00 medium: 28 ## Mean :173.7 Mean : 19.15 none : 31 ## 3rd Qu.:180.3 3rd Qu.: 25.00 ## Max. :195.6 Max. :150.00 ## NA&#39;s :23 None of these commands showed any remaining problems, so we are now ready to use the data. Common Problems There are many things which can go wrong when importing data. Some commonly encountered problems are the following: If the data file contains no line with headers, but the header=FALSE option for read.csv() has been omitted, the first row of data will be used in place of the column names. This can, for example, be seen in the str() output. The solution to this problem is to correctly use the header=FALSE option. If the data in a .csv file is not separated by commas but by some other character like a semicolon, R will be unable to separate the columns. When this is the case, the imported data will appear to only have one column, where each entry shows as some garbled version of the data for the whole row. The solution to this problem is to use the sep=... option. If attribute values are encoded inconsistently, e.g. if a mix of m and M is used to encode the gender “male”, this will be visible in the str() output. One solution to this problem is to fix the .csv file using Microsoft Excel, before trying to import it into R again. If a numeric column in the input file contains one or more entries which are neither numbers nor empty, R will interpret the whole column as attribute data. This problem can be detected in the str() output, when a numeric column is listed as having data type Factor. One solution to this problem is to use Microsoft Excel to remove or fix the offending entry from the file. "],["S07-examples.html", "Section 7 Examples 7.1 Simple Confidence Interval 7.2 Confidence Intervals for the Mean 7.3 Testing a Single Coefficient 7.4 Testing Multiple Coefficents", " Section 7 Examples To illustrate the results of the last two sections, we consider a series of examples. 7.1 Simple Confidence Interval In this subsection we will illustrate how to compute a simple confidence interval for a single regression coefficient. We use a dataset about toxicity of chemicals towards aquatic life from the from the UCI Machine Learning Repository. The data is available at https://archive.ics.uci.edu/ml/datasets/QSAR+aquatic+toxicity There are nine different variables. (The web page explains the interpretation of these variables.) We will fit a model which describes LC50 as a function of the remaining variables: # data from https://archive.ics.uci.edu/ml/datasets/QSAR+aquatic+toxicity qsar &lt;- read.csv(&quot;data/qsar_aquatic_toxicity.csv&quot;, sep = &quot;;&quot;, header = FALSE) names(qsar) &lt;- c( &quot;TPSA&quot;, &quot;SAacc&quot;, &quot;H050&quot;, &quot;MLOGP&quot;, &quot;RDCHI&quot;, &quot;GATS1p&quot;, &quot;nN&quot;, &quot;C040&quot;, &quot;LC50&quot; ) m &lt;- lm(LC50 ~ ., data = qsar) summary(m) ## ## Call: ## lm(formula = LC50 ~ ., data = qsar) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.4934 -0.7579 -0.1120 0.5829 4.9778 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.698887 0.244554 11.036 &lt; 2e-16 *** ## TPSA 0.027201 0.002661 10.220 &lt; 2e-16 *** ## SAacc -0.015081 0.002091 -7.211 1.90e-12 *** ## H050 0.040619 0.059787 0.679 0.497186 ## MLOGP 0.446108 0.063296 7.048 5.60e-12 *** ## RDCHI 0.513928 0.135565 3.791 0.000167 *** ## GATS1p -0.571313 0.153882 -3.713 0.000227 *** ## nN -0.224751 0.048301 -4.653 4.12e-06 *** ## C040 0.003194 0.077972 0.041 0.967340 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.203 on 537 degrees of freedom ## Multiple R-squared: 0.4861, Adjusted R-squared: 0.4785 ## F-statistic: 63.5 on 8 and 537 DF, p-value: &lt; 2.2e-16 Our aim is to find a 95% confidence interval for GATS1p. 7.1.1 From First Principles To compute the confidence interval from first principles, we can use Lemma 5.3. The centre of the confidence interval is given by \\(\\hat\\beta_i\\): mid &lt;- coef(m)[7] mid ## GATS1p ## -0.5713131 To find the half-width of the interval, we need to first construct the covariance matrix \\(C\\) and the estimated residual variance \\(\\hat\\sigma^2\\). We will also need the corresponding quantile of the \\(t\\)-distribution. X &lt;- model.matrix(m) y &lt;- qsar$LC50 n &lt;- nrow(X) p &lt;- ncol(X) - 1 Cii &lt;- solve(t(X) %*% X)[7, 7] cat(&quot;Cii =&quot;, Cii, &quot;\\n&quot;) ## Cii = 0.01637454 sigma.hat.squared &lt;- sum((fitted(m) - y)^2) / (n - p - 1) cat(&quot;hat.sigma.square =&quot;, sigma.hat.squared, &quot;\\n&quot;) ## hat.sigma.square = 1.446134 alpha &lt;- 0.05 t &lt;- qt(1 - alpha/2, n - p - 1) cat(&quot;t =&quot;, t, &quot;\\n&quot;) ## t = 1.964391 With this information in place, we can easily find the confidence interval: d &lt;- sqrt(Cii * sigma.hat.squared) * t cat(&quot;[&quot;, mid - d, &quot;, &quot;, mid + d, &quot;]\\n&quot;, sep = &quot;&quot;) ## [-0.8735983, -0.2690279] 7.1.2 From the lm() Output We can obtain the same result using only the quantile \\(t\\) and numbers shows in the summary(m) output. We focus on the row corresponding to GATS1p in the output shown above: GATS1p -0.571313 0.153882 -3.713 0.000227 *** The first number in this row is the centre of the confidence interval, the second number corresponds to \\(\\sqrt{\\hat\\sigma^2 C_{ii}}\\). To have everything in one place, we copy the commands for computing \\(t\\): alpha &lt;- 0.05 t &lt;- qt(1 - alpha/2, n - p - 1) cat(&quot;[&quot;, -0.571313 - 0.153882*t, &quot;, &quot;, -0.571313 + 0.153882*t, &quot;]\\n&quot;, sep = &quot;&quot;) ## [-0.8735975, -0.2690285] This is the same result as above. 7.2 Confidence Intervals for the Mean So far we have only seen how to compute confidence intervals for the regression coefficients \\(\\beta_i\\). Using the same techniques we can also compute confidence intervals for the model mean corresponding to an input \\((\\tilde x_1, \\ldots, \\tilde x_p)\\). If we write \\[\\begin{equation*} \\tilde x = (1, \\tilde x_1, \\ldots, \\tilde x_p), \\end{equation*}\\] then the model mean corresponding to this input is \\[\\begin{equation*} \\tilde y = \\beta_0 + x_1 \\beta_1 + \\cdots + x_p \\beta_p. = \\tilde x^\\top \\beta, \\end{equation*}\\] To derive a confidence interval for this quantity, we follow the same steps as we did when we derived a confidence interval for \\(\\hat\\beta_i\\). Our estimator for this \\(\\tilde y\\) is \\[\\begin{equation*} \\hat y = \\tilde x^\\top \\hat\\beta \\sim \\mathcal{N}\\bigl( \\tilde x^\\top \\beta, \\sigma^2 \\tilde x^\\top (X^\\top X)^{-1} \\tilde x \\bigr). \\end{equation*}\\] Thus, \\[\\begin{align*} \\tilde T &amp;:= \\frac{\\tilde x^\\top \\hat\\beta - \\tilde x^\\top \\beta} {\\sqrt{\\hat\\sigma^2 \\tilde x^\\top (X^\\top X)^{-1} \\tilde x}} \\\\ &amp;= \\frac{\\frac{1}{\\sqrt{\\sigma^2\\tilde x^\\top (X^\\top X)^{-1} \\tilde x}}\\bigl(\\tilde x^\\top \\hat\\beta - \\tilde x^\\top \\beta\\bigr)} {\\sqrt{\\frac{1}{\\sigma^2}\\hat\\sigma^2}} \\end{align*}\\] is \\(t(n-p-1)\\)-distributed, since the numerator is \\(\\mathcal{N}(0,1)\\), the denominator is \\(\\sqrt{\\chi^2(n-p-1) / (n-p-1)}\\) and \\(\\hat\\beta\\) is independent of \\(\\hat\\sigma^2\\). Let \\(t_{n-p-1}(\\alpha/2)\\) be the \\((1 - \\alpha/2)\\)-quantile of the \\(t(n-p-1)\\)-distribution. Then we can solve the inequality \\(|T| \\leq t_{n-p-1}(\\alpha/2)\\) for \\(\\tilde x^\\top \\beta\\) to find the required confidence interval: \\[\\begin{equation*} \\Bigl[ \\tilde x^\\top \\hat\\beta - \\sqrt{\\hat\\sigma^2 \\tilde x^\\top (X^\\top X)^{-1} \\tilde x} t_{n-p-1}(\\alpha/2), \\tilde x^\\top \\hat\\beta + \\sqrt{\\hat\\sigma^2 \\tilde x^\\top (X^\\top X)^{-1} \\tilde x} t_{n-p-1}(\\alpha/2) \\Bigr]. \\end{equation*}\\] We illustrate this using a numerical example. For the stackloss data set with input values Air.Flow = 60, Water.Temp = 21 and Acid.Conc = 87, we can get the following results: m &lt;- lm(stack.loss ~ ., data = stackloss) X &lt;- model.matrix(m) n &lt;- nrow(X) p &lt;- ncol(X) - 1 tilde.x &lt;- c(1, 60, 21, 87) hat.beta &lt;- coef(m) c &lt;- t(tilde.x) %*% solve(t(X) %*% X, tilde.x) sigma.hat.squared &lt;- sum((fitted(m) - stackloss$stack.loss)^2) / (n - p - 1) t &lt;- qt(0.975, n - p - 1) mid &lt;- t(tilde.x) %*% hat.beta d &lt;- sqrt(c * sigma.hat.squared) * t cat(&quot;[&quot;, mid - d, &quot;, &quot;, mid + d, &quot;]\\n&quot;, sep = &quot;&quot;) ## [15.46463, 18.50554] 7.3 Testing a Single Coefficient Continuing with the dataset from the previous example, we can test whether the regression coefficient corresponding to the water temperature might equal zero. We will perform a test at \\(5\\%\\)-level. 7.3.1 From First Principles We can compute the test statistic manually, using the formula from equation (5.2) with \\(b = 0\\): T &lt;- hat.beta[3] / sqrt(sigma.hat.squared * solve(t(X) %*% X)[3, 3]) T ## Water.Temp ## 3.519567 t.crit &lt;- qt(0.975, n - p - 1) abs(T) &gt; t.crit ## Water.Temp ## TRUE Since the test statistic is larger than the critical value, we reject the hypothesis that \\(\\beta_2 = 0\\). 7.3.2 Using the lm() Output, I We can also find the test statistic in the lm() output: summary(m) ## ## Call: ## lm(formula = stack.loss ~ ., data = stackloss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.2377 -1.7117 -0.4551 2.3614 5.6978 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -39.9197 11.8960 -3.356 0.00375 ** ## Air.Flow 0.7156 0.1349 5.307 5.8e-05 *** ## Water.Temp 1.2953 0.3680 3.520 0.00263 ** ## Acid.Conc. -0.1521 0.1563 -0.973 0.34405 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.243 on 17 degrees of freedom ## Multiple R-squared: 0.9136, Adjusted R-squared: 0.8983 ## F-statistic: 59.9 on 3 and 17 DF, p-value: 3.016e-09 The column t value shows the value of the test statistic \\(T\\) for the hypothesis that \\(\\beta_i = 0\\). For \\(\\beta_2\\) we find the value \\(3.520\\), which is a rounded version of the \\(3.519567\\) we found above. Again, we can use the command qt(0.975, 17) to get the critical value, where we can find the degrees of freedom in the Residual standard error row near the bottom. Alternatively, we can use the so called “p-value” listed in the Pr(&gt;|t|) column. This value is the smallest significance level at which \\(H_0\\) is still rejected. Here we find \\(0.00263\\) and since we have \\(\\alpha = 0.05 &gt; 0.00263\\), we can see that \\(H_0\\) is rejected. If we wanted to test \\(\\beta_2 = 1\\) instead, we could use the value \\(\\hat\\beta_2 = 1.2953\\) from the Estimate column and \\(\\sqrt{\\hat\\sigma^2 C_{ii}} = 0.3680\\) from the Std. Error column, to get \\(T = (1.2953 - 1) / 0.3680\\) and then we would accept or reject the hypothesis \\(\\beta_2 = 1\\) by comparing the result to the critical value qt(0.975, 17). 7.3.3 Using the lm() Output, II For the hypotheses \\(\\beta_i = 0\\), we can also use the stars (or sometimes dots) shown in the right margin to perform the test. Since the Water.Temp row ends in **, we would reject \\(\\beta_2 = 0\\) even at the stricter \\(\\alpha=0.01\\) level, and thus we would also reject this hypothesis at \\(\\alpha=0.05\\) level. 7.4 Testing Multiple Coefficents This section illustrates how a group of coefficients can be tested simultaneously. We consider a small, synthetic dataset: # data from https://www1.maths.leeds.ac.uk/~voss/data/small/small.csv small &lt;- read.csv(&quot;data/small.csv&quot;) m &lt;- lm(y ~ ., data = small) summary(m) ## ## Call: ## lm(formula = y ~ ., data = small) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.64664 -0.09369 -0.03420 0.07768 0.63353 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.53329 0.12360 44.768 6.92e-12 *** ## x1 0.18903 0.12059 1.568 0.1514 ## x2 -0.30731 0.18011 -1.706 0.1221 ## x3 -0.12382 0.05639 -2.196 0.0557 . ## x4 -0.11731 0.12619 -0.930 0.3768 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3822 on 9 degrees of freedom ## Multiple R-squared: 0.6404, Adjusted R-squared: 0.4806 ## F-statistic: 4.007 on 4 and 9 DF, p-value: 0.03891 From the R output it is clear that the intercept is non-zero, but none of the remaining regression coefficients are significantly different from zero when testing at \\(5\\%\\) level. This poses the question whether the inputs have any effect on the output at all. To answer this question, we test the hypothesis \\[\\begin{equation*} H_0\\colon \\beta_1 = \\beta_2 = \\beta_3 = \\beta_4 = 0, \\end{equation*}\\] omitting only \\(\\beta_0\\) in the hypothesis. We use the method from section 6.3 to perform this test. 7.4.1 From First Principles The matrix \\(K\\) which selects the regression coefficents is \\[\\begin{equation*} K = \\begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix} \\in \\mathbb{R}^{4\\times 5}. \\end{equation*}\\] The test statistic is \\[\\begin{equation*} F = \\frac{\\bigl\\| K \\hat\\beta - 0 \\bigr\\|_{K(X^\\top X)^{-1} K^\\top}^2} {4 \\hat\\sigma^2}, \\end{equation*}\\] where \\(K \\hat\\beta = (\\hat\\beta_1, \\ldots, \\hat\\beta_4)\\) and \\(K(X^\\top X)^{-1} K^\\top\\) is obtained by removing the first row and column from \\((X^\\top X)^{-1}\\): b &lt;- coef(m)[2:5] X &lt;- model.matrix(m) C &lt;- solve(t(X) %*% X)[2:5, 2:5] F &lt;- t(b) %*% solve(C, b) / (4 * summary(m)$sigma^2) F ## [,1] ## [1,] 4.007393 From lemma 6.3 we know that under \\(H_0\\) the test statistic is \\(F_{k, n-p-1}\\) distributed, and the critical value is the \\(1-\\alpha\\) quantile of this distribution: n &lt;- nrow(X) # 14 p &lt;- ncol(X) - 1 # 4 k &lt;- 4 t &lt;- qf(0.95, k, n - p - 1) t ## [1] 3.633089 Since the test statistic is larger than the critical value, we can reject \\(H_0\\) and conclude at significance level \\(5\\%\\) that not all regression coefficients can be zero. 7.4.2 Using the lm() output The information required to perform this \\(F\\) test is contained in the last row of the summary(m) output: F-statistic: 4.007 on 4 and 9 DF, p-value: 0.03891 Here we can see that the test statistic is \\(4.007\\) (a rounded version of the value we found above) and the degreen of freedom for the \\(F\\)-test are \\(k = 4\\) and \\(n - p - 1 = 9\\). The “p-value” listed is the smallest significance level at which the test still rejects \\(H_0\\). We see that \\(H_0\\) is rejected at significance level \\(0.03891\\), and thus also at \\(\\alpha=0.05\\). "],["P02.html", "Problem Sheet 2", " Problem Sheet 2 .fold-btn { float: right; margin: -12px 0 0 0; } .myanswers { display: none !important; } 5 Assume that \\(X \\sim \\mathcal{N}(0, 1)\\), \\(Y\\sim \\chi^2(2)\\) and \\(Z \\sim \\chi^2(3)\\) are independent. What are the distributions of the following random variables? \\(X^2\\), \\(X^2 + Y + Z\\), \\(\\displaystyle \\frac{\\sqrt{2}X}{\\sqrt{Y}}\\), \\(\\displaystyle \\frac{2X^2}{Y}\\), and \\(1.5\\, Y/Z\\). The distributions involved in this question are the normal distribution, the \\(\\chi^2\\)-dis-tri-bu-tion, the \\(t\\)-distribution and the \\(F\\)-distribution. The general rules are: (1) the sum of squares of \\(d\\) independent, standard normally distributed random variables follows a \\(\\chi^2(d)\\)-distribution. (2) If \\(Z\\sim\\mathcal{N}(0,1)\\) and \\(V \\sim \\chi^2(d)\\) are independent, then \\(Z / \\sqrt{V / d} \\sim t(d)\\), and finally (3) If \\(V_1\\sim \\chi^2(d_1)\\) and \\(V_2\\sim \\chi^2(d_2)\\) are independent, then \\((V_1/d_1)/(V_2/d_2) \\sim F(d_1, d_2)\\). Using these rules: \\(X^2 \\sim \\chi^2(1)\\), \\(X^2 + Y + Z \\sim \\chi^2(1+2+3) = \\chi^2(6)\\), \\(\\displaystyle \\frac{\\sqrt{2}X}{\\sqrt{Y}} = \\frac{X}{\\sqrt{Y/2}} \\sim t(2)\\), \\(\\displaystyle \\frac{2X^2}{Y} = \\frac{X^2/1}{Y/2} \\sim F(1,2)\\), \\(\\displaystyle 1.5\\, Y/Z = \\frac{Y/2}{Z/3} \\sim F(2, 3)\\). 6 Let \\(\\mathbf{1} = (1, 1, \\ldots, 1) \\in\\mathbb{R}^n\\) and let \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2 I)\\) be a normally-distributed random vector, where \\(I\\) is the \\(n\\times n\\) identity matrix. Define \\(A = \\frac1n \\mathbf{1} \\mathbf{1}^\\top \\in \\mathbb{R}^{n\\times n}\\). Show that \\((AX)_i = \\bar X\\) for all \\(i \\in \\{1, \\ldots, n\\}\\), where \\(\\bar X = \\frac1n \\sum_{j=1}^n X_j\\). Show that \\(A\\) is symmetric. Show that \\(A\\) is idempotent. Use results from lectures to conclude that the random variables \\(AX\\) and \\(X^\\top (I-A)X\\) are independent. Using the previous parts of the question, show that \\(\\bar X\\) and \\(\\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar X)^2\\) are independent. The matrix \\(A = (a_{ij})\\) has entries \\(a_{ij} = 1/n\\) for all \\(i, j \\in \\{1, \\ldots, n\\}\\). Using this observation, the statements of the first few questions follow easily: We have \\[\\begin{equation*} (Ax)_i = \\sum_{j=1}^n a_{ij} x_j = \\sum_{j=1}^n \\frac1n x_j = \\frac1n \\sum_{j=1}^n x_j = \\bar x \\end{equation*}\\] for all \\(i \\in \\{1, \\ldots, n\\}\\). We have \\[\\begin{equation*} a_{ij} = \\frac1n = a_{ji} \\end{equation*}\\] for all \\(i, j \\in \\{1, \\ldots, n\\}\\) and thus \\(A\\) is symmetric. We have \\(\\mathbf{1}^\\top \\mathbf{1} = \\sum_{i=1}^n 1 \\cdot 1 = n\\) and thus \\[\\begin{equation*} A^2 = \\bigl( \\frac1n \\mathbf{1} \\mathbf{1}^\\top \\bigr) \\bigl( \\frac1n \\mathbf{1} \\mathbf{1}^\\top \\bigr) = \\frac{1}{n^2} \\mathbf{1} \\bigl(\\mathbf{1}^\\top\\mathbf{1}\\bigr) \\mathbf{1}^\\top = \\frac1n \\mathbf{1} \\mathbf{1}^\\top = A. \\end{equation*}\\] (Alternatively, we could use the fact that we know the entries of \\(A\\) and compute \\(A^2\\) explicitly.) For the remaining statements we will use the fact that \\(X\\) is normally distributed. We will repeatedly use the result that if two random variables \\(X\\) and \\(Y\\) are independent, that then any function of \\(X\\) is independent of any function of \\(Y\\). In lectures we learned that, if \\(A\\) is symmetric and idempotent and if \\(\\varepsilon\\sim \\mathcal{N}(0, I)\\), then \\(A\\varepsilon\\) and \\((I-A)\\varepsilon\\) are independent. Applying this result with \\(\\varepsilon= (X - \\mu \\mathbf{1}) / \\sigma\\) we find that \\(AX = \\sigma A\\varepsilon+ \\mu A \\mathbf{1}\\) and \\((I-A)X = \\sigma (I-A)\\varepsilon+ \\mu (I-A)\\mathbf{1}\\) are independent, since they are functions of \\(A\\varepsilon\\) and \\((I-A)\\varepsilon\\). Since \\((I - A)^\\top (I - A) = I^2 - I A - A I + A^2 = I - A - A + A = I - A\\) we have \\[\\begin{equation*} X^\\top (I-A)X = X^\\top (I - A)^\\top (I-A)X = \\Bigl( (I-A)X \\Bigr)^\\top \\Bigl( (I-A)X \\Bigr). \\end{equation*}\\] Thus, \\(X^\\top (I-A)X\\) is a function of \\((I-A)X\\) and as such is also independent of \\(AX\\). We have seen \\((AX)_i = \\bar X\\) and thus \\(\\bigl((I - A)X\\bigr)_i = X_i - \\bar X\\). This gives \\[\\begin{align*} X^\\top (I-A)X &amp;= \\Bigl( (I-A)X \\Bigr)^\\top \\Bigl( (I-A)X \\Bigr) \\\\ &amp;= \\sum_{i=1}^n \\Bigl( (I-A)X \\Bigr)_i^2 \\\\ &amp;= \\sum_{i=1}^n (X_i - \\bar X)^2 \\end{align*}\\] Thus we can write the sample mean as \\(\\bar X = (AX)_i\\), and the sample variance as \\(\\mathrm{s}_X^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar X)^2 = X^\\top (I-A)X / (n-1)\\), and if \\(X\\) is normally distributed the two values \\(\\bar X\\) and \\(\\mathrm{s}_X^2\\) are independent. 7 Consider the following R commands: m &lt;- lm(stack.loss ~ ., data=stackloss) summary(m) ## ## Call: ## lm(formula = stack.loss ~ ., data = stackloss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.2377 -1.7117 -0.4551 2.3614 5.6978 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -39.9197 11.8960 -3.356 0.00375 ** ## Air.Flow 0.7156 0.1349 5.307 5.8e-05 *** ## Water.Temp 1.2953 0.3680 3.520 0.00263 ** ## Acid.Conc. -0.1521 0.1563 -0.973 0.34405 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.243 on 17 degrees of freedom ## Multiple R-squared: 0.9136, Adjusted R-squared: 0.8983 ## F-statistic: 59.9 on 3 and 17 DF, p-value: 3.016e-09 Either using this output, or using R to further inspect the built-in stackloss data set, find a \\(99\\%\\)-confidence interval for the parameter \\(\\beta_\\mathtt{Acid.Conc.}\\). From lectures, we know that a confidence interval for a single coefficient \\(\\beta_j\\) is given by \\[\\begin{equation*} [U, V] = \\Bigl[ \\hat\\beta_j - t_{n-p-1}(\\alpha/2) \\sqrt{\\widehat{\\sigma^2} C_{jj}}, \\hat\\beta_j + t_{n-p-1}(\\alpha/2) \\sqrt{\\widehat{\\sigma^2} C_{jj}} \\Bigr], \\end{equation*}\\] where \\(t_{n-p-1}\\) is a quantile of the \\(t\\)-distribution, \\(C_{jj} = (X^\\top X)^{-1}_{jj}\\), and \\(X\\) is the design matrix. We can read off the required values for computing the confidence interval from the output of summary(m): the centre of the confidence interval can be found in the column Estimate, the standard error \\(\\sqrt{\\widehat{\\sigma^2} C_{jj}}\\) is given in column Std. Error, and the value \\(n-p-1\\) for the \\(t\\)-quantile can be found as the degrees of freedom for the residual standard error, near the bottom of the output. Using these values, we can find a \\(95\\%\\)-confidence interval for Acid.Conc. as follows: c(-0.1521 - qt(0.995, 17) * 0.1563, -0.1521 + qt(0.995, 17) * 0.1563) ## [1] -0.6050934 0.3008934 Thus, the confidence interval is \\(\\bigl[ -0.605, 0.301 \\bigr]\\). 8 The file 20211020.csv from https://www1.maths.leeds.ac.uk/~voss/data/20211020.csv contains pairs of \\((x, y)\\) values. Fit the following models to the data: \\(y = \\beta_0 + \\beta_1 x + \\varepsilon\\) \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\varepsilon\\) \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\varepsilon\\) For each model, create a “residual plot”, i.e. a scatter plot which has the fitted values on the horizontal axis and the residuals on the vertical axis. Comment on the resulting plots. # data from https://www1.maths.leeds.ac.uk/~voss/data/20211020.csv d &lt;- read.csv(&quot;data/20211020.csv&quot;) m1 &lt;- lm(y ~ x, data = d) plot(fitted(m1), resid(m1)) m2 &lt;- lm(y ~ x + I(x^2), data = d) plot(fitted(m2), resid(m2)) m3 &lt;- lm(y ~ x + I(x^2) + I(x^3), data = d) plot(fitted(m3), resid(m3)) "],["Sx1-matrices.html", "A Linear Algebra Reminders A.1 Vectors A.2 Matrices A.3 Eigenvalues", " A Linear Algebra Reminders A.1 Vectors We write \\(v \\in \\mathbb{R}^d\\) if \\(v = (v_1, \\ldots, v_d)\\) for numbers \\(v_1, \\ldots, v_d \\in\\mathbb{R}\\). We say that \\(v\\) is a \\(d\\)-dimensional vector, and \\(\\mathbb{R}^d\\) is the \\(d\\)-dimensional Euclidean space. Vectors are often graphically represented as “column vectors”: \\[\\begin{equation*} v = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_d \\end{pmatrix}. \\end{equation*}\\] If \\(u,v\\in\\mathbb{R}^d\\) are two vectors, the inner product of \\(u\\) and \\(v\\) is given by \\[\\begin{equation} u^\\top v = \\sum_{i=1}^d u_i v_i. \\tag{A.1} \\end{equation}\\] Note that the two vectors must have the same length for the inner product to exist. The vectors \\(u\\) and \\(v\\) are said to be orthogonal, if \\(u^\\top v = 0\\). Using this notation, the Euclidean length of a vector \\(v\\) can be written as \\[\\begin{equation*} \\|v\\| = \\sqrt{\\sum_{i=1}^d v_i^2} = \\sqrt{v^\\top v}. \\end{equation*}\\] A.2 Matrices We write \\(A \\in \\mathbb{R}^{m\\times n}\\) if \\[\\begin{equation*} A = \\begin{pmatrix} a_{1,1} &amp; \\ldots &amp; a_{1,n}\\\\ a_{2,1} &amp; \\ldots &amp; a_{2,n}\\\\ \\vdots &amp; \\ddots &amp; \\vdots\\\\ a_{m,1} &amp; \\ldots &amp; a_{m,n} \\end{pmatrix}, \\end{equation*}\\] where \\(a_{i,j}\\), sometimes also written as \\(a_{ij}\\) are numbers for \\(i \\in \\{1, \\ldots, m\\}\\) and \\(j \\in \\{1, \\ldots, n\\}\\). A.2.1 Transpose If \\(A \\in \\mathbb{R}^{m\\times n}\\), then the transpose of \\(A\\) is the matrix \\(A^\\top \\in \\mathbb{R}^{n\\times m}\\), with \\((A^\\top)_{ij} = a_{ji}\\) for all \\(i \\in \\{1, \\ldots, n\\}\\) and \\(j \\in \\{1, \\ldots, m\\}\\). Graphically, this can be written as \\[\\begin{equation*} A^\\top = \\begin{pmatrix} a_{1,1} &amp; a_{2,1} \\ldots &amp; a_{m,1}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ a_{1,n} &amp; a_{2,n} \\ldots &amp; a_{m,n} \\end{pmatrix}, \\end{equation*}\\] Definition A.1 A matrix \\(A\\) is called symmetric, if \\(A^\\top = A\\). A.2.2 Matrix-vector Product If \\(A \\in \\mathbb{R}^{m \\times n}\\) and \\(v \\in \\mathbb{R}^n\\), then \\(Av \\in \\mathbb{R}^m\\) is the vector with \\[\\begin{equation*} (Av)_i = \\sum_{j=1}^n a_{ij} v_j \\end{equation*}\\] for all \\(i \\in \\{1, \\ldots, m\\}\\). If we consider \\(v\\) to be a \\((n\\times 1)\\)-matrix instead of a vector, \\(Av\\) can also be interpreted as a matrix-matrix product between an \\(m \\times n\\) and an \\(n\\times 1\\) matrix. Using this convention, \\(v^\\top\\) is then interpreted as an \\(1 \\times n\\) matrix and if \\(u\\in\\mathbb{R}^m\\) we have \\(u^\\top A \\in \\mathbb{R}^{1 \\times n} \\cong \\mathbb{R}^n\\) with \\[\\begin{equation*} (u^\\top A)_j = \\sum_{i=1}^m u_i a_{ij} \\end{equation*}\\] for all \\(j \\in \\{1, \\ldots, n\\}\\). Going one step further, this notation also motivates the expression \\(u^\\top v\\) in equation (A.1). A.2.3 Matrix-matrix Product If \\(A \\in \\mathbb{R}^{\\ell \\times m}\\) and \\(B \\in \\mathbb{R}^{m\\times n}\\), then \\(AB \\in \\mathbb{R}^{\\ell \\times n}\\) is the matrix with \\[\\begin{equation*} (AB)_{ik} = \\sum_{j=1}^m a_{ij} b_{jk} \\end{equation*}\\] for all \\(i \\in \\{1, \\ldots, \\ell\\}\\) and \\(j \\in \\{1, \\ldots, n\\}\\). This is called the matrix product of \\(A\\) and \\(B\\). Note that \\(A\\) and \\(B\\) must have compatible shapes for the product to exist. Properties: The matrix product is associative: if \\(A\\), \\(B\\) and \\(C\\) are matrices with shapes such that \\(AB\\) and \\(BC\\) exist, then we have \\(A(BC) = (AB)C\\). It does not matter in which order we perform the matrix products here. The matrix product is transitive: if \\(A\\), \\(B\\) and \\(C\\) have the correct shapes, we have \\(A(B+C) = AB + AC\\). The matrix product is not commutative: if \\(AB\\) exists, in general \\(A\\) and \\(B\\) don’t have the correct shapes for \\(BA\\) to also exist, and even if \\(BA\\) exists, in general we have \\(AB \\neq BA\\). Taking the transpose swaps the order in a matrix product: we have \\[\\begin{equation} (AB)^\\top = B^\\top A^\\top \\tag{A.2} \\end{equation}\\] A.2.4 Matrix Inverse If \\(A\\) is a square matrix and if there is a matrix \\(B\\) such that \\(AB = I\\), then \\(A\\) is called invertible and the matrix \\(B\\) is called the inverse of \\(A\\), denoted by \\(A^{-1} := B\\). Some important properties of the inverse: The inverse, if it exists, is unique. Left-inverse and right-inverse for matrices are the same: \\(A^{-1} A = I\\) holds if and only if \\(A A^{-1} = I\\). If \\(A\\) is symmetric and invertible, then \\(A^{-1}\\) is also symmetric. (Proof: \\(A (A^{-1})^\\top = (A^{-1} A)^\\top = I^\\top = I\\) and thus \\((A^{-1})^\\top\\) is an inverse of \\(A\\). Since the inverse is unique, \\((A^{-1})^\\top = A^{-1}\\).) A.2.5 Orthogonal Matrices Definition A.2 A matrix \\(U\\) is called orthogonal, if \\(U^\\top U = I = U U^\\top\\). If \\(U\\) is orthogonal, the inverse and the transpose are the same: \\(U^\\top = U^{-1}\\). A.2.6 Positive Definite Matrices Definition A.3 A symmetric matrix \\(A \\in \\mathbb{R}^{n\\times n}\\) is called positive definite, if \\[\\begin{equation*} x^\\top A x &gt; 0 \\end{equation*}\\] for all \\(x \\in \\mathbb{R}^n\\) with \\(x\\neq 0\\). The matrix is called positive semi-definite, if \\[\\begin{equation*} x^\\top A x \\geq 0 \\end{equation*}\\] for all \\(x \\in \\mathbb{R}^n\\). A.2.7 Idempotent Matrices Definition A.4 The matrix \\(A\\) is idempotent, if \\(A^2 = A\\). A.3 Eigenvalues Definition A.5 Let \\(A \\in\\mathbb{R}^{n\\times n}\\) be a square matrix and \\(\\lambda\\in R\\). The number \\(\\lambda\\) is called an eigenvalue of \\(A\\), if there exists a vector \\(v \\neq 0\\) such that \\(A x = \\lambda x\\). Any such vector \\(x\\) is called an eigenvector of \\(A\\) with eigenvalue \\(\\lambda\\). While there are very many results about eigenvectors and eigenvalues in Linear Algebra, here we will only use a small number of these results. We summarise what we need for this module: If \\(A\\) is idempotent and \\(x\\) is an eigenvector with eigenvalue \\(\\lambda\\), then we have \\(\\lambda x = A x = A^2 x = \\lambda Ax = \\lambda^2 x\\). Thus we have \\(\\lambda^2 = \\lambda\\). This shows that the only eigenvalues possible for idempotent matrices are \\(0\\) and \\(1\\). Theorem A.1 Let \\(A\\in\\mathbb{R}^{n\\times n}\\) be symmetric. Then there is an orthogonal matrix \\(U\\) such that \\(D := U A U^\\top\\) is diagonal. The diagonal elements of \\(D\\) are the eigenvalues of \\(A\\) and the rows of \\(U\\) are corresponding eigenvectors. "],["Sx2-probability.html", "B Probability Reminders B.1 Independence B.2 The Chi-Squared Distribution B.3 The t-distribution", " B Probability Reminders B.1 Independence Definition B.1 Two random variables \\(X\\) and \\(Y\\) are (statistically) independent, if \\(P(X\\in A, Y\\in B) = P(X\\in A) P(Y\\in B)\\) for all sets \\(A\\) and \\(B\\). We list some properties of independent random variables: If \\(X\\) and \\(Y\\) are independent, and if \\(f\\) and \\(g\\) are functions, then \\(f(X)\\) and \\(g(Y)\\) are also independent. B.2 The Chi-Squared Distribution Definition B.2 Let \\(X_1, \\ldots, X_\\nu \\sim \\mathcal{N}(0, 1)\\) be i.i.d. Then the distribution of \\(\\sum_{i=1}^\\nu X_i^2\\) is called the \\(\\chi^2\\)-distribution with \\(\\nu\\) degrees of freedom. The distribution is denoted by \\(\\chi^2(\\nu)\\). Some important results about the \\(\\chi^2\\)-distribution are: \\(\\chi^2\\)-distributed random variables are always positive. If \\(Y\\sim \\chi^2(\\nu)\\), then \\(\\mathbb{E}(Y) = \\nu\\) and \\(\\mathop{\\mathrm{Var}}(Y) = 2\\nu\\). The R command pchisq(|\\(x\\),\\(\\nu\\)) gives the value \\(\\Phi_\\nu(x)\\) of the CDF of the \\(\\chi^2(\\nu)\\)-distribution. The R command qchisq(\\(\\alpha\\),\\(\\nu\\)) can be used to obtain the \\(\\alpha\\)-quantile of the \\(\\chi^2(\\nu)\\)-distribution. More properties can be found on Wikipedia. B.3 The t-distribution Definition B.3 Let \\(Z \\sim \\mathcal{N}(0,1)\\) and \\(Y \\sim \\chi^2(\\nu)\\) be independent. Then the distribution of \\[\\begin{equation} T = \\frac{\\,Z\\,}{\\,\\sqrt{Y / \\nu}\\,} \\tag{B.1} \\end{equation}\\] is called the \\(t\\)-distribution with \\(\\nu\\) degrees of freedom. This distribution is denoted by \\(t(\\nu)\\). Some important results about the \\(t\\)-distribution are: The \\(t\\)-distribution is symmetric: if \\(T \\sim t(\\nu)\\), then \\(-T \\sim t(\\nu)\\) If \\(T\\sim t(\\nu)\\), then \\(\\mathbb{E}(T) = 0\\). The R command pt(|\\(x\\),\\(\\nu\\)) gives the value \\(\\Phi_\\nu(x)\\) of the CDF of the \\(t(\\nu)\\)-distribution. The R command qt(\\(\\alpha\\),\\(\\nu\\)) can be used to obtain the \\(\\alpha\\)-quantile of the \\(t(\\nu)\\)-distribution. More properties can be found on Wikipedia. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
